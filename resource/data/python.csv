	title	author	date	url	text	comment_count	fullname	type	co_authors	api_index
0	Sunday Daily Thread: What's everyone working on this week?	Unknown	2023-12-31 00:00:09	https://www.reddit.com/r/Python/comments/18utrn3/sunday_daily_thread_whats_everyone_working_on/	# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to! ## How it Works: 1. **Show & Tell**: Share your current projects, completed works, or future ideas. 2. **Discuss**: Get feedback, find collaborators, or just chat about your project. 3. **Inspire**: Your project might inspire someone else, just as you might get inspired here. ## Guidelines: * Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome. * Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here. ## Example Shares: 1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate! 2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better. 3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier! Let's build and grow together! Share your journey and learn from others. Happy coding! üåü	6.0	t3_18utrn3	reddit		
1	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2024-01-06 00:00:18	https://www.reddit.com/r/Python/comments/18zlr3i/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	1.0	t3_18zlr3i	reddit		
2	How to write a python decorator (and why)	Unknown	2024-01-06 11:19:32	https://www.reddit.com/r/Python/comments/18zxxak/how_to_write_a_python_decorator_and_why/	tldr; Decorators are a nice way to wrap functions in other functions and re-use code [https://www.jaredbwelch.com/blog/How\_to\_write\_a\_python\_decorator\_and\_why](https://www.jaredbwelch.com/blog/How_to_write_a_python_decorator_and_why)	9.0	t3_18zxxak	reddit		
3	Astronomy / Space Science: Working on meteor data	git push -f	2024-01-06 17:53:49	https://www.reddit.com/r/Python/comments/1905n4g/astronomy_space_science_working_on_meteor_data/	"Hey everyone, I started with a new ""Space Science with Python"" tutorial and would like to share it with you. It is about tiny dust particles entering our atmosphere: meteors. Thankfully, the International Astronomical Union (IAU) provides free-accessible datasets with almost 1 Million meteors. These data contain physical and dynamical properties like the brightness of the meteor, its original orbit around the Sun and the appearance coordinates on the sky. If you are interested, there data is here: [https://ceres.ta3.sk/iaumdcdb/home/catalog/video](https://ceres.ta3.sk/iaumdcdb/home/catalog/video) But if you are not that deep into this space-scientific field: I started to create some Jupyter Notebooks to work with the data. The objective of my tutorial: first, understanding the meteor physics and then I'll create a Variational Autoencoder based meteor shower detection model. Anyway, if you are interested, here is the code: [https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/tree/main/Project-Meteor-Science](https://github.com/ThomasAlbin/Astroniz-YT-Tutorials/tree/main/Project-Meteor-Science) And the corresponding tutorial playlist, and yes, my videos are not fancy YouTube professional productions, I am just a guy sharing his passion with the world: [https://www.youtube.com/watch?v=5FK3dTrW\_Fc&list=PLNvIBWkEdZ2g3ifrQ6O06fkeetf8e1NDg](https://www.youtube.com/watch?v=5FK3dTrW_Fc&list=PLNvIBWkEdZ2g3ifrQ6O06fkeetf8e1NDg) Cheers, Thomas"	2.0	t3_1905n4g	reddit		
4	One billion row challenge	Unknown	2024-01-05 21:23:44	https://www.reddit.com/r/Python/comments/18zi0o5/one_billion_row_challenge/	Just saw this repo trending and thought of doing this in different languages, e.g. Python. [https://github.com/gunnarmorling/1brc](https://github.com/gunnarmorling/1brc) Do you know if it's already available?	7.0	t3_18zi0o5	reddit		
5	Question for real devs	Unknown	2024-01-06 18:32:46	https://www.reddit.com/r/Python/comments/1906kh7/question_for_real_devs/	I've been a hobbyist game developer for a while, not very good, because of a certain wall I keep hitting. I can spend weeks on a simple idea, grow it until the code is too convoluted to work on effectively any more, and then fail to finish the project entirely. How does a real developer maintain the focus on SRP and modularization so that a really complex thing can eventually be finished? Or am I focusing on the wrong things? Mostly using Python and Pygame these days but open to change.	2.0	t3_1906kh7	reddit		
6	Python Docker Monitor to test my skills	Unknown	2024-01-06 13:52:58	https://www.reddit.com/r/Python/comments/1900fxe/python_docker_monitor_to_test_my_skills/	Hi everyone, I'm excited to share a project I've been working on to get back into development after a two-year hiatus. I'd love to get your feedback on it! I'd like to get back as a dev but I'm not sure I'm capable. [repo link](https://github.com/smscarano/docker_monitor)	1.0	t3_1900fxe	reddit		
7	FlaskyLMS - A Simple Leave Management System in Flask with Google Calendar Integration	Unknown	2024-01-06 11:30:19	https://www.reddit.com/r/Python/comments/18zy385/flaskylms_a_simple_leave_management_system_in/	A simple leave management tool that I built for myself a while ago. Now sharing on GitHub. Not the most pretty looking and lacks many bells and whistles but still gets the main job done xD * User-friendly leave request process: Employees can easily submit leave requests through a simple and intuitive interface. * Admin approval and rejection: Admins can review leave requests, approve or reject them, and provide feedback if needed. * Email notifications: Both employees and admins receive timely email notifications for new requests, approvals, and rejections. * Google Calendar integration: Approved leaves are automatically added as events to the admin's Google Calendar, ensuring visibility. Try or give your feedback: https://github.com/Suleman-Elahi/FlaskyLMS	0.0	t3_18zy385	reddit		
8	"2,000 free sign ups available for the ""Automate the Boring Stuff with Python"" online course. (Jan 2024)"	"Author of ""Automate the Boring Stuff"""	2024-01-05 21:51:36	https://www.reddit.com/r/Python/comments/18ziobn/2000_free_sign_ups_available_for_the_automate_the/	"If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): https://udemy.com/course/automate/?couponCode=JAN2024FREE https://udemy.com/course/automate/?couponCode=JAN2024FREE2 If you are reading this after the sign ups are used up, you can always find [the first 15 of the course's 50 videos are free on YouTube if you want to preview them.](https://www.youtube.com/watch?v=1F_OgqRuSdI&list=PL0-84-yl1fUnRuXGFe_F7qSH1LEnn9LkW) YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have ""preview"" turned on. Scroll down to find and click ""Expand All Sections"" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. **NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.** **I'm also working on another Udemy course** that follows my recent book ""Beyond the Basic Stuff with Python"". So far I have [the first 15 of the planned 56 videos done. You can watch them for free on YouTube.](https://www.youtube.com/watch?v=kSrnLbioN6w&list=PL0-84-yl1fUmeV_2bBSguF_S0TVZk8wow&index=1) **Frequently Asked Questions:** (*read this before posting questions*) * This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. * If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. * This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com * The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ * I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. * It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. * **You're not too old to learn to code. You don't need to be ""good at math"" to be good at coding.** * Signing up is the first step. Actually finishing the course is the next. :) [There are several ways to get/stay motivated.](https://www.reddit.com/r/learnprogramming/wiki/faq#wiki_how_can_i_get.2Fstay_motivated_to_learn_programming.3F) I suggest getting a ""gym buddy"" to learn with. Check out /r/ProgrammingBuddies"	0.0	t3_18ziobn	reddit		
9	MiniFirehose - A lightweight data buffering and delivery system	Unknown	2024-01-06 14:10:55	https://www.reddit.com/r/Python/comments/1900se3/minifirehose_a_lightweight_data_buffering_and/	Hi everyone! I wanted to share a project I made in just a week. It's called MiniFirehose (somewhat similar to AWS Firehose). It's a really simple tool for managing data, kind of like Kafka or MQTT but much easier to use and lighter on resources. With MiniFirehose, you can collect messages and send them to places like your local filesystem or Amazon S3. It's not complicated to set up, and it's great if you're working on something small and don't need a big system. Since I made it pretty quickly, there might be some small bugs. If you try it and have any ideas on how to make it better, I'd love to hear from you. Or, if you know other tools like this, let me know! so I don't spend much time on it. You can check it out here: \[[mini-firehose](https://github.com/waqar-ahmed/mini-firehose)\]. I'm looking forward to hearing what you think! Thanks!	0.0	t3_1900se3	reddit		
10	VisioNomicon - GPT-4V Smart Image Renamer	Unknown	2024-01-06 02:20:59	https://www.reddit.com/r/Python/comments/18zovd6/visionomicon_gpt4v_smart_image_renamer/	VisioNomicon is a powerful Python-based command-line utility tool designed to rename image files using the capabilities of GPT-4. Descriptive filenames are generated based on a user given template and the content of the image. Try it out, and let me know what you guys think! All the details can be found at: [https://github.com/rehanzo/VisioNomicon](https://github.com/rehanzo/VisioNomicon)	0.0	t3_18zovd6	reddit		
11	Polars in Aggregate: A small subselection on where we have been working on.	Unknown	2024-01-05 09:26:56	https://www.reddit.com/r/Python/comments/18z2wp7/polars_in_aggregate_a_small_subselection_on_where/	See the blog here: https://pola.rs/posts/polars\_in\_aggregrate-0.20/	3.0	t3_18z2wp7	reddit		
12	Event Driven vs Loop Driven what's your preference?	Unknown	2024-01-05 06:33:25	https://www.reddit.com/r/Python/comments/18z08uh/event_driven_vs_loop_driven_whats_your_preference/	Streamlit is loop driven it runs every time any thing change on the app, is it performance issue for you? do you prefer Event driven framework against loop driven?	11.0	t3_18z08uh	reddit		
13	Is style transfer possible in python without downloading heavy packages?	Unknown	2024-01-06 04:43:26	https://www.reddit.com/r/Python/comments/18zroew/is_style_transfer_possible_in_python_without/	I found this project called arbitrary style transfer in the browser using tensorflow.js: https://reiinakano.com/arbitrary-image-stylization-tfjs/ (source code: https://github.com/reiinakano/arbitrary-image-stylization-tfjs) This style transfer works in the browser without uploading anything in server, it just installs a smaller version of inception v3 model to transfer style from any image. You just have to select the images from your disk and it stylizes the image. Though I have found python repositories for arbitrary style transfer but they require lots of things to setup like torch, tensoflow, network models, training datasets and also needs a gpu. Moreover I don't want to use google collab, I want something light weight like that website, but in python so that I can use it in offline mode. (with that low size inception v3 model only) In short I want some fast style transfer without downloading any heavy python packages on my system. Comment if you have any project links...	1.0	t3_18zroew	reddit		
14	MineSweeper Game in Python and Tkinter	Unknown	2024-01-05 16:32:02	https://www.reddit.com/r/Python/comments/18zaz6v/minesweeper_game_in_python_and_tkinter/	I made a MineSweeper game using Python and Tkinter. Code: https://github.com/DataWizual/MineSweeper Here's the video explaining how I did it: https://www.youtube.com/watch?v=uYv26qoHLN0	1.0	t3_18zaz6v	reddit		
15	Saving Metadata When Using Python Decorators | Jacob Padilla	Unknown	2024-01-05 19:16:22	https://www.reddit.com/r/Python/comments/18zexf0/saving_metadata_when_using_python_decorators/	Wrapping one object over another can result in the loss of valuable metadata; that's why using the functools.wraps decorator is crucial when developing your own Python decorators. In this [article](https://jacobpadilla.com/articles/Functools-Deep-Dive), explore the intricacies of functools.wraps and how it works under the hood!	0.0	t3_18zexf0	reddit		
16	Draw2Img: A simple web UI for interactive text-guided image to image generation via SDXL-Turbo, intended for any age and level of expertise.	Unknown	2024-01-05 18:06:30	https://www.reddit.com/r/Python/comments/18zd9dx/draw2img_a_simple_web_ui_for_interactive/	With the release of SDXL-Turbo in late Nov 2023, it became feasible to perform text-guided image to image generation in real-time on a consumer grade GPU. Inspired by this progress and my little cousins' artwork, I put together a web app that integrates an interactive canvas & paint tool with the capabilities of SDXL-Turbo. The project is called Draw2Img, and I want to share it here in the hopes that you or your family enjoys it as much as we have. What makes this project unique is the combination of: 1) a simple and accessible web based UI for younger or non-technical users 2) the stunning speed & quality of image generation 3) user friendly & LAN party ready (easy to run, multiple concurrent users supported) Thanks in advance for your feedback and support, cheers! https://github.com/GradientSurfer/Draw2Img	0.0	t3_18zd9dx	reddit		
17	PyWindowsScreenCapture, Tool for efficient screen capturing on Windows	Unknown	2024-01-05 17:34:58	https://www.reddit.com/r/Python/comments/18zcht6/pywindowsscreencapture_tool_for_efficient_screen/	"I stepping into the world of C programming with my latest project, ""Windows Screen Capture DLL,"" and its Python wrapper, ""PyWindowsScreenCapture."" This project represents not just a tool for efficient screen capturing on Windows but also my journey in learning and improving my C programming skills. \*\*About the Project:\*\* \- \*\*Windows Screen Capture DLL\*\*: A dynamic link library I developed to push the boundaries of screen capture performance on Windows platforms. \- \*\*PyWindowsScreenCapture\*\*: A Python wrapper that provides an easy-to-use interface to the DLL, making it accessible for Python developers. \*\*Key Features:\*\* \- Optimized for performance, potentially outperforming MSS (Python Screen Capture). \- Multi-monitor support, capturing high-resolution screens efficiently. \- Designed with simplicity and minimal dependencies in mind. The motivation behind this project was not only to create a high-performance tool but also to challenge myself and enhance my understanding of C programming. As my first serious foray into C, I'm keen on receiving feedback, suggestions, and contributions from the community to help me grow as a developer. While this project is still in its beta phase, I am proud of what I've accomplished and am excited to see how it can evolve with community input. Whether you're interested in high-speed screen capturing or have insights into C development, I welcome your thoughts and contributions. You can check out the project and contribute here: \- DLL: https://github.com/offerrall/WindowsScreenCapture \- Python Wrapper: https://github.com/offerrall/PyWindowsScreenCapture I‚Äôm looking forward to your feedback and suggestions. Thank you for being a part of my programming journey!"	0.0	t3_18zcht6	reddit		
18	Traktstats without premium	Unknown	2024-01-05 16:05:50	https://www.reddit.com/r/Python/comments/18zachx/traktstats_without_premium/	Hi Everyone, I wanted to share my python project, this python [program](https://github.com/Ahmedazim7804/trakt_vip_stats) uses trakt and tmdb api to generate all-time-stats like the official feature of trakt which requires subscription. You can then use [this app](https://github.com/Ahmedazim7804/traktstats_app) to view those stats visually. if you have any feedback, i would love to hear it. Edit :- English is not my first language, so excuse any grammer mistake.	1.0	t3_18zachx	reddit		
19	LLama.cpp AI reddit poster / commenter	Unknown	2024-01-05 15:51:39	https://www.reddit.com/r/Python/comments/18za06z/llamacpp_ai_reddit_poster_commenter/	These two scripts use llama.cpp but note - rename the llama.cpp main.exe file to main2.exe or change the bits in the code to main.exe . You can change the AI model in the code when it runs the main2.exe to a gguf model of your choice. Both ask what subreddit you want to post to and with what prompt. The scripts use reddit API which you have to have. I dunno about the reddit AI policy. I tested it on a subreddit called Hergidonia I made up to play with it. Install the modules in the beginning of the script and llama.cpp and it should work fine if the main.exe is in the folder you run it and the ai model points to your gguf model. https://pastebin.com/yqTq1aMt	0.0	t3_18za06z	reddit		
20	API Logic Server - Kafka Application Integration	Unknown	2024-01-05 03:15:19	https://www.reddit.com/r/Python/comments/18ywi8a/api_logic_server_kafka_application_integration/	API Logic Server is an open source Python project that creates executable web app projects instantly from da database, with a single CLI command -- a JSON:API with Swagger, and a multi-page multi-table Admin App. SQLAlchemy classes are created automatically. Customize the project in your IDE using Python and rules. Rules are spreadsheet-like assertions expressed in Python, implementing role-based row-level security, and multi-table constraint and derivation logic for database integrity. You can extend the API with standard Flask and SQLAlchemy. Application Integration is supported via APIs and Kafka Message handling. Consulting and Training are available. Links: * \[Docs\]([https://apilogicserver.github.io/Docs/](https://apilogicserver.github.io/Docs/)) - includes 5 min video * \[Application Integration\]([https://apilogicserver.github.io/Docs/Sample-Integration/](https://apilogicserver.github.io/Docs/Sample-Integration/)) * \[Rules\]([https://apilogicserver.github.io/Docs/Logic-Why/](https://apilogicserver.github.io/Docs/Logic-Why/)) * \[API\]([https://apilogicserver.github.io/Docs/API/](https://apilogicserver.github.io/Docs/API/)) * \[Admin Web App\](https://apilogicserver.github.io/Docs/Admin-Tour/)	2.0	t3_18ywi8a	reddit		
21	Statically enforcing frozen data classes in Python	Unknown	2024-01-04 11:54:33	https://www.reddit.com/r/Python/comments/18ybepc/statically_enforcing_frozen_data_classes_in_python/	Wrote a quick TIL on how to statically enforce frozen data classes in Python. Had to resort to crowd sourcing & good ol' stackoverflow to figure this one out since LLMs were of no help: https://rednafi.com/python/statically_enforcing_frozen_dataclasses/	6.0	t3_18ybepc	reddit		
22	GGUF LLAMA AI - Package for simplified text generation with Llama models quantized to GGUF format	Unknown	2024-01-04 17:25:39	https://www.reddit.com/r/Python/comments/18yijbp/gguf_llama_ai_package_for_simplified_text/	I'm looking for some developer who'd be interested in helping develop this simple project that tries to maximally simplify gguf models deployment on cpu to make this tech more accessible [https://github.com/laelhalawani/glai](https://github.com/laelhalawani/glai) It is a llama-ccp wrapper that simplifies use of llama based models.It features a built in ModelDB with json entries that can be used to automatically download and deploy quantized gguf models from hf.Then there are to classes AutoAI and EasyAI.The frist one takes min of 3 arguments including search query or path or url and max tokens and max input tokens. And that's all that's needed to load the model for inference. The later allows configuration of the model in few simple steps. Giving more granular control, while still prioritizing simplicity. There's a bunch of examples and detailed documentation already. The project is also published on pypi \`pip install glai\`	2.0	t3_18yijbp	reddit		
23	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2024-01-05 00:01:09	https://www.reddit.com/r/Python/comments/18ys8mu/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	2.0	t3_18ys8mu	reddit		
24	Clickstream Aggregation in Python with Redis	Unknown	2024-01-04 18:07:01	https://www.reddit.com/r/Python/comments/18yjjow/clickstream_aggregation_in_python_with_redis/	This is a tutorial article on how you can use Redis PubSub capabilities with the Python streaming library Bytewax to aggregate clickstreams in Python. &#x200B; https://redis.com/blog/redis-driven-dataflow-for-clickstream-aggregation-with-bytewax/	0.0	t3_18yjjow	reddit		
25	Why Python is slower than Java?	Unknown	2024-01-03 09:44:10	https://www.reddit.com/r/Python/comments/18xfmq2/why_python_is_slower_than_java/	Sorry for the stupid question, I just have strange question. If CPython interprets Python source code and saves them as byte-code in .pyc and java does similar thing only with compiler, In next request to code, interpreter will not interpret source code ,it will take previously interpreted .pyc files , why python is slower here? Both PVM and JVM will read previously saved byte code then why JVM executes much faster than PVM? Sorry for my english , let me know if u don't understand anything. I will try to explain	42.0	t3_18xfmq2	reddit		
26	Fast and secure routing development and OpenAPI bindings in sanic, flask, tornado, starlette	Unknown	2024-01-04 07:59:38	https://www.reddit.com/r/Python/comments/18y7t6j/fast_and_secure_routing_development_and_openapi/	I created a library - [pait](https://github.com/so1n/pait), which is compatible with multiple Python web frameworks. At the same time, it has absorbed some excellent designs from FastAPI. Through pait, you can quickly and safely develop routing functions and view OpenAPI data.	0.0	t3_18y7t6j	reddit		
27	Check Out Flasknotes - A Flask Note-Taking Web App!	Unknown	2024-01-04 04:13:13	https://www.reddit.com/r/Python/comments/18y3x63/check_out_flasknotes_a_flask_notetaking_web_app/	**Hey Python enthusiasts!** I'm excited to share my latest project, Flasknotes - a simple web-based note-taking application built using Flask and MySQL. This project boasts various features, including role-based authentication, CRUD operations, and the ability to mark your favorite notes. **Project Links:** * **GitHub Repository:** [Flasknotes GitHub Repo](https://github.com/ghandylan/flask-notes) * **Live Demo:** [Flasknotes Demo](https://flasknotes.pythonanywhere.com/home)	2.0	t3_18y3x63	reddit		
28	Fastest Way to Read Excel in Python	Unknown	2024-01-03 12:58:43	https://hakibenita.com/fast-excel-python	Empty	10.0	t3_18xitr3	reddit		
29	Ezsynth -- Ebsynth Video Stylization as a Python Library.	Unknown	2024-01-04 01:58:49	https://www.reddit.com/r/Python/comments/18y13rv/ezsynth_ebsynth_video_stylization_as_a_python/	Hey all! I've been working on this project on and off for a few months, and for the most part, its pretty stable at this point.If you're familiar with the program ebsynth, you'll be right at home with Ezsynth.Ezsynth is a recreation of the ebsynth video stylization process, through the use of: - a) The ebsynth.dll source code, interfaced through ctypes, - b) RAFT Optical Flow, and - C) PhyCV physics based edge detection. Ultimately the goal was to create a simple, pythonic way to achieve a similar output to the ebsynth program. Most current methods involve GUI mapping, etc, in order to make ebsynth work with python. Ezsynth is a full recreation of the original ebsynth paper, offering both the original method of stylization, and updated versions using RAFT and various PhyCV features. I've only been programming since April, most of my work is done by coming up with workflows, writing pseudo code, and then bouncing back and forth with GPT 4 on how to accomplish what I want. Check it [here](https://github.com/Trentonom0r3/Ezsynth), and I apologize for any noob mistakes or things you may find-- I'm still learning! A demo of what it does can be found [here](https://github.com/Trentonom0r3/Ezsynth/blob/a3981fa3169fb076284fba432f239017e3d7e021/ezsynthdemo.mp4)=	2.0	t3_18y13rv	reddit		
30	Simple keylogger made in Python	Unknown	2024-01-04 09:25:36	https://www.reddit.com/r/Python/comments/18y92tw/simple_keylogger_made_in_python/	Here is the repo: [https://github.com/Migue8gl/Python-scripts](https://github.com/Migue8gl/Python-scripts) I am open to criticism on what I can improve in this small project. Over time I would like to create more Python scripts touching on different areas.I have to say that. I'm not new to programming and I'm not new to Python either, but I'm sure I have some things wrong that can be fixed.	1.0	t3_18y92tw	reddit		
31	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2024-01-04 00:00:07	https://www.reddit.com/r/Python/comments/18xydsg/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	1.0	t3_18xydsg	reddit		
32	Knuckledragger: Experimenting with a Python Proof Assistant	Unknown	2024-01-03 16:19:24	https://www.reddit.com/r/Python/comments/18xn6ne/knuckledragger_experimenting_with_a_python_proof/	Hi, An idea I've toyed around for a while is how to chain together inferences of automated theorem provers like z3py into larger developments. I've started putting fingers to keyboard. The point is to make something accessible to a larger, less specialized audience and to target mathematics akin to that in sympy, so I'm very interested in feedback about what works for people. * Blog post: https://www.philipzucker.com/python-itp/ * Very WIP repo: https://github.com/philzook58/knuckledragger	2.0	t3_18xn6ne	reddit		
33	AWS Hosting Flask/Python Application on EC2 Instance	Unknown	2024-01-03 20:07:02	https://youtu.be/VFTeLN0J9Lw?si=kdOsJW004fnhzMWa	Empty	0.0	t3_18xsn2l	reddit		
34	An AI Python Web app to analyze resumes	Unknown	2024-01-04 17:08:09	https://www.reddit.com/r/Python/comments/18yi41b/an_ai_python_web_app_to_analyze_resumes/	Hey r/python, say goodbye to tedious resume evaluations ‚Äì https://resume-analyzer.ploomberapp.io/. üåê What do you guys think? I wanted to supercharge my hiring process and make smarter decisions in a snap. I've connected this code, based on Open AI and on Streamlit. This Python code is open-sourced and is available in the [GitHub repo](https://github.com/gopiashokan/AI-Powered-Resume-Analyzer-and-LinkedIn-Scraper-with-Selenium). I've hosted it on Ploomber Cloud.	8.0	t3_18yi41b	reddit		
35	Okay, I have this genuine question, why does Python allow hashing functions?	Unknown	2024-01-02 23:58:18	https://www.reddit.com/r/Python/comments/18x4kw6/okay_i_have_this_genuine_question_why_does_python/	So I was working on my programming language and was taking inspiration from Python, and realized that Python allows hashing functions and you can have functions As keys, this is what confuses me here what is the use of this, and how do they do it to begin with? Like do they have something like a dictionary data structure that has Python objects as keys? That sounds like it's memory inefficient. &#x200B;	11.0	t3_18x4kw6	reddit		
36	Pydantic has too much deprecation, making it difficult to keep up with updates and maintaining the code. Lots of functionality has been renamed, and some are removed during v1‚Üív2 transition. Even sample code from November 2023 is deprecated now! Are there better alternatives?	Unknown	2024-01-03 19:15:33	https://www.reddit.com/r/Python/comments/18xrc1y/pydantic_has_too_much_deprecation_making_it/	Almost all tutorials I see online (and ChatGPT's knowledge base) teach you Pydantic v1. There are numerous things that are deprecated during transition to v2 (@root_validator, @validator, using 'always', etc. are all gone now.). I even found a code example on its Github from November 2023 which now throws an error, saying that FieldValidationInfo is deprecated now, use <new_thing> instead... I wanted to use something to validate user inputs to my API, but getting Pydantic right and then keeping it updated has been too much unnecessary work, which makes me wonder if you have also faced this problem and what your solution is?	27.0	t3_18xrc1y	reddit		
37	PyJigsaw: A Digital Jigsaw Puzzle Factory	Unknown	2024-01-03 10:48:14	https://www.reddit.com/r/Python/comments/18xglbx/pyjigsaw_a_digital_jigsaw_puzzle_factory/	I created a jigsaw puzzle constructor which uses mostly Python with a small assist from Inkscape. You can create SVG puzzle sets and templates programmatically, which could be useful as part of a jigsaw puzzle app (my original intention for it). I've shelved the wider project but the constructor was in a decent place, so decided to tidy it up and package it. Repo and install instructions are available on my [GitHub](https://github.com/tomdeabreucodes/PyJig). If you're interested in a slightly longer post with some additional background, I also put a post up about it [here](https://tomdeabreu.uk/posts/jigsaw-puzzle-cut-template-svg/). [blank cut template](https://preview.redd.it/esielejcf7ac1.png?width=1320&format=png&auto=webp&s=2c9ceb9e18c5c5ffa4c4b07bbfcf6963b35138f5) &#x200B; [Applied cut on image](https://preview.redd.it/0b5i72uef7ac1.png?width=1320&format=png&auto=webp&s=229a039f2fc9363060698044eccbd6833f524af1) &#x200B;	0.0	t3_18xglbx	reddit		
38	llama.cpp GGUF inference in a couple lines of code	Unknown	2024-01-03 14:09:54	https://www.reddit.com/r/Python/comments/18xk9eo/llamacpp_gguf_inference_in_a_couple_lines_of_code/	&#x200B; https://preview.redd.it/l4r0ne7tf8ac1.png?width=1296&format=png&auto=webp&s=932ff628716371aa4770e574df9134d30c2bc9f5 [txtai](https://github.com/neuml/txtai) has a unified LLM pipeline that can load Hugging Face models, llama.cpp GGUF files and LLM APIs. The example above downloads a GGUF file from the Hugging Face Hub and runs inference with the model. See this article for more: [https://neuml.hashnode.dev/integrate-llm-frameworks](https://neuml.hashnode.dev/integrate-llm-frameworks)	1.0	t3_18xk9eo	reddit		
39	Python GUI framework for windows applications and embedded systems.	Unknown	2024-01-03 04:52:15	https://www.reddit.com/r/Python/comments/18xav2l/python_gui_framework_for_windows_applications_and/	A few months ago I saw someone posting maybe here or in another subreddit about a Python framework for making GUIs. I distinctly remember that it can be used to make guis for embedded systems as well as something similar to windows apps. Unfortunately, I forgot the name of it and can't seem to find that post anywhere. Does anyone have any clues as to what this framework is called? &#x200B; Many thanks\~	13.0	t3_18xav2l	reddit		
40	Polars DataFrames now have a `.plot` namespace!	:pandas_Logo: pandas Core Dev 	2024-01-02 16:32:49	https://www.reddit.com/r/Python/comments/18wti72/polars_dataframes_now_have_a_plot_namespace/	As of Polars 0.20.3, you can use \`polars.DataFrame.plot\` to visualise your data. The plotting logic isn't in Polars itself, but in hvplot (so you'll need that installed too) &#x200B; Here's some examples of what you can do: https://preview.redd.it/h8fhtnvi02ac1.png?width=693&format=png&auto=webp&s=5a299bac0df26575f3a4efa071707061cea719c4 https://preview.redd.it/k2071pvi02ac1.png?width=728&format=png&auto=webp&s=3ed2ce9e07f39b7c694f4dc8648647bed34daa60 https://preview.redd.it/r8t6oovi02ac1.png?width=680&format=png&auto=webp&s=907461be7c05fdd63b1b469cd8fd5c24c2b9741d https://preview.redd.it/bm8yuqvi02ac1.png?width=742&format=png&auto=webp&s=358da56c5c3e2d13bbf3576e655f3bc7b6e9d24a https://preview.redd.it/mi0udtvi02ac1.png?width=734&format=png&auto=webp&s=160f01651d2723742630cb5af50a90c74972c8d4	11.0	t3_18wti72	reddit		
41	I made an IDE using PyQt6 [UPDATE]	Unknown	2024-01-02 14:40:27	https://www.reddit.com/r/Python/comments/18wqxkm/i_made_an_ide_using_pyqt6_update/	&#x200B; [Editor](https://preview.redd.it/b79eg796g1ac1.png?width=1763&format=png&auto=webp&s=851ec11f48cc38652f96e4a515241dde89cf1705) [Markdown Editing](https://preview.redd.it/pi26k767g1ac1.png?width=1920&format=png&auto=webp&s=5eea5ff8f5f595a238a1010fb643183251306965) Highlighted Features: * Supports up to 30 languages w Syntax highlighting * auto complete * split pane markdown editor * terminal with Aura Text specific commands and also terminal history * plugin support * autocomplete (you can literally theme anything) GitHub: [https://github.com/rohankishore/Aura-Text](https://github.com/rohankishore/Aura-Text)	4.0	t3_18wqxkm	reddit		
42	I made an Educational Deep Learning Framework from scratch	Unknown	2024-01-02 21:02:46	https://www.reddit.com/r/Python/comments/18x097o/i_made_an_educational_deep_learning_framework/	Learning ML, I‚Äôve always been interested in **PyTorch** and its **Autograd engine** (which creates the backprop automatically). In [this project](https://github.com/eduardoleao052/Autograd-from-scratch.git), I tried to **reimplement most of PyTorch** (including the Autograd) from scratch in a **well-documented, unit tested, and interpretable** way. It was really useful for me, and I hope it can help you understand Autograd better as well! Hope you enjoy! GitHub repository [here](https://github.com/eduardoleao052/Autograd-from-scratch.git)!	3.0	t3_18x097o	reddit		
43	üçÄ How to Create Stunning Music Posters in Seconds: Introducing BeatPrints!	Unknown	2024-01-02 14:06:26	https://www.reddit.com/r/Python/comments/18wq7ru/how_to_create_stunning_music_posters_in_seconds/	&#x200B; https://preview.redd.it/2ae0jplk91ac1.png?width=1280&format=png&auto=webp&s=d6c6b7276f42dac753eb309e47fa6b12f49a1672 **Ever wondered how to create music posters like the ones you see on Pinterest?** üé® Maybe you've wanted something aesthetic to jazz up your Instagram stories or noticed your walls looking a tad empty? Perhaps you're the collector type or simply love decking out your space with artistic vibes? Look no further‚Äîintroducing BeatPrints! üé® **What's BeatPrints?** BeatPrints is your one-stop tool for crafting eye-catching music posters that stand out! It's your gateway to generate custom, beautiful posters that capture the essence of your favorite music track from Spotify. **ü§∑ Why you want to use it?** * **Ease of Use:** Say goodbye to complex design software‚ÄîBeatPrints makes poster creation straightforward and fun! * **Aesthetic Appeal:** Create posters perfect that's for Instagram, Pinterest, or maybe sprucing up your living space. * **Versatility:** Whether you're a collector, a decorator, or just love stylish visuals, BeatPrints has you covered. üîó **GitHub Project Link:** [BeatPrints on GitHub](https://github.com/TrueMyst/BeatPrints) Let BeatPrints transform your favourites music tracks into stunning posters! üé®‚ú®	1.0	t3_18wq7ru	reddit		
44	PyPy has moved to GitHub	pmatti - mattip was taken	2024-01-01 19:19:08	https://www.reddit.com/r/Python/comments/18w45u2/pypy_has_moved_to_github/	PyPy has moved its development efforts from Mercurial + Heptapod to Git + GitHub. Read more about it [here](https://www.pypy.org/posts/2023/12/pypy-moved-to-git-github.html)	6.0	t3_18w45u2	reddit		
45	Good pytube alternative?	Unknown	2024-01-02 20:44:17	https://www.reddit.com/r/Python/comments/18wzsg8/good_pytube_alternative/	I was using pytube to download my Shazam library and, well, it worked; but the AdBlock blocker in YouTube broke everything. Pytube cannot skip the ads, but is actively downloading them. Pytube seems discontinued. Does somebody know about a good alternative, or do I have to live without my downloader?	3.0	t3_18wzsg8	reddit		
46	Wednesday Daily Thread: Beginner questions	Unknown	2024-01-03 00:00:10	https://www.reddit.com/r/Python/comments/18x4mfq/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	1.0	t3_18x4mfq	reddit		
47	Python's Array: Working With Numeric Data Efficiently	Unknown	2024-01-02 17:26:23	https://realpython.com/python-array/	Empty	0.0	t3_18wut5u	reddit		
48	i made a ChatGPT bot for Telegram	Unknown	2024-01-02 14:06:58	https://www.reddit.com/r/Python/comments/18wq86m/i_made_a_chatgpt_bot_for_telegram/	This project utilizing the new Assistant OpenAl API. link to source: https://github.com/andykras/gptbot Bot is built using the async features of aiogram and AsyncOpenAl, showcasing modern asynchronous programming in Python.	1.0	t3_18wq86m	reddit		
49	Arrest v0.1.5 is released! Including a lot of new improvements based on the community feedback!	Unknown	2024-01-02 05:45:30	https://www.reddit.com/r/Python/comments/18whybi/arrest_v015_is_released_including_a_lot_of_new/	Hi everyone! I am really happy to announce Arrest [v0.1.5](https://pypi.org/project/arrest/) after the overwhelming number of feedback from the community in my last post here. A brief overview of the changes made: &#x200B; * Added support for most of the httpx arguments (i.e., cookies, auth, transport, cert, etc) as kwargs in both service and resource initializations. * Added backoff retries for all the http calls being made with configurable max retries (set as environment variable) * Add a new decorator for a resource instance \`.handler(...)\` where you can specify the subpath from the resource and define your own custom function. The function is injected with a reference to the resource instance, and the complete url for easier access. The function can be triggered as a free function but also is registered under the resource instance, so can be invoked via \`resource\_name.func\_name(...)\` * Added support for passing your own \`httpx.AsyncClient\` instance to either service or resource. (can also be a subclass of \`httpx.AsyncClient\`) For more details, please check out the [docs](https://s-bose.github.io/arrest/). [Github repo](https://github.com/s-bose/arrest) for anyone who wants to take a look! Thank you to everyone whose feedback made these changes happen. I'd appreciate it if you could let me know of anything else that might be helpful to be implemented!	1.0	t3_18whybi	reddit		
50	Build amazing AI projects with Google‚Äôs Gemini models and Python	Unknown	2024-01-02 15:49:46	https://www.reddit.com/r/Python/comments/18wshab/build_amazing_ai_projects_with_googles_gemini/	Hi everyone, I‚Äôm excited to share with you my repository of Python projects and ideas that use Google‚Äôs latest and most powerful generative AI models: Gemini-pro and Gemini-pro-vision. These models can perform various tasks such as text-to-speech conversion, interactive chat, image and video processing, and content generation. With my repository, you can: * Convert any text into natural-sounding speech with Gemini-pro * Chat with a friendly and engaging AI assistant powered by Gemini-pro-chat * Recognize and analyze images and videos with Gemini-pro-vision * Generate diverse and dynamic content such as poems, stories, code, essays, and more with Gemini-pro My repository also provides educational insights and project ideas for students and researchers who want to learn more about Google‚Äôs Gemini models and their applications. You can explore the limitless possibilities of AI with Gemini and embark on a journey of innovation and discovery. If you are interested, please check out my repository here: [https://github.com/GitCoder052023/Build-with-Gemini](https://github.com/GitCoder052023/Build-with-Gemini) I would love to hear your feedback and suggestions on how to improve my projects and ideas. Feel free to leave a comment or open an issue on GitHub. Thank you for your time and attention. I hope you enjoy building amazing AI projects with Gemini and Python. üòä	0.0	t3_18wshab	reddit		
51	I made a program that solves mazes from images!	Unknown	2024-01-01 15:41:08	https://www.reddit.com/r/Python/comments/18vz81t/i_made_a_program_that_solves_mazes_from_images/	It was made in Python, except for one file that was written in Cython. You can read more about it here -> [https://github.com/triskj0/maze-solver](https://github.com/triskj0/maze-solver) I'll be glad some of you guys check it out, and maybe even try it for yourself! I am, of course, open to any suggestions on how to continue improving it. Have a nice day, everyone!	3.0	t3_18vz81t	reddit		
52	Hypercorn 0.16.0 released - a WSGI/ASGI server supporting HTTP 1, 2, 3 and Websockets	Unknown	2024-01-01 14:07:49	https://www.reddit.com/r/Python/comments/18vxfyu/hypercorn_0160_released_a_wsgiasgi_server/	Hypercorn is a WSGI and ASGI server that supports HTTP/1, HTTP/2, HTTP/3, and WebSockets. It also supports asyncio, uvloop, and trio worker classes. This release: - Adds ProxyFixMiddleware to make it much easier to run Hypercorn behind a proxy with the headers pointing at the client rather than the proxy. - A max_requests config that forces workers to restart when hit. This helps with memory leaks as the restart frees any leaked memory. - A max keep alive requests config that limits the requests per kept-alive connection. This mitigates the HTTP/2 rapid reset attack in the same manner as Nginx. - Finally fixes many bugs. [Read more](https://github.com/pgjones/hypercorn/blob/main/CHANGELOG.rst).	3.0	t3_18vxfyu	reddit		
53	Hey Guys! Just went looking back at older projects and found this one, its a level editor/maker!	Unknown	2024-01-02 00:47:48	https://www.reddit.com/r/Python/comments/18wbx3k/hey_guys_just_went_looking_back_at_older_projects/	It is a 2d level maker that taught me more about how pygame works, now I use Godot but I really appreciate this project for how much it taught me. Just wanted feedback, I know it looks bad being just colored squares but the code is really were I think I did the best. I dunno I dont code for a living, perhaps its pathetic and worthy of public shamming, let me know what you guys think. [link](https://github.com/GithubUserNotABot/Level-maker/blob/main/main.py)	2.0	t3_18wbx3k	reddit		
54	PDFSyntax, a new Python API library to inspect and update PDF files	Unknown	2024-01-01 20:35:21	https://www.reddit.com/r/Python/comments/18w5zj3/pdfsyntax_a_new_python_api_library_to_inspect_and/	Hi! This is my pet project, written from scratch because there is so much to discover and learn in the process. The focus is on API simplicity and incremental updates (a PDF feature that is often overlooked). Progress is slow because I do not have much spare time to work on this. This ALPHA quality software is far from finished but I would love to hear some feedback and feature requests. Here is the link to the project on GitHub: [https://github.com/desgeeko/pdfsyntax](https://github.com/desgeeko/pdfsyntax) Regards	1.0	t3_18w5zj3	reddit		
55	What is SLOW_SUM in the CPython source code?	Unknown	2024-01-01 11:39:31	https://www.reddit.com/r/Python/comments/18vv3aa/what_is_slow_sum_in_the_cpython_source_code/	File: `Python/bltinmodule.c` ([link to precise line](https://github.com/python/cpython/blob/471aa752415029c508693fa7971076f5148022a6/Python/bltinmodule.c#L2551C9-L2551C17)) While reading CPython's source code I came across the `SLOW_SUM` symbol, but I couldn't find its definition. `SLOW_SUM` is referenced only once in the entire CPython source code, so I couldn't find any information on why it exists. From the source code, I understand that it's a compiler flag that disables an optimization when performing sums on numeric types through the `sum()` built-in function. However, why would you pass `SLOW_SUM` to the compiler to disable optimized sums on numeric types? I don't know if this is the right place to ask such a specific question. If it's not, can you point me to the right forum?	3.0	t3_18vv3aa	reddit		
56	Arezzo: Automatic polyphonic piano music transcription in Python	Unknown	2024-01-01 23:38:59	https://www.reddit.com/r/Python/comments/18wacbi/arezzo_automatic_polyphonic_piano_music/	[https://github.com/Kat9-123/Arezzo](https://github.com/Kat9-123/Arezzo) Through the power of Machine Learning‚Ñ¢ this program can take an audio file of (polyphonic) piano music and generate the corresponding sheet music! The code is dodgy in places, and not very well documented. As this was a school project, I didn't spend as much time as I'd have liked to refine it, because I simply ran out of time and steam. Especially the bits added last are very messy. Still, the UX is great, with a bunch of features easily accessible through a config file and command line switches. This is my first project using ML and audio processing, so that may explain why it lacks in some departments. So does it work? Sure, but not very well. Marginally worse than the free\* options I found online. *testing/results/TEST\_RESULTS\_V1.csv* contains some stats. It does have quite some limitations, as is doesn't recognise rests, tempo changes (like rubato), dynamics, articulations, upbeats and more. These limitations are bad, but not catastrophic. Oh and it actually generates MIDI files and uses MuseScore4 to generate the sheet music PDF's, but it does actually find key, tempo and time signature. \**Not really of course* **Please give feedback! :D**	0.0	t3_18wacbi	reddit		
57	Tuesday Daily Thread: Advanced questions	Unknown	2024-01-02 00:00:09	https://www.reddit.com/r/Python/comments/18watqc/tuesday_daily_thread_advanced_questions/	# Weekly Wednesday Thread: Advanced Questions üêç Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices. ## How it Works: 1. **Ask Away**: Post your advanced Python questions here. 2. **Expert Insights**: Get answers from experienced developers. 3. **Resource Pool**: Share or discover tutorials, articles, and tips. ## Guidelines: * This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday. * Questions that are not advanced may be removed and redirected to the appropriate thread. ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **How can you implement a custom memory allocator in Python?** 2. **What are the best practices for optimizing Cython code for heavy numerical computations?** 3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?** 4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?** 5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?** 6. **What are some advanced use-cases for Python's decorators?** 7. **How can you achieve real-time data streaming in Python with WebSockets?** 8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?** 9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?** 10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)** Let's deepen our Python knowledge together. Happy coding! üåü	0.0	t3_18watqc	reddit		
58	Yet another multidirectory/multirepository git runner.	Unknown	2024-01-01 19:43:45	https://www.reddit.com/r/Python/comments/18w4quj/yet_another_multidirectorymultirepository_git/	Hi, I made this thing for git: [https://github.com/jasursadikov/mud](https://github.com/jasursadikov/mud) This tool allows you to run git commands in multiple repositories simultaneously. I was using other tools like that but mine has some features that I was looking for: 1. Asyncronous commands running 2. Nerd fonts to have some pretty view 3. Status table that shows what is going on with all repos that I have 4. Aliases, so I can safe 0.03125ms by avoiding typing long commands 5. Filtering. This tool can filter repos by branch/tag/modified/diverged 6. Global config and local configs.	1.0	t3_18w4quj	reddit		
59	Get Surfline surf forecasts with python	Unknown	2024-01-01 18:32:14	https://medium.com/@giocaizzi/get-surfline-surf-forecasts-with-python-93fe92230c01	Empty	3.0	t3_18w31mr	reddit		
60	I made a video showcasing the projects in 2023 using Python and Pygame!	Unknown	2024-01-01 09:49:27	https://www.reddit.com/r/Python/comments/18vtkcs/i_made_a_video_showcasing_the_projects_in_2023/	You can watch it here - [https://youtu.be/o6ISmnLqVDQ](https://youtu.be/o6ISmnLqVDQ) Source code for most of the project is available on my [GitHub page](https://github.com/robomarchello) Happy New Year everyone! And this is one of the projects: [pressure soft body simulation](https://i.redd.it/872nmxhrts9c1.gif)	1.0	t3_18vtkcs	reddit		
61	Discover Your Personality Traits with Persai: A Python Package for detecting Big Five personality	Unknown	2024-01-02 17:46:06	https://www.reddit.com/r/Python/comments/18wvb6d/discover_your_personality_traits_with_persai_a/	Greetings everyone, I'm excited to introduce you to Persai, a Python package designed to provide deep insights into your personality. Using the Big Five personality traits model, Persai can analyze your Twitter posts and give you a unique perspective on your personality. All you have to do is input the tweets.js file you get when you export your Twitter data, and Persai will return your Big Five data. The technology behind Persai is GPT-4, and it's based on the findings from the paper ‚ÄúIs ChatGPT a Good Personality Recognizer? A Preliminary Study‚Äù. For more information about Persai, you can check out the GitHub repository here: https://github.com/yachty66/persai To delve deeper into what Persai can do, visit the website: https://www.persai.org/ Let's embark on this journey of self-discovery together! üß≠üíº	2.0	t3_18wvb6d	reddit		
62	URL-Shorter with Python	Unknown	2024-01-01 16:55:06	https://www.reddit.com/r/Python/comments/18w0tw8/urlshorter_with_python/	Hi everyoen, I want to introduce my latest project, URL-Shorter; You can deploy your own url-shorter service with that repository. [https://github.com/uysalserkan/url-shorter](https://github.com/uysalserkan/url-shorter)	0.0	t3_18w0tw8	reddit		
63	chrono24 - a simple API wrapper for watch enthusiasts üïí	Unknown	2024-01-01 04:57:33	https://www.reddit.com/r/Python/comments/18vpdsd/chrono24_a_simple_api_wrapper_for_watch/	"The [Chrono24](https://www.chrono24.com/) [API wrapper](https://github.com/irahorecka/chrono24/) is designed for watch enthusiasts in the Python community. This library offers in-depth access to Chrono24's watch listings. `pip install chrono24`, and explore brands and listings using simple Python commands: import chrono24 for listing in chrono24(query=""Rolex DateJust"").search(): print(listing) Dive into one of the biggest timepiece markets with [chrono24](https://github.com/irahorecka/chrono24) &#x200B; **Edit:** Given the constructive feedback from u/striata, the new API is as follows: import chrono24 for listing in chrono24.query(""Rolex DateJust"").search(): print(listing)"	1.0	t3_18vpdsd	reddit		
64	DocFlow - Document Management API	Unknown	2023-12-31 17:27:51	https://www.reddit.com/r/Python/comments/18vcjrw/docflow_document_management_api/	üöÄ Excited to announce the release of DocFlow - a Document Management API! I have been working on this project from quite some tie now. And learnt a lot. Writing this post, just to share how year ended for me. DocFlow is build using u/FastAPI, PostgreSQL, AWS S3, and Docker. It provides document's Upload, Download, Organization, Searching, Versioning, Sharing, Access Control List, Deletion, Archiving, Authentication and Authorization. The complete documentation of the API and ways to test and run DocFlow is mentioned on the GitHub Repository. üñáÔ∏è [Here](https://github.com/jiisanda/docflow) üì© I invite you to the repo, to do a code review, suggest changes and collaborate over the Discussions of [DocFlow](https://github.com/jiisanda/docflow/discussions). Happy Coding üôÜ‚Äç‚ôÇÔ∏è! **#DocFLow** **#DocumentManagement** **#API** **#release** **#github** **#fastapi** **#aws** **#docker** **#postgresql** **#awsservices** **#python** [DocFlow](https://preview.redd.it/nlqs5ypm0o9c1.png?width=500&format=png&auto=webp&s=bf8aa96aa81771c6208703844c4f9e004d7259e9)	3.0	t3_18vcjrw	reddit		
65	Xmas decoration, part 2	Unknown	2023-12-31 22:02:02	https://www.bitecode.dev/p/xmas-decoration-part-2	Empty	0.0	t3_18vi8wx	reddit		
66	Monday Daily Thread: Project ideas!	Unknown	2024-01-01 00:00:08	https://www.reddit.com/r/Python/comments/18vkgtu/monday_daily_thread_project_ideas/	"# Weekly Thread: Project Ideas üí° Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you. ## How it Works: 1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced. 2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code. 3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration. ## Guidelines: * Clearly state the difficulty level. * Provide a brief description and, if possible, outline the tech stack. * Feel free to link to tutorials or resources that might help. # Example Submissions: ## Project Idea: Chatbot **Difficulty**: Intermediate **Tech Stack**: Python, NLP, Flask/FastAPI/Litestar **Description**: Create a chatbot that can answer FAQs for a website. **Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM) # Project Idea: Weather Dashboard **Difficulty**: Beginner **Tech Stack**: HTML, CSS, JavaScript, API **Description**: Build a dashboard that displays real-time weather information using a weather API. **Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8) ## Project Idea: File Organizer **Difficulty**: Beginner **Tech Stack**: Python, File I/O **Description**: Create a script that organizes files in a directory into sub-folders based on file type. **Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/) Let's help each other grow. Happy coding! üåü"	2.0	t3_18vkgtu	reddit		
67	Stockstir is a tool written in Python that lets you get stock information from any script at no cost - Version 2 is officially out!	Unknown	2023-12-31 00:53:09	https://www.reddit.com/r/Python/comments/18uuyjr/stockstir_is_a_tool_written_in_python_that_lets/	"Hello again! A few days ago I showcased my Stockstir project which I had made a while ago. You can refer to that thread [here](https://www.reddit.com/r/Python/comments/18sxqsc/stockstir_is_a_python_project_that_lets_you_get/). V2 is out! You can take a look at the [documentation](https://stockstir.readthedocs.io/en/latest/index.html) for up-to-date information on the new functions, enhancements, and fixes in the project. Also, the project link is here: [Stockstir Link](https://github.com/PatzEdi/Stockstir) As far as additions and suggestions which were made on the previous thread, Stockstir V2 now has a complete fail-safe system that uses more than one provider. It also has initial integration of an Alpha Vantage API (to be further developed still, now it is just an initial integration), and new options to gather prices and other stock info through CNBC and their JSON format API (Thank you to [Gr1pp717](https://www.reddit.com/user/Gr1pp717/) for that information!). As far as the quick usage, nothing has changed: ``` import Stockstir price = Stockstir.getSinglePrice(""ticker/stockSymbol"") print(price) ``` With the new provider system, the default provider is still CNBC. However, you can set a provider manually (There are three as of now) like so: ``` from Stockstir import Providers Providers.provider_number = 1 # Here, you can put any number that is between 0 and 2, as there are three providers now. The default remains 0. ``` The new fail-safe system automatically picks other providers in case one fails, bringing more reliability to the library as a whole. If you want to manually check if providers are working, you can do this: ``` from Stockstir import Providers Providers.runProviderChecks() ``` Hope you enjoy! Edit: Some suggestions/improvements have already been suggested in one of the comments below, thank you so much for that information, super useful! Here is the [link of the improvements that will come soon to Stockstir V2](https://www.reddit.com/r/Python/comments/18uuyjr/comment/kfnju1d/?utm_source=share&utm_medium=web2x&context=3)"	5.0	t3_18uuyjr	reddit		
68	I Created a game (kind of)	Unknown	2023-12-31 16:07:45	https://www.reddit.com/r/Python/comments/18vattd/i_created_a_game_kind_of/	I created a practice app to practice wordle (more specifically Duotrigordle), and I was wondering if anyone would be interested in playing it/testing it. Here's the github: [https://github.com/dcjvliet/Duotrigordle](https://github.com/dcjvliet/Duotrigordle) The code's pretty messy, but it's all open source and I figured I might as well share it. If you do decide to play it, feel free to leave feedback here.	0.0	t3_18vattd	reddit		
69	Transfer YouTube History from One Channel to Another Channel Using Python	Unknown	2023-12-30 20:54:06	https://www.reddit.com/r/Python/comments/18upgvh/transfer_youtube_history_from_one_channel_to/	*Transfer YouTube History from One Account to Another Account Using Python.* **Information:** There is no direct way to transport YouTube history from one account to another; you have to use Python to do it. It will take some time. - I've created a Python script that automates the transfer for you; it will take 10 seconds for each link. **Advice:** Be aware that you will need to monitor the transport because YouTube will detect unusual traffic, and it will sign out all your logged-in accounts. Therefore, I ADVISE YOU TO SAVE YOUR PASSWORD if you don't know it. **Preparation:** 1. Visual Studio Code 2. Google Chrome browser 3. MS Excel **Exporting youtube history:** 1. Go to [https://takeout.google.com](https://takeout.google.com/) 2. Choose the account (or brand account if you have more than one YouTube channel). 3. Click Deselect all. Check only: YouTube and YouTube Music section (Scroll to the end). 4. Click multiple formats: Scroll to history and change HTML to JSON, hit Ok. 5. Click All YouTube data included: Deselect all and choose only history. 6. Click next step. 7. Create export. 8. It will be sent to your email. 9. Create an empty folder called YoutubeHistory. This folder is for running every Python script that I will provide. 10. Drag and drop the watch-history.json file into the YoutubeHistory folder. **Convert JSON file to txt file using python:** 1. Create python script named: , save it in the YoutubeHistory folder. 2. Load this script and click save: &#8203; import json # Load the JSON data from the file with specified encoding with open('watch-history.json', 'r', encoding='utf-8') as file: data = json.load(file) urls = [] # Extract the URLs from the JSON data for item in data: if 'titleUrl' in item: urls.append(item['titleUrl']) if 'subtitles' in item: for subtitle in item['subtitles']: if 'url' in subtitle: urls.append(subtitle['url']) # Save the URLs to a text file with open('urls.txt', 'w') as file: for url in urls: file.write(url + '\n') Now click Ctrl+F5 to run the script. - You will get urls.txt file in the YoutubeHistory folder. **MS Excel (Organizing the history from old to new and removing duplicates):** First: You will need to remove the duplicates: 1. Copy the links from urls.txt and paste in a new excel work book. 2. CTRL+A to select all. - Click data: Remove duplicates and hit ok. Second: You will need to order the links (Old to New): 1. On column B: Number the links starting from 1. 2. CTRL+A to select all. 3. Click data: Sort. 4. Choose Sort by column B & Order by largest to smallest. Third: 1. Now create txt file and name it Flipped.txt in the YoutubeHistory folder. 2. Copy the links from the excel file and paste them in Flipped.txt **Transferring the history:** 1. Make sure you have selected the correct YouTube channel that you want to transfer to. 2. Open an empty tab in Chrome tab and keep it open. 3. Create python script named: automate\_youtube\_history.py , save it in the YoutubeHistory folder. 4. Load this script and click save: &#8203; import webbrowser import time import pyautogui # Read the URLs from a text file with open('Flipped.txt', 'r') as file: urls = file.readlines() urls = [url.strip() for url in urls] # Open each URL in a web browser for url in urls: webbrowser.open(url) time.sleep(10) # Close the current tab (you might need to adjust the coordinates) pyautogui.hotkey('ctrl', 'w') # This shortcut closes the current tab Now click Ctrl+F5 to run the script. **The process:** I recommend that you monitor the process every hour or 30 minutes because after some time, you may get logged out from all your accounts (this happened to me after 2 hours). If this occurs: 1. Stop the Python script from running. 2. Go to your watch history, and check the last video it stopped on. 3. Remove the links that were transferred successfully and keep the others. 4. Re-run the script.	11.0	t3_18upgvh	reddit		
70	I shared a Python Course (1.5 hours) on YouTube	 	2023-12-31 06:09:27	https://www.youtube.com/watch?v=VOdPQmm298o&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=1	Empty	3.0	t3_18v157k	reddit		
71	BALanced Execution through Natural Activation : a human-computer interaction methodology for code running	Unknown	2023-12-31 10:20:47	https://www.reddit.com/r/Python/comments/18v4yqg/balanced_execution_through_natural_activation_a/	BALENA is a voice interaction framework utilizing state-of-the-art natural language processing and audio processing models to create a system that can interpret voice commands and associate them with predefined actions. The framework leverages the power of transformers and signal processing to understand user intent via spoken language and correlates them with a series of predefined actionable responses. [Framework workflow](https://preview.redd.it/juxyviw4wl9c1.png?width=6130&format=png&auto=webp&s=ce2a4cd2ba32dc062a704a1544a48233319ef432) ## Features * **Real-time audio streaming and recording**: Record audio from the microphone in real time for processing. * **Speech recognition with Wav2Vec 2.0**: Use a pre-trained Wav2Vec 2.0 model to convert speech to text. * **Text similarity and action triggering**: Encode the transcribed text to a vector space and find the closest action using sentence similarity techniques. * **High-pass filtering**: Process the audio signal with a high-pass filter to enhance signal quality. * **Auto-correction**: Utilize the Jaccard distance to correct words in the transcribed text auto-magically. * **Framework flexibility**: Support for different device execution contexts, allowing for usage on both CPU and CUDA devices. Link to the repo : [https://github.com/louisbrulenaudet/balena](https://github.com/louisbrulenaudet/balena)	0.0	t3_18v4yqg	reddit		
72	RecoverPy 2.1.5: Python file recovery tool	Unknown	2023-12-30 17:25:15	https://www.reddit.com/r/Python/comments/18uknqm/recoverpy_215_python_file_recovery_tool/	&#x200B; https://i.redd.it/t1foxzpbvg9c1.gif **Github**: [https://github.com/PabloLec/RecoverPy](https://github.com/PabloLec/RecoverPy) Hey everyone! I'm here to share something I've been working on for nearly three years now, RecoverPy, and its new 2.1.5 version. It's a nifty tool that can really be a lifesaver when you've accidentally deleted or overwritten files. It works its magic by conducting a text-based search to find the lost data. It sports a TUI built with Textual. I found it to be quite enjoyable to use and it seems many others agree, given its rise as one of the most (or the most?) popular TUI libraries in Python, despite still being in beta. Since its creation, RecoverPy has gone through quite a transformation. It's integrated lots of feedback from its user community, improved many aspects to enhance the user experience, and even underwent almost a full rewrite to switch up the TUI library in its second version. Essentially, it uses the strength of grep and dd to sift through partition blocks, giving you a user-friendly way to sift through the results. Interestingly, it found a niche not only among individuals looking to recover files but has also piqued interest in the hacking scene, which was a bit of a pleasant surprise for me. It seems the tool lends itself well to that sphere too. I manage to chip away at it from time to time, given that my free moments are becoming a bit of a rarity these days. It still has room to grow, and if anyone here feels like contributing, I'm more than open to collaborations. Your PRs would certainly be welcome! Feel free to give it a glance, and if you find it interesting or useful, a star on the repository would be greatly appreciated.	2.0	t3_18uknqm	reddit		
73	A small collection of lesser-known statistical functions - obscure_stats	Unknown	2023-12-30 09:17:56	https://www.reddit.com/r/Python/comments/18ubr4s/a_small_collection_of_lesserknown_statistical/	Hello r/Python I‚Äôm excited to share with you my new python package called `obscure_stats`. It is a collection of lesser-known statistical functions that are not available in the standard libraries like `scipy`, `statsmodels`, or `numpy`. The package is still in development, but I hope you will find it useful and interesting. You can install it with `pip install obscure_stats` or check out the source code on [GitHub](https://github.com/glevv/obscure_stats). I would appreciate any feedback, suggestions, or bug reports.	4.0	t3_18ubr4s	reddit		
74	Paracelsus: Visualize SQLAlchemy Databases using Mermaid or Dot Diagrams.	Unknown	2023-12-30 14:43:34	https://github.com/tedivm/paracelsus	Empty	0.0	t3_18uh43x	reddit		
75	My first public package: wigner-symbols	Unknown	2023-12-31 02:39:08	https://www.reddit.com/r/Python/comments/18ux6et/my_first_public_package_wignersymbols/	Hi everyone! I would like to share my first public repository with you all: [wigner-symbols](https://github.com/sheodun/wigner-symbols). This package just calculates Wigner [3j](https://en.wikipedia.org/wiki/3-j_symbol) and [6j](https://en.wikipedia.org/wiki/6-j_symbol) symbols, which are commonly used in quantum mechanics for calculating coefficients of angular momentum coupling. I come from a research background and often relied on a [very good site](https://www-stone.ch.cam.ac.uk/wigner.shtml) to check these symbol values, but I would rather use a python package; so I decided to make my own. It is a small repo and there isn't much here at the moment but it performs the job. I would really appreciate your feedback. Thank you! Edit: I have not been able to publish the package on PyPi yet as registration is temporarily suspended but I plan to once we are able to again	1.0	t3_18ux6et	reddit		
76	The Python Mega Course is still free on Udemy	Unknown	2023-12-29 13:34:26	https://www.reddit.com/r/Python/comments/18tn8y0/the_python_mega_course_is_still_free_on_udemy/	"As some of you may know, ""**The Python Mega Course: Build 10 Real World Applications**"" is one of the top Python courses on Udemy. Last year, I made that version of the course available for free to the Reddit community, and I am doing the same today. In 2023, the course attracted 20,000+ students and collected 900+ reviews, achieving an exceptionally high average rating of 4.8/5 on Udemy. This makes the course exceptionally highly rated on Udemy. **How can you get the course for free today?** Three simple steps: 1. Login to Udemy. 2. Go to the course page: [https://udemy.com/course/former-python-mega-course-build-10-real-world-applications/](https://udemy.com/course/former-python-mega-course-build-10-real-world-applications/) 3. Enter the password **mega\_course** to get the course for free. Thanks and have a relaxing end of the year!"	107.0	t3_18tn8y0	reddit		
77	ChatGPT API Basics for Developers (in Python)	Unknown	2023-12-31 09:04:20	https://www.tomaspanik.eu/en/posts/chatgpt-api/	Empty	0.0	t3_18v3v7n	reddit		
78	Grep over IPython output!	Unknown	2023-12-30 13:17:40	https://www.reddit.com/r/Python/comments/18ufgl3/grep_over_ipython_output/	Hi there! i just finish a little project allowing you to use `grep` over IPython output like so: ```ipython In [1]: {i:i for i in range(3)} Out[1]: {0: 0, 1: 1, 2: 2, } In [2]: %greps 1 Out[2]: ' 1: 1,\n' ``` [https://github.com/royreznik/greps](https://github.com/royreznik/greps)	1.0	t3_18ufgl3	reddit		
79	CPython Type System Internals: Video Series	Unknown	2023-12-30 14:51:58	https://codeconfessions.substack.com/p/cpython-type-system-internals-video	Empty	0.0	t3_18uhack	reddit		
80	A Free AI Scribe Project I am Working on! Please Provide Feedback!	Unknown	2023-12-30 16:26:16	https://www.reddit.com/r/Python/comments/18ujbet/a_free_ai_scribe_project_i_am_working_on_please/	Thought I would share a project that I have been working on. AI medical scribe products have popped up everywhere but have been very expensive to deploy. I wrote a program that can connect with a local server running a version of ChatGPT and Speech-To-Text that can take a conversation via microphone and create a SOAP note. You can turn off the AI scribe and use it in a normal chat-based manner. The LLM variables are locked in on the executable option since I wrote this for an end-user physician. Looking for any feedback (I am very much an amateur) from the community! What proved to be a bit tricky was developing a client that could use the device's microphone. [https://github.com/1984Doc/AI-Scribe](https://github.com/1984Doc/AI-Scribe)	0.0	t3_18ujbet	reddit		
81	Less is More? An Empirical Study on Configuration Issues in Python PyPI Ecosystem	Unknown	2023-12-30 09:19:48	https://www.reddit.com/r/Python/comments/18ubs0y/less_is_more_an_empirical_study_on_configuration/	The utilization of third-party libraries can potentially lead to conflicts in dependencies, prompting researchers to develop dependency conflict detectors. Specifically, the researchers propose PyCon, a source-level detector, for detecting potential configuration issues. [https://arxiv.org/abs/2310.12598](https://arxiv.org/abs/2310.12598) &#x200B;	2.0	t3_18ubs0y	reddit		
82	UdemyPy - The Free Udemy courses bot	Unknown	2023-12-30 15:33:16	https://www.reddit.com/r/Python/comments/18ui5nc/udemypy_the_free_udemy_courses_bot/	UdemyPy is an open-source Python project with the mission of making education accessible to everybody. It brings free Udemy courses directly to you on [WhatsApp](https://www.whatsapp.com/channel/0029VaHwvWZ7NoZsk8UOUl0z) and [Telegram](https://t.me/freecourses000). # How it Works UdemyPy scours the web for Udemy courses offering a 100% discount, ensuring you have access to a diverse range of subjects in any language. When looking for free courses shared by UdemyPy, remember to check the offer time left. Once it‚Äôs expired, the course will no longer be free. Whenever you find a course you like, click on the link and enroll in it. Despite the fact that courses are free only for a limited amount of time, once you are enrolled *they will be yours forever!* # Why UdemyPy? * Free of Charge: No payment methods needed; just create a Udemy account. * Unrestricted Learning: Explore courses in any category or language. * Supportive: UdemyPy supports content creators in reaching a wider audience. # Learn More Explore the project on [GitHub](https://github.com/dylannalex/udemypy). Feel free to show your support with a üåü if you find the project intriguing!	0.0	t3_18ui5nc	reddit		
83	Coding year in review 2023!	Unknown	2023-12-30 14:56:19	https://www.reddit.com/r/Python/comments/18uhdfv/coding_year_in_review_2023/	Love to hear your thoughts on last year with coding and coming year. Here's to our first script of 2024! https://www.youtube.com/watch?v=YcmKs2M1xAo	1.0	t3_18uhdfv	reddit		
84	Curser, and other AI code editors	Unknown	2023-12-30 13:24:15	https://www.reddit.com/r/Python/comments/18ufktx/curser_and_other_ai_code_editors/	Do you use these types of tool editors to help write code more effectively/efficiently? What are you using?	3.0	t3_18ufktx	reddit		
85	How to prevent python software from being reverse engineered or pirated?	Unknown	2023-12-29 03:56:54	https://www.reddit.com/r/Python/comments/18tdmiv/how_to_prevent_python_software_from_being_reverse/	I have a program on the internet that users pay to download and use. I'm thinking about adding a free trial, but I'm very concerned that users can simply download the trial and bypass the restrictions. The program is fully offline and somewhat simple. It's not like you need an entire team to crack it. In fact, there is literally a pyinstaller unpacker out there that can revert the EXE straight back to its python source code. I use pyinstaller. Anything I can do? One thing to look out for is unpackers, and the other thing is how to make it difficult for Ghidra for example to reverse the program. Edit: to clarify, I can't just offer this as an online service/program because it requires interaction with the user's system.	89.0	t3_18tdmiv	reddit		
86	CRAP - Clear Redundant Added Packages	Unknown	2023-12-29 16:20:45	https://www.reddit.com/r/Python/comments/18tqu9m/crap_clear_redundant_added_packages/	[https://github.com/ValdonVitija/crap](https://github.com/ValdonVitija/crap) Automatically clear redundant packages from virtual environments in python üêçüì¶üóëÔ∏è.	7.0	t3_18tqu9m	reddit		
87	Jake: A Free Alternative to Linktree Using GitHub Pages	Unknown	2023-12-29 10:47:28	https://www.reddit.com/r/Python/comments/18tkf7e/jake_a_free_alternative_to_linktree_using_github/	"Hello, I wanted to share a new Python project I've been working on called Jake. It's an alternative to popular link aggregator services like Linktree and OneLink. Jake leverages the power of GitHub Pages to provide you with a hassle-free way to create your one-link website. The best part? It won't cost you a dime! With Jake, you can easily showcase all your important links and content in one central hub, neatly organized and easily accessible. Your website will have a sleek URL in the format of ""username.github.io,"" giving it a professional touch. Jake is completely written in Python and uses the \`tinyhtml\` library to generate static HTML websites. Simply fill in the \`data.toml\` file with your information, and Jake will automatically build and deploy your website to GitHub Pages using a GitHub action. To give you a taste of what Jake can do, I've prepared a demo project for you to explore. Just visit [https://thevahidal.github.io/jake](https://thevahidal.github.io/jake) and see the potential for yourself. If you're interested in contributing or want to dive deeper into the project, you can find the Jake repository on GitHub at [https://github.com/thevahidal/jake](https://github.com/thevahidal/jake). I welcome all contributions, feedback, and bug reports. Your input will help shape the future of Jake and make it even better. Thank you for taking the time to read about Jake. I can't wait to see what we can achieve together. Best regards, Al"	9.0	t3_18tkf7e	reddit		
88	Tastymap, create/customize matplotlib color palettes for your palate	Unknown	2023-12-29 19:51:02	https://www.reddit.com/r/Python/comments/18tvpdz/tastymap_createcustomize_matplotlib_color/	"The number of colormaps matplotlib felt limiting to me so I created a web app and Python package to customize existing colormaps or start from scratch! from tastymap import cook_tmap tmap = cook_tmap( [""red"", ""green"", ""blue""], num_colors=256, reverse=True ) Install by: \`pip install tastymap\` or try it online here: [TastyKitchen - a Hugging Face Space by ahuang11](https://huggingface.co/spaces/ahuang11/tastykitchen) [TastyKitchen](https://i.redd.it/hf6fm1ukga9c1.gif) Docs here: [TastyMap (ahuang11.github.io)](https://ahuang11.github.io/tastymap/) Code here: [ahuang11/tastymap: colormaps cooked for your palate (github.com)](https://github.com/ahuang11/tastymap) There's also a way to have AI suggest a colormap based on a description: from tastymap import ai tmap = ai.suggest_tmap(""Pikachu"") tmap [Pikachu](https://preview.redd.it/3clppe0pga9c1.png?width=512&format=png&auto=webp&s=82b5570b05cd3b1f07683a7b837860d4442ce9ff)"	0.0	t3_18tvpdz	reddit		
89	Understanding Numeric Data Types in Python	Unknown	2023-12-29 19:18:05	https://fullspeedpython.com/articles/understanding-numeric-data-types/	Empty	0.0	t3_18tuy3y	reddit		
90	UniDep: Unified Conda and Pip dependency management via pyproject.toml	Unknown	2023-12-29 20:50:55	https://www.reddit.com/r/Python/comments/18tx3pm/unidep_unified_conda_and_pip_dependency/	"[UniDep](https://github.com/basnijholt/unidep) streamlines Python project dependency management by unifying Conda and Pip packages in a single system. Handling dependencies in Python projects can be challenging, especially when juggling Python and non-Python packages.This often leads to confusion and inefficiency, as developers juggle between multiple dependency files. - **üìù Unified Dependency File**: Use either `requirements.yaml` or `pyproject.toml` to manage both Conda and Pip dependencies in one place. - **‚öôÔ∏è Build System Integration**: Integrates with Setuptools and Hatchling for automatic dependency handling during `pip install ./your-package`. - **üíª One-Command Installation**: `unidep install` handles Conda, Pip, and local dependencies effortlessly. - **üè¢ Monorepo-Friendly**: Render (multiple) `requirements.yaml` or `pyproject.toml` files into one Conda `environment.yaml` file and maintain fully consistent global *and* per sub package `conda-lock` files. - **üåç Platform-Specific Support**: Specify dependencies for different operating systems or architectures. - **üîß `pip-compile` Integration**: Generate fully pinned `requirements.txt` files from `requirements.yaml` or `pyproject.toml` files using `pip-compile`. - **üîí Integration with `conda-lock`**: Generate fully pinned `conda-lock.yml` files from (multiple) `requirements.yaml` or `pyproject.toml` file(s), leveraging `conda-lock`. ### Example #### Example `requirements.yaml` Example of a `requirements.yaml` file: ```yaml name: example_environment channels: - conda-forge dependencies: - numpy # same name on conda and pip - conda: python-graphviz # When names differ between Conda and Pip pip: graphviz - pip: slurm-usage >=1.1.0,<2 # pip-only - conda: mumps # conda-only # Use platform selectors - conda: cuda-toolkit =11.8 # [linux64] local_dependencies: - ../other-project-using-unidep # include other projects that use unidep - ../common-requirements.yaml # include other requirements.yaml files - ../project-not-managed-by-unidep # üö® Skips its dependencies! platforms: # (Optional) specify platforms that are supported (used in conda-lock) - linux-64 - osx-arm64 ``` > `unidep` can process this during `pip install` and create a Conda installable `environment.yaml` or `conda-lock.yml` file, and more! > For a more in-depth example containing multiple installable projects, see the [`example`](https://github.com/basnijholt/unidep/tree/main/example) directory. #### Example `pyproject.toml` ***Alternatively***, one can fully configure the dependencies in the `pyproject.toml` file in the `[tool.unidep]` section: ```toml [tool.unidep] channels = [""conda-forge""] dependencies = [ ""numpy"", # same name on conda and pip { conda = ""python-graphviz"", pip = ""graphviz"" }, # When names differ between Conda and Pip { pip = ""slurm-usage >=1.1.0,<2"" }, # pip-only { conda = ""mumps"" }, # conda-only { conda = ""cuda-toolkit =11.8:linux64"" } # Use platform selectors by appending `:linux64` ] local_dependencies = [ ""../other-project-using-unidep"", # include other projects that use unidep ""../common-requirements.yaml"" # include other requirements.yaml files ""../project-not-managed-by-unidep"" # üö® Skips its dependencies! ] platforms = [ # (Optional) specify platforms that are supported (used in conda-lock) ""linux-64"", ""osx-arm64"" ] ``` This data structure is *identical* to the `requirements.yaml` format, with the exception of the `name` field and the [platform selectors](#platform-selectors). In the `requirements.yaml` file, one can use e.g., `# [linux64]`, which in the `pyproject.toml` file is `:linux64` at the end of the package name. Check out https://github.com/basnijholt/unidep"	1.0	t3_18tx3pm	reddit		
91	Voicebox: Python TTS lib with built-in audio effects	Unknown	2023-12-29 19:27:39	https://www.reddit.com/r/Python/comments/18tv650/voicebox_python_tts_lib_with_builtin_audio_effects/	"Hello there, I'm sharing a project I've been working on for some future robotics projects, and would appreciate some feedback. It's called [Voicebox](https://voicebox.readthedocs.io) ([GitHub](https://github.com/austin-bowen/voicebox)), and it's a Python library that essentially provides wrappers for a bunch of different text-to-speech programs/APIs, and includes lots of built-in audio effects like vocoder, ring mod, glitch, etc. It also includes [example voices](https://voicebox.readthedocs.io/en/stable/voicebox.examples.html) like Star Wars battle droid, GlaDOS, and 343 Guilty Spark. Audio samples [here](https://voicebox.readthedocs.io/en/stable/samples.html). The ""problem"" I was trying to solve was that a lot of TTS programs sound *too* realistic now, and I want an easy way to make audio from TTS sound more fun/robotic. There are also utilities like [`reliable_tts`](https://voicebox.readthedocs.io/en/stable/voicebox.html#voicebox.utils.reliable_tts) and [`ParallelVoicebox`](https://voicebox.readthedocs.io/en/stable/voicebox.voiceboxes.html#voicebox.voiceboxes.parallel.ParallelVoicebox) that make it easy to build responsive and robust TTS systems, which is important for robot projects. LMK what you think! &#x200B; Example: Use gTTS with a vocoder effect to speak in a robotic voice from voicebox import SimpleVoicebox from voicebox.tts import gTTS from voicebox.effects import Vocoder, Normalize voicebox = SimpleVoicebox( tts=gTTS(), effects=[Vocoder.build(), Normalize()], ) voicebox.say('Hello, world! How are you today?')"	0.0	t3_18tv650	reddit		
92	A Python monorepo template	Unknown	2023-12-29 16:55:20	https://www.reddit.com/r/Python/comments/18trmnq/a_python_monorepo_template/	Hey all! üëã Just wanted to share a Python monorepo template I've been working on. It's designed to streamline managing multiple packages in one place. I've incorporated tools like Poetry, Black, mypy, and Ruff for setup and linting, plus my personal touch with [Breadcrumbs](https://github.com/niqodea/breadcrumbs) for cleaner path management. Happy to hear your feedback or ideas! Check it out here: https://github.com/niqodea/python-monorepo	1.0	t3_18trmnq	reddit		
93	A Python implementation of Conway's Game of Life (Cellular Automaton)	Unknown	2023-12-29 15:56:03	https://www.reddit.com/r/Python/comments/18tq9ns/a_python_implementation_of_conways_game_of_life/	I am a student of both mathematics and computer science, so I coded up a terminal-based implementation of John Conway's Game of Life. Check it out here: [https://github.com/atiumcache/game\_of\_life](https://github.com/atiumcache/game_of_life) It is packaged as an executable for UNIX, so you can quickly get it playing. Or, just view the demo on the README.	1.0	t3_18tq9ns	reddit		
94	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2023-12-30 00:00:19	https://www.reddit.com/r/Python/comments/18u1fep/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	1.0	t3_18u1fep	reddit		
95	Pure Recipe is a CLI app to save or view online recipes in well-formatted markdown. No more ads!	Unknown	2023-12-28 22:51:15	https://www.reddit.com/r/Python/comments/18t726b/pure_recipe_is_a_cli_app_to_save_or_view_online/	I am a long-time cook and aspiring developer, so I made a command-line recipe viewer to bypass the ads and blogs that plague recipe websites. It can also save the recipes to markdown. You can even pass in a whole list of URLs to save a bunch of recipes at once. Similar to Paprika, except it is free/open-source and you can easily save and share the recipes in markdown format. Check it out on GitHub, I would appreciate any feedback/testers: [https://github.com/atiumcache/pure-recipe](https://github.com/atiumcache/pure-recipe)	9.0	t3_18t726b	reddit		
96	attrs iv: Zero-overhead Frozen attrs Classes	Unknown	2023-12-29 11:49:19	https://threeofwands.com/attra-iv-zero-overhead-frozen-attrs-classes/	Empty	0.0	t3_18tleqp	reddit		
97	Oreiller: An image library for easy Pillow manipulations.	 Python&OpenSource	2023-12-29 08:44:43	https://www.reddit.com/r/Python/comments/18til8l/oreiller_an_image_library_for_easy_pillow/	Really, [oreiller](https://www.google.com/search?q=oreiller) is the french word for a pillow. I always heard about the PIL fork, Pillow but never used it. When I finally used it, I found it to be tedious sometimes. Like including emojis in text and the general programming. I finally got around to create a small library WIP called oreiller \[[pypi](https://pypi.org/project/oreiller/) | [github](https://github.com/Abdur-RahmaanJ/oreiller)\]. &#x200B; Code demo &#x200B; https://preview.redd.it/r8ixcd8f579c1.png?width=682&format=png&auto=webp&s=3f137f150c3c1482bd38f5d1d8005fd56057e368	1.0	t3_18til8l	reddit		
98	I created a program to align thousands of selfies for daily picture videos!	Unknown	2023-12-29 02:08:34	https://www.reddit.com/r/Python/comments/18tbeet/i_created_a_program_to_align_thousands_of_selfies/	I started taking pictures 'everyday' in 2019 after seeing [Hugo's famous video](https://www.youtube.com/watch?v=65nfbW-27ps), but after aligning \~40 pictures I knew the process had to be automated. Since I barely knew python at the time, and I still find myself learning more and more everyday, it's taken a lot of on and off work, but now I have a script that can do what would've taken years of consistent effort in a few minutes. Even though another solution *kinda* exists, I'm super proud of it because it's mine (I say kinda because I couldn't get it to work for me). Here's the github repo with a lot of details on how it works: [https://github.com/Noah6544/Daily-Picture-Aligner](https://github.com/Noah6544/Daily-Picture-Aligner) Here's my video explanation: [https://www.youtube.com/watch?v=\_ow6GLv7VSA&](https://www.youtube.com/watch?v=_ow6GLv7VSA&) Please let me know ***any*** feedback you have!	1.0	t3_18tbeet	reddit		
99	A Better Way to Wrangle Figures Out of Jupyter Notebooks	Unknown	2023-12-29 07:24:08	https://www.reddit.com/r/Python/comments/18thcub/a_better_way_to_wrangle_figures_out_of_jupyter/	"*Stop wasting time saving plots manually ‚Äî automate it with an extra line of code!* Hopping in to share a bit of Python that's been in my everyday workflow for the last 2 years. Finally decided it would be worth the lift to put out there for others to use, too. I always get bogged down naming things --- and **saving visualizations out of notebooks after finishing up analysis work** is a particular sore spot. So, I wrote a one-off tool to use plotting arguments to automatically name plot outputs. It ended up getting reused over and over, and then eventually became *teeplot.* *teeplot* wraps plotting calls with logic that **automatically manages matplotlib file output**, picking **meaningful file names** based on the plotting function and semantic plotting variables. # Example This example shows a call to *seaborn*'s **lmplot** dispatched through **teeplot.tee** to save out the visualization as '*teeplots/col=time+hue=sex+viz=lmplot+x=total-bill+y=tip+ext={.pdf,.png}'.* Here's what a *teeplot*'ed notebook cell and output look like, # adapted from seaborn.pydata.org/generated/seaborn.FacetGrid.html import seaborn as sns from teeplot import teeplot as tp tp.tee(sns.lmplot, # plotter, then forwarded args/kwargs sns.load_dataset(""tips""), col=""time"", hue=""sex"", x=""total_bill"", y=""tip"") https://preview.redd.it/sj6f6u69q69c1.png?width=5880&format=png&auto=webp&s=4c684e13bd05336f710545298fb3ff436ffa02f1 >teeplots/col=time+hue=sex+viz=lmplot+x=total-bill+y=tip+ext=.pdfteeplots/col=time+hue=sex+viz=lmplot+x=total-bill+y=tip+ext=.png The idea here is to make the process of saving and cataloging plots more *efficient, systematic, and meaningful*, taking the hassle out of manual file management. # Further Information *teeplot* can be installed as python3 -m pip install teeplot The library has additional advanced features, as well, including an interface to globally configure visualization output file types (i.e., "".pdf"", "".png""), etc. You can read more in the project's [*usage guide*](https://github.com/mmore500/teeplot/blob/master/README.rst#usage) and [*API listing*](https://github.com/mmore500/teeplot/blob/master/README.rst#api). *disclaimer*: am library author"	0.0	t3_18thcub	reddit		
100	Egg-smol Python: A Pythonic Library for E-graphs	Saul Shanabrook	2023-05-07 15:35:17	http://arxiv.org/abs/2305.04311v1	E-graphs have emerged as a versatile data structure with applications in synthesis, optimization, and verification through techniques such as equality saturation. This paper introduces Python bindings for the experimental egg-smol library, which aims to bring the benefits of e-graphs to the Python ecosystem. The bindings offer a high-level, Pythonic API providing an accessible and familiar interface for Python users. By integrating e-graph techniques with Python, we hope to enable collaboration and innovation across various domains in the scientific computing and machine learning communities. We discuss the advantages of using Python bindings for both Python and existing egg-smol users, as well as possible future directions for development.			arxiv	[]	0.0
101	Making Python Code Idiomatic by Automatic Refactoring Non-Idiomatic Python Code with Pythonic Idioms	Zejun Zhang	2022-07-12 15:30:46	http://arxiv.org/abs/2207.05613v1	Compared to other programming languages (e.g., Java), Python has more idioms to make Python code concise and efficient. Although pythonic idioms are well accepted in the Python community, Python programmers are often faced with many challenges in using them, for example, being unaware of certain pythonic idioms or do not know how to use them properly. Based on an analysis of 7,638 Python repositories on GitHub, we find that non-idiomatic Python code that can be implemented with pythonic idioms occurs frequently and widely. Unfortunately, there is no tool for automatically refactoring such non-idiomatic code into idiomatic code. In this paper, we design and implement an automatic refactoring tool to make Python code idiomatic. We identify nine pythonic idioms by systematically contrasting the abstract syntax grammar of Python and Java. Then we define the syntactic patterns for detecting non-idiomatic code for each pythonic idiom. Finally, we devise atomic AST-rewriting operations and refactoring steps to refactor non-idiomatic code into idiomatic code. We test and review over 4,115 refactorings applied to 1,065 Python projects from GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to 84 projects. These evaluations confirm the high-accuracy, practicality and usefulness of our refactoring tool on real-world Python code. Our refactoring tool can be accessed at 47.242.131.128:5000.			arxiv	['Zhenchang Xing', 'Xin Xia', 'Xiwei Xu', 'Liming Zhu']	1.0
102	Modern Python at the Large Synoptic Survey Telescope	Tim Jenness	2017-12-01 19:04:46	http://arxiv.org/abs/1712.00461v1	The LSST software systems make extensive use of Python, with almost all of it initially being developed solely in Python 2. Since LSST will be commissioned when Python 2 is end-of-lifed it is critical that we have all our code support Python 3 before commissioning begins. Over the past year we have made significant progress in migrating the bulk of the code from the Data Management system onto Python 3. This paper presents our migration methodology, and the current status of the port, with our eventual aim to be running completely on Python 3 by early 2018. We also discuss recent modernizations to our Python codebase.			arxiv	[]	2.0
103	Python GUI Scripting Interface for Running Atomic Physics Applications	Amani Tahat	2011-06-05 01:11:08	http://arxiv.org/abs/1106.0868v1	We create a Python GUI scripting interface working under Windows in addition to (UNIX/Linux). The GUI has been built around the Python open-source programming language. We use the Python's GUI library that so called Python Mega Widgets (PMW) and based on Tkinter Python module (http://www.freenetpages.co.uk/hp/alan.gauld/tutgui.htm). The new GUI was motivated primarily by the desire of more updated operations, more flexibility incorporating future and current improvements in producing atomic data. Furthermore it will be useful for a variety of applications of atomic physics, plasma physics and astrophysics and will help in calculating various atomic properties.			arxiv	['Mofleh Tahat']	3.0
104	Towards Memory Safe Python Enclave for Security Sensitive Computation	Huibo Wang	2020-05-12 18:19:08	http://arxiv.org/abs/2005.05996v1	Intel SGX Guard eXtensions (SGX), a hardware-supported trusted execution environment (TEE), is designed to protect security-sensitive applications. However, since enclave applications are developed with memory unsafe languages such as C/C++, traditional memory corruption is not eliminated in SGX. Rust-SGX is the first toolkit providing enclave developers with a memory-language. However, Rust is considered a Systems language and has become the right choice for concurrent applications and web browsers. Many application domains such as Big Data, Machine Learning, Robotics, Computer Vision are more commonly developed in the python programming language. Therefore, Python application developers cannot benefit from secure enclaves like Intel SGX and rust-SGX. To fill this gap, we propose Python-SGX, which is a memory-safe SGX SDK providing enclave developers a memory-safe Python development environment. The key idea is to enable memory-safe Python language in SGX by solving the following key challenges: (1) defining a memory-safe Python interpreter (2)replacing unsafe elements of Python interpreter with safe ones,(3) achieving comparable performance to non-enclave Python applications, and (4) not introducing any unsafe new code or libraries into SGX. We propose to build Python-SGX with PyPy, a Python interpreter written by RPython, which is a subset of Python, and tame unsafe parts in PyPy by formal verification, security hardening, and memory safe language. We have implemented python-SGX and tested it with a series of benchmarks programs. Our evaluation results show that Python-SGX does not cause significant overhead.			arxiv	['Mingshen Sun', 'Qian Feng', 'Pei Wang', 'Tongxin Li', 'Yu Ding']	4.0
105	Porting the LSST Data Management Pipeline Software to Python 3	Tim Jenness	2016-11-02 19:48:34	http://arxiv.org/abs/1611.00751v1	The LSST data management science pipelines software consists of more than 100,000 lines of Python 2 code. LSST operations will begin after support for Python 2 has been dropped by the Python community in 2020, and we must therefore plan to migrate the codebase to Python 3. During the transition period we must also support our community of active Python 2 users and this complicates the porting significantly. We have decided to use the Python future package as the basis for our port to enable support for Python 2 and Python 3 simultaneously, whilst developing with a mindset more suited to Python 3. In this paper we report on the current status of the port and the difficulties that have been encountered.			arxiv	[]	5.0
106	A general approach for running Python codes in OpenFOAM using an embedded pybind11 Python interpreter	Simon Rodriguez	2022-03-30 15:25:03	http://arxiv.org/abs/2203.16394v1	As the overlap between traditional computational mechanics and machine learning grows, there is an increasing demand for straight-forward approaches to interface Python-based procedures with C++-based OpenFOAM. This article introduces one such general methodology, allowing the execution of Python code directly within an OpenFOAM solver without the need for Python code translation. The proposed approach is based on the lightweight library pybind11, where OpenFOAM data is transferred to an embedded Python interpreter for manipulation, and results are returned as needed. Following a review of related approaches, the article describes the approach, with a particular focus on data transfer between Python and OpenFOAM, executing Python scripts and functions, and practical details about the implementation in OpenFOAM. Three complementary test cases are presented to highlight the functionality and demonstrate the effect of different data transfer approaches: a Python-based velocity profile boundary condition; a Python-based solver for prototyping; and a machine learning mechanical constitutive law class for solids4foam which performs field calculations.			arxiv	['Philip Cardiff']	6.0
107	Python for education: the exact cover problem	Andrzej Kapanowski	2010-10-28 08:53:26	http://arxiv.org/abs/1010.5890v1	Python implementation of Algorithm X by Knuth is presented. Algorithm X finds all solutions to the exact cover problem. The exemplary results for pentominoes, Latin squares and Sudoku are given.			arxiv	[]	7.0
108	Teddy: Automatic Recommendation of Pythonic Idiom Usage For Pull-Based Software Projects	Purit Phan-udom	2020-09-05 12:54:57	http://arxiv.org/abs/2009.03302v1	Pythonic code is idiomatic code that follows guiding principles and practices within the Python community. Offering performance and readability benefits, Pythonic code is claimed to be widely adopted by experienced Python developers, but can be a learning curve to novice programmers. To aid with Pythonic learning, we create an automated tool, called Teddy, that can help checking the Pythonic idiom usage. The tool offers a prevention mode with Just-In-Time analysis to recommend the use of Pythonic idiom during code review and a detection mode with historical analysis to run a thorough scan of idiomatic and non-idiomatic code. In this paper, we first describe our tool and an evaluation of its performance. Furthermore, we present a case study that demonstrates how to use Teddy in a real-life scenario on an Open Source project. An evaluation shows that Teddy has high precision for detecting Pythonic idiom and non-Pythonic code. Using interactive visualizations, we demonstrate how novice programmers can navigate and identify Pythonic idiom and non-Pythonic code in their projects. Our video demo with the full interactive visualizations is available at https://youtu.be/vOCQReSvBxA.			arxiv	['Naruedon Wattanakul', 'Tattiya Sakulniwat', 'Chaiyong Ragkhitwetsagul', 'Thanwadee Sunetnanta', 'Morakot Choetkiertikul', 'Raula Gaikovina Kula']	8.0
109	Machine Learning using Stata/Python	Giovanni Cerulli	2021-03-03 10:31:44	http://arxiv.org/abs/2103.03122v1	We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting popular Machine Learning (ML) methods both in regression and classification settings. Using the recent Stata/Python integration platform (sfi) of Stata 16, these commands provide hyper-parameters' optimal tuning via K-fold cross-validation using greed search. More specifically, they make use of the Python Scikit-learn API to carry out both cross-validation and outcome/label prediction.			arxiv	[]	9.0
110	Using Python for Model Inference in Deep Learning	Zachary DeVito	2021-04-01 04:48:52	http://arxiv.org/abs/2104.00254v1	Python has become the de-facto language for training deep neural networks, coupling a large suite of scientific computing libraries with efficient libraries for tensor computation such as PyTorch or TensorFlow. However, when models are used for inference they are typically extracted from Python as TensorFlow graphs or TorchScript programs in order to meet performance and packaging constraints. The extraction process can be time consuming, impeding fast prototyping. We show how it is possible to meet these performance and packaging constraints while performing inference in Python. In particular, we present a way of using multiple Python interpreters within a single process to achieve scalable inference and describe a new container format for models that contains both native Python code and data. This approach simplifies the model deployment story by eliminating the model extraction step, and makes it easier to integrate existing performance-enhancing Python libraries. We evaluate our design on a suite of popular PyTorch models on Github, showing how they can be packaged in our inference format, and comparing their performance to TorchScript. For larger models, our packaged Python models perform the same as TorchScript, and for smaller models where there is some Python overhead, our multi-interpreter approach ensures inference is still scalable.			arxiv	['Jason Ansel', 'Will Constable', 'Michael Suo', 'Ailing Zhang', 'Kim Hazelwood']	10.0
111	Python Type Hints are Turing Complete	Ori Roth	2022-08-31 10:11:42	http://arxiv.org/abs/2208.14755v1	Grigore showed that Java generics are Turing complete by describing a reduction from Turing machines to Java subtyping. We apply Grigore's algorithm to Python type hints and deduce that they are Turing complete. In addition, we present an alternative reduction in which the Turing machines are simulated in real time, resulting in significantly lower compilation times. Our work is accompanied by a Python implementation of both reductions that compiles Turing machines into Python subtyping machines.			arxiv	[]	11.0
112	OMB-Py: Python Micro-Benchmarks for Evaluating Performance of MPI Libraries on HPC Systems	Nawras Alnaasan	2021-10-20 16:59:14	http://arxiv.org/abs/2110.10659v2	Python has become a dominant programming language for emerging areas like Machine Learning (ML), Deep Learning (DL), and Data Science (DS). An attractive feature of Python is that it provides easy-to-use programming interface while allowing library developers to enhance performance of their applications by harnessing the computing power offered by High Performance Computing (HPC) platforms. Efficient communication is key to scaling applications on parallel systems, which is typically enabled by the Message Passing Interface (MPI) standard and compliant libraries on HPC hardware. mpi4py is a Python-based communication library that provides an MPI-like interface for Python applications allowing application developers to utilize parallel processing elements including GPUs. However, there is currently no benchmark suite to evaluate communication performance of mpi4py -- and Python MPI codes in general -- on modern HPC systems. In order to bridge this gap, we propose OMB-Py -- Python extensions to the open-source OSU Micro-Benchmark (OMB) suite -- aimed to evaluate communication performance of MPI-based parallel applications in Python. To the best of our knowledge, OMB-Py is the first communication benchmark suite for parallel Python applications. OMB-Py consists of a variety of point-to-point and collective communication benchmark tests that are implemented for a range of popular Python libraries including NumPy, CuPy, Numba, and PyCUDA. Our evaluation reveals that mpi4py introduces a small overhead when compared to native MPI libraries. We plan to publicly release OMB-Py to benefit the Python HPC community.			arxiv	['Arpan Jain', 'Aamir Shafi', 'Hari Subramoni', 'Dhabaleswar K Panda']	12.0
113	Does Coding in Pythonic Zen Peak Performance? Preliminary Experiments of Nine Pythonic Idioms at Scale	Pattara Leelaprute	2022-03-28 04:05:54	http://arxiv.org/abs/2203.14484v1	In the field of data science, and for academics in general, the Python programming language is a popular choice, mainly because of its libraries for storing, manipulating, and gaining insight from data. Evidence includes the versatile set of machine learning, data visualization, and manipulation packages used for the ever-growing size of available data. The Zen of Python is a set of guiding design principles that developers use to write acceptable and elegant Python code. Most principles revolve around simplicity. However, as the need to compute large amounts of data, performance has become a necessity for the Python programmer. The new idea in this paper is to confirm whether writing the Pythonic way peaks performance at scale. As a starting point, we conduct a set of preliminary experiments to evaluate nine Pythonic code examples by comparing the performance of both Pythonic and Non-Pythonic code snippets. Our results reveal that writing in Pythonic idioms may save memory and time. We show that incorporating list comprehension, generator expression, zip, and itertools.zip_longest idioms can save up to 7,000 MB and up to 32.25 seconds. The results open more questions on how they could be utilized in a real-world setting. The replication package includes all scripts, and the results are available at https://doi.org/10.5281/zenodo.5712349			arxiv	['Bodin Chinthanet', 'Supatsara Wattanakriengkrai', 'Raula Gaikovina Kula', 'Pongchai Jaisri', 'Takashi Ishio']	13.0
114	Pydelay - a python tool for solving delay differential equations	V. Flunkert	2009-11-09 11:00:43	http://arxiv.org/abs/0911.1633v1	pydelay is a python library which translates a system of delay differential equations into C-code and simulates the code using scipy weave.			arxiv	['E. Schoell']	14.0
115	How fast can we make interpreted Python?	Russell Power	2013-06-25 17:57:00	http://arxiv.org/abs/1306.6047v2	Python is a popular dynamic language with a large part of its appeal coming from powerful libraries and extension modules. These augment the language and make it a productive environment for a wide variety of tasks, ranging from web development (Django) to numerical analysis (NumPy). Unfortunately, Python's performance is quite poor when compared to modern implementations of languages such as Lua and JavaScript. Why does Python lag so far behind these other languages? As we show, the very same API and extension libraries that make Python a powerful language also make it very difficult to efficiently execute. Given that we want to retain access to the great extension libraries that already exist for Python, how fast can we make it? To evaluate this, we designed and implemented Falcon, a high-performance bytecode interpreter fully compatible with the standard CPython interpreter. Falcon applies a number of well known optimizations and introduces several new techniques to speed up execution of Python bytecode. In our evaluation, we found Falcon an average of 25% faster than the standard Python interpreter on most benchmarks and in some cases about 2.5X faster.			arxiv	['Alex Rubinsteyn']	15.0
116	Performance of Python runtimes on a non-numeric scientific code	Riccardo Murri	2014-04-25 10:55:48	http://arxiv.org/abs/1404.6388v2	The Python library FatGHol FatGHoL used in Murri2012 to reckon the rational homology of the moduli space of Riemann surfaces is an example of a non-numeric scientific code: most of the processing it does is generating graphs (represented by complex Python objects) and computing their isomorphisms (a triple of Python lists; again a nested data structure). These operations are repeated many times over: for example, the spaces and are triangulated by 4'583'322 and 747'664 graphs, respectively. This is an opportunity for every Python runtime to prove its strength in optimization. The purpose of this experiment was to assess the maturity of alternative Python runtimes, in terms of: compatibility with the language as implemented in CPython 2.7, and performance speedup. This paper compares the results and experiences from running FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython 0.19, Numba 0.11, Nuitka 0.4.4 and Falcon.			arxiv	[]	16.0
117	Generating Python Code From Object-Z Specifications	A. F. Al Azzawi	2018-02-17 11:41:24	http://arxiv.org/abs/1802.06224v1	Object-Z is an object-oriented specification language which extends the Z language with classes, objects, inheritance and polymorphism that can be used to represent the specification of a complex system as collections of objects. There are a number of existing works that mapped Object-Z to C++ and Java programming languages. Since Python and Object-Z share many similarities, both are object-oriented paradigm, support set theory and predicate calculus moreover, Python is a functional programming language which is naturally closer to formal specifications, we propose a mapping from Object-Z specifications to Python code that covers some Object-Z constructs and express its specifications in Python to validate these specifications. The validations are used in the mapping covered preconditions, post-conditions, and invariants that are built using lambda function and Python's decorator. This work has found Python is an excellent language for developing libraries to map Object-Z specifications to Python.			arxiv	['M. Bettaz', 'H. M. Al-Refai']	17.0
118	Building a scalable python distribution for HEP data analysis	David Lange	2018-04-24 10:07:02	http://arxiv.org/abs/1804.08939v1	There are numerous approaches to building analysis applications across the high-energy physics community. Among them are Python-based, or at least Python-driven, analysis workflows. We aim to ease the adoption of a Python-based analysis toolkit by making it easier for non-expert users to gain access to Python tools for scientific analysis. Experimental software distributions and individual user analysis have quite different requirements. Distributions tend to worry most about stability, usability and reproducibility, while the users usually strive to be fast and nimble. We discuss how we built and now maintain a python distribution for analysis while satisfying requirements both a large software distribution (in our case, that of CMSSW) and user, or laptop, level analysis. We pursued the integration of tools used by the broader data science community as well as HEP developed (e.g., histogrammar, root_numpy) Python packages. We discuss concepts we investigated for package integration and testing, as well as issues we encountered through this process. Distribution and platform support are important topics. We discuss our approach and progress towards a sustainable infrastructure for supporting this Python stack for the CMS user community and for the broader HEP user community.			arxiv	[]	18.0
119	A Python Extension for the Massively Parallel Multiphysics Simulation Framework waLBerla	Martin Bauer	2015-11-23 15:06:47	http://arxiv.org/abs/1511.07261v1	We present a Python extension to the massively parallel HPC simulation toolkit waLBerla. waLBerla is a framework for stencil based algorithms operating on block-structured grids, with the main application field being fluid simulations in complex geometries using the lattice Boltzmann method. Careful performance engineering results in excellent node performance and good scalability to over 400,000 cores. To increase the usability and flexibility of the framework, a Python interface was developed. Python extensions are used at all stages of the simulation pipeline: They simplify and automate scenario setup, evaluation, and plotting. We show how our Python interface outperforms the existing text-file-based configuration mechanism, providing features like automatic nondimensionalization of physical quantities and handling of complex parameter dependencies. Furthermore, Python is used to process and evaluate results while the simulation is running, leading to smaller output files and the possibility to adjust parameters dependent on the current simulation state. C++ data structures are exported such that a seamless interfacing to other numerical Python libraries is possible. The expressive power of Python and the performance of C++ make development of efficient code with low time effort possible.			arxiv	['Florian Schornbaum', 'Christian Godenschwager', 'Matthias Markl', 'Daniela Anderl', 'Harald K√∂stler', 'Ulrich R√ºde']	19.0
120	The Dune Python Module	Andreas Dedner	2018-07-13 19:17:48	http://arxiv.org/abs/1807.05252v1	In this paper we present the new Dune-Python module which provides Python bindings for the Dune core, which is a C++ environment for solving partial differential equations. The aim of this new module is to firstly provide the general infrastructure for exporting realizations of statically polymorphic interfaces based on just-in-time compilation and secondly to provide bindings for the central interfaces of the dune core modules. In the first release we focus on the grid interface. Our aim is to only introduce a thin layer when passing objects into Python which can be removed when the object is passed back into a C++ algorithm. Thus no efficiency is lost and little additional code maintenance cost is incurred. To make the transition for Dune users to the Python environment straightforward the Python classes provide a very similar interface to their C++ counterparts. In addition, vectorized versions of many interfaces allow for more efficient code on the Python side. The infrastructure for exporting these interfaces and the resulting bindings for a Dune grid are explained in detail in this paper for both experienced Dune users and others interested in a flexible Python environment for implementing grid based schemes for solving partial differential equations.			arxiv	['Martin Nolte']	20.0
121	Image Processing in Python With Montage	John Good	2019-08-26 15:50:25	http://arxiv.org/abs/1908.09753v1	The Montage image mosaic engine has found wide applicability in astronomy research, integration into processing environments, and is an examplar application for the development of advanced cyber-infrastructure. It is written in C to provide performance and portability. Linking C/C++ libraries to the Python kernel at run time as binary extensions allows them to run under Python at compiled speeds and enables users to take advantage of all the functionality in Python. We have built Python binary extensions of the 59 ANSI-C modules that make up version 5 of the Montage toolkit. This has involved a turning the code into a C library, with driver code fully separated to reproduce the calling sequence of the command-line tools; and then adding Python and C linkage code with the Cython library, which acts as a bridge between general C libraries and the Python interface. We will demonstrate how to use these Python binary extensions to perform image processing, including reprojecting and resampling images, rectifying background emission to a common level, creation of image mosaics that preserve the calibration and astrometric fidelity of the input images, creating visualizations with an adaptive stretch algorithm, processing HEALPix images, and analyzing and managing image metadata.			arxiv	['G. Bruce Berriman']	21.0
122	An Analysis of Python's Topics, Trends, and Technologies Through Mining Stack Overflow Discussions	Hamed Tahmooresi	2020-04-14 02:59:16	http://arxiv.org/abs/2004.06280v1	Python is a popular, widely used, and general-purpose programming language. In spite of its ever-growing community, researchers have not performed much analysis on Python's topics, trends, and technologies which provides insights for developers about Python community trends and main issues. In this article, we examine the main topics related to this language being discussed by developers on one of the most popular Q\&A websites, Stack Overflow, as well as temporal trends through mining 2461876 posts. To be more useful for the software engineers, we study what Python provides as the alternative to popular technologies offered by common programming languages like Java. Our results indicate that discussions about Python standard features, web programming, and scientific programming. Programming in areas such as mathematics, data science, statistics, machine learning, natural language processing (NLP), and so forth. are the most popular areas in the Python community. At the same time, areas related to scientific programming are steadily receiving more attention from the Python developers.			arxiv	['Abbas Heydarnoori', 'Alireza Aghamohammadi']	22.0
123	Python Workflows on HPC Systems	Dominik Strassel	2020-12-01 09:51:12	http://arxiv.org/abs/2012.00365v1	The recent successes and wide spread application of compute intensive machine learning and data analytics methods have been boosting the usage of the Python programming language on HPC systems. While Python provides many advantages for the users, it has not been designed with a focus on multi-user environments or parallel programming - making it quite challenging to maintain stable and secure Python workflows on a HPC system. In this paper, we analyze the key problems induced by the usage of Python on HPC clusters and sketch appropriate workarounds for efficiently maintaining multi-user Python software environments, securing and restricting resources of Python jobs and containing Python processes, while focusing on Deep Learning applications running on GPU clusters.			arxiv	['Philipp Reusch', 'Janis Keuper']	23.0
124	Conflict-aware Inference of Python Compatible Runtime Environments with Domain Knowledge Graph	Wei Cheng	2022-01-18 14:55:00	http://arxiv.org/abs/2201.07029v1	Code sharing and reuse is a widespread use practice in software engineering. Although a vast amount of open-source Python code is accessible on many online platforms, programmers often find it difficult to restore a successful runtime environment. Previous studies validated automatic inference of Python dependencies using pre-built knowledge bases. However, these studies do not cover sufficient knowledge to accurately match the Python code and also ignore the potential conflicts between their inferred dependencies, thus resulting in a low success rate of inference. In this paper, we propose PyCRE, a new approach to automatically inferring Python compatible runtime environments with domain knowledge graph (KG). Specifically, we design a domain-specific ontology for Python third-party packages and construct KGs for over 10,000 popular packages in Python 2 and Python 3. PyCRE discovers candidate libraries by measuring the matching degree between the known libraries and the third-party resources used in target code. For the NP-complete problem of dependency solving, we propose a heuristic graph traversal algorithm to efficiently guarantee the compatibility between packages. PyCRE achieves superior performance on a real-world dataset and efficiently resolves nearly half more import errors than previous methods.			arxiv	['Xiangrong Zhu', 'Wei Hu']	24.0
125	Triangulating Python Performance Issues with Scalene	Emery D. Berger	2022-12-15 02:56:25	http://arxiv.org/abs/2212.07597v1	This paper proposes Scalene, a profiler specialized for Python. Scalene combines a suite of innovations to precisely and simultaneously profile CPU, memory, and GPU usage, all with low overhead. Scalene's CPU and memory profilers help Python programmers direct their optimization efforts by distinguishing between inefficient Python and efficient native execution time and memory usage. Scalene's memory profiler employs a novel sampling algorithm that lets it operate with low overhead yet high precision. It also incorporates a novel algorithm that automatically pinpoints memory leaks, whether within Python or across the Python-native boundary. Scalene tracks a new metric called copy volume, which highlights costly copying operations that can occur when Python silently converts between C and Python data representations, or between CPU and GPU. Since its introduction, Scalene has been widely adopted, with over 500,000 downloads to date. We present experience reports from developers who used Scalene to achieve significant performance improvements and memory savings.			arxiv	['Sam Stern', 'Juan Altmayer Pizzorno']	25.0
126	Python for education: permutations	Andrzej Kapanowski	2013-07-26 14:18:21	http://arxiv.org/abs/1307.7042v1	Python implementation of permutations is presented. Three classes are introduced: Perm for permutations, Group for permutation groups, and PermError to report any errors for both classes. The class Perm is based on Python dictionaries and utilize cycle notation. The methods of calculation for the perm order, parity, ranking and unranking are given. A random permutation generation is also shown. The class Group is very simple and it is also based on dictionaries. It is mainly the presentation of the permutation groups interface with methods for the group order, subgroups (normalizer, centralizer, center, stabilizer), orbits, and several tests. The corresponding Python code is contained in the modules perms and groups.			arxiv	[]	26.0
127	Python bindings for libcloudph++	Dorota Jarecka	2015-04-05 20:58:18	http://arxiv.org/abs/1504.01161v1	This technical note introduces the Python bindings for libcloudph++. The libcloudph++ is a C++ library of algorithms for representing atmospheric cloud microphysics in numerical models. The bindings expose the complete functionality of the library to the Python users. The bindings are implemented using the Boost.Python C++ library and use NumPy arrays. This note includes listings with Python scripts exemplifying the use of selected library components. An example solution for using the Python bindings to access libcloudph++ from Fortran is presented.			arxiv	['Sylwester Arabas', 'Davide Del Vento']	27.0
128	Yaps: Python Frontend to Stan	Guillaume Baudart	2018-12-06 01:24:29	http://arxiv.org/abs/1812.04125v1	Stan is a popular probabilistic programming language with a self-contained syntax and semantics that is close to graphical models. Unfortunately, existing embeddings of Stan in Python use multi-line strings. That approach forces users to switch between two different language styles, with no support for syntax highlighting or simple error reporting within the Stan code. This paper tackles the question of whether Stan could use Python syntax while retaining its self-contained semantics. The answer is yes, that can be accomplished by reinterpreting the Python syntax. This paper introduces Yaps, a new frontend to Stan based on reinterpreted Python. We tested Yaps on over a thousand Stan models and made it available open-source.			arxiv	['Martin Hirzel', 'Kiran Kate', 'Louis Mandel', 'Avraham Shinnar']	28.0
129	Pytrec_eval: An Extremely Fast Python Interface to trec_eval	Christophe Van Gysel	2018-05-04 03:37:03	http://arxiv.org/abs/1805.01597v2	We introduce pytrec_eval, a Python interface to the tree_eval information retrieval evaluation toolkit. pytrec_eval exposes the reference implementations of trec_eval within Python as a native extension. We show that pytrec_eval is around one order of magnitude faster than invoking trec_eval as a sub process from within Python. Compared to a native Python implementation of NDCG, pytrec_eval is twice as fast for practically-sized rankings. Finally, we demonstrate its effectiveness in an application where pytrec_eval is combined with Pyndri and the OpenAI Gym where query expansion is learned using Q-learning.			arxiv	['Maarten de Rijke']	29.0
130	Nonparametric Estimation of the Random Coefficients Model in Python	Emil Mendoza	2021-08-08 07:27:49	http://arxiv.org/abs/2108.03582v2	We present $\textbf{PyRMLE}$, a Python module that implements Regularized Maximum Likelihood Estimation for the analysis of Random Coefficient models. $\textbf{PyRMLE}$ is simple to use and readily works with data formats that are typical to Random Coefficient problems. The module makes use of Python's scientific libraries $\textbf{NumPy}$ and $\textbf{SciPy}$ for computational efficiency. The main implementation of the algorithm is executed purely in Python code which takes advantage of Python's high-level features.			arxiv	['Fabian Dunker', 'Marco Reale']	30.0
131	Running HMC Simulation with Python via QUDA	Shuhei Yamamoto	2022-12-13 15:40:29	http://arxiv.org/abs/2212.06657v1	Lyncs-API is a Python API for Lattice QCD applications. It is designed as a Python toolkit that allows the user to use and run various lattice QCD libraries while programming in Python. The goal is to provide the user an easy programming experience without scarifying performance across multiple platforms, by preparing a common framework for various softwares for lattice QCD calculations. As such, it contains interfaces to, e.g., c-lime, DDalphaAMG, tmLQCD, and QUDA. In this proceeding, we focus on a Lyncs interface to QUDA, named Lyncs-QUDA, and present a small tutorial on how to use this Python interface to perform a HMC simulation using QUDA.			arxiv	['Simone Bacchio', 'Jacob Finenrath']	31.0
132	Python client for Isabelle server	Boris Shminke	2022-12-09 12:05:28	http://arxiv.org/abs/2212.11173v1	We contribute a Python client for the Isabelle server, which gives researchers and students using Python as their primary programming language an opportunity to communicate with the Isabelle server through TCP directly from a Python script. Such an approach helps avoid the complexities of integrating the existing Python script with languages used for Isabelle development (ML and Scala). We also describe new features that appeared since the announcement of the first version of the client a year ago. Finally, we give examples of the client's applications in research and education and discuss known limitations and possible directions for future development.			arxiv	[]	32.0
133	ePython: An implementation of Python for the many-core Epiphany coprocessor	Nick Brown	2020-10-28 09:01:27	http://arxiv.org/abs/2010.14827v1	The Epiphany is a many-core, low power, low on-chip memory architecture and one can very cheaply gain access to a number of parallel cores which is beneficial for HPC education and prototyping. The very low power nature of these architectures also means that there is potential for their use in future HPC machines, however there is a high barrier to entry in programming them due to the associated complexities and immaturity of supporting tools. In this paper we present our work on ePython, a subset of Python for the Epiphany and similar many-core co-processors. Due to the limited on-chip memory per core we have developed a new Python interpreter and this, combined with additional support for parallelism, has meant that novices can take advantage of Python to very quickly write parallel codes on the Epiphany and explore concepts of HPC using a smaller scale parallel machine. The high level nature of Python opens up new possibilities on the Epiphany, we examine a computationally intensive Gauss-Seidel code from the programmability and performance perspective, discuss running Python hybrid on both the host CPU and Epiphany, and interoperability between a full Python interpreter on the CPU and ePython on the Epiphany. The result of this work is support for developing Python on the Epiphany, which can be applied to other similar architectures, that the community have already started to adopt and use to explore concepts of parallelism and HPC.			arxiv	[]	33.0
134	Characterizing Bugs in Python and R Data Analytics Programs	Shibbir Ahmed	2023-06-14 16:50:01	http://arxiv.org/abs/2306.08632v1	R and Python are among the most popular languages used in many critical data analytics tasks. However, we still do not fully understand the capabilities of these two languages w.r.t. bugs encountered in data analytics tasks. What type of bugs are common? What are the main root causes? What is the relation between bugs and root causes? How to mitigate these bugs? We present a comprehensive study of 5,068 Stack Overflow posts, 1,800 bug fix commits from GitHub repositories, and several GitHub issues of the most used libraries to understand bugs in R and Python. Our key findings include: while both R and Python have bugs due to inexperience with data analysis, Python see significantly larger data preprocessing bugs compared to R. Developers experience significantly more data flow bugs in R because intermediate results are often implicit. We also found changes and bugs in packages and libraries cause more bugs in R compared to Python while package or library misselection and conflicts cause more bugs in Python than R. While R has a slightly higher readability barrier for data analysts, the statistical power of R leads to a less number of bad performance bugs. In terms of data visualization, R packages have significantly more bugs than Python libraries. We also identified a strong correlation between comparable packages in R and Python despite their linguistic and methodological differences. Lastly, we contribute a large dataset of manually verified R and Python bugs.			arxiv	['Mohammad Wardat', 'Hamid Bagheri', 'Breno Dantas Cruz', 'Hridesh Rajan']	34.0
135	Simplifying Parallelization of Scientific Codes by a Function-Centric Approach in Python	Jon K. Nilsen	2010-02-03 12:31:14	http://arxiv.org/abs/1002.0705v1	The purpose of this paper is to show how existing scientific software can be parallelized using a separate thin layer of Python code where all parallel communication is implemented. We provide specific examples on such layers of code, and these examples may act as templates for parallelizing a wide set of serial scientific codes. The use of Python for parallelization is motivated by the fact that the language is well suited for reusing existing serial codes programmed in other languages. The extreme flexibility of Python with regard to handling functions makes it very easy to wrap up decomposed computational tasks of a serial scientific application as Python functions. Many parallelization-specific components can be implemented as generic Python functions, which may take as input those functions that perform concrete computational tasks. The overall programming effort needed by this parallelization approach is rather limited, and the resulting parallel Python scripts have a compact and clean structure. The usefulness of the parallelization approach is exemplified by three different classes of applications in natural and social sciences.			arxiv	['Xing Cai', 'Bjorn Hoyland', 'Hans Petter Langtangen']	35.0
136	DockerizeMe: Automatic Inference of Environment Dependencies for Python Code Snippets	Eric Horton	2019-05-27 11:23:29	http://arxiv.org/abs/1905.11127v1	Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.			arxiv	['Chris Parnin']	36.0
137	OpenML-Python: an extensible Python API for OpenML	Matthias Feurer	2019-11-06 16:59:30	http://arxiv.org/abs/1911.02490v2	OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper we introduce OpenML-Python, a client API for Python, opening up the OpenML platform for a wide range of Python-based tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation is available at https://github.com/openml/openml-python/.			arxiv	['Jan N. van Rijn', 'Arlind Kadra', 'Pieter Gijsbers', 'Neeratyoy Mallik', 'Sahithya Ravi', 'Andreas M√ºller', 'Joaquin Vanschoren', 'Frank Hutter']	37.0
138	Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations and visualizations via ParaMonte::Python library	Amir Shahmoradi	2020-10-01 23:26:42	http://arxiv.org/abs/2010.00724v1	ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial and MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for sampling mathematical objective functions, in particular, the posterior distributions of parameters in Bayesian modeling and analysis in data science, Machine Learning, and scientific inference in general. In addition to providing access to fast high-performance serial/parallel Monte Carlo and MCMC sampling routines, the ParaMonte::Python library provides extensive post-processing and visualization tools that aim to automate and streamline the process of model calibration and uncertainty quantification in Bayesian data analysis. Furthermore, the automatically-enabled restart functionality of ParaMonte::Python samplers ensure seamless fully-deterministic into-the-future restart of Monte Carlo simulations, should any interruptions happen. The ParaMonte::Python library is MIT-licensed and is permanently maintained on GitHub at https://github.com/cdslaborg/paramonte/tree/master/src/interface/Python.			arxiv	['Fatemeh Bagheri', 'Joshua Alexander Osborne']	38.0
139	Productivity, Portability, Performance: Data-Centric Python	Alexandros Nikolaos Ziogas	2021-07-01 15:51:18	http://arxiv.org/abs/2107.00555v2	Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. In this work, we present a workflow that retains Python's high productivity while achieving portable performance across different architectures. The workflow's key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16% scaling efficiency on 512 nodes.			arxiv	['Timo Schneider', 'Tal Ben-Nun', 'Alexandru Calotoiu', 'Tiziano De Matteis', 'Johannes de Fine Licht', 'Luca Lavarini', 'Torsten Hoefler']	39.0
140	PyTracer: Automatically profiling numerical instabilities in Python	Yohan Chatelain	2021-12-21 20:22:34	http://arxiv.org/abs/2112.11508v2	Numerical stability is a crucial requirement of reliable scientific computing. However, despite the pervasiveness of Python in data science, analyzing large Python programs remains challenging due to the lack of scalable numerical analysis tools available for this language. To fill this gap, we developed PyTracer, a profiler to quantify numerical instability in Python applications. PyTracer transparently instruments Python code to produce numerical traces and visualize them interactively in a Plotly dashboard. We designed PyTracer to be agnostic to numerical noise model, allowing for tool evaluation through Monte-Carlo Arithmetic, random rounding, random data perturbation, or structured noise for a particular application. We illustrate PyTracer's capabilities by testing the numerical stability of key functions in both SciPy and Scikit-learn, two dominant Python libraries for mathematical modeling. Through these evaluations, we demonstrate PyTracer as a scalable, automatic, and generic framework for numerical profiling in Python.			arxiv	['Nigel Yong', 'Gregory Kiar', 'Tristan Glatard']	40.0
141	GAP-Gen: Guided Automatic Python Code Generation	Junchen Zhao	2022-01-19 06:32:47	http://arxiv.org/abs/2201.08810v2	Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose GAP-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract Syntax Tree but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently through out the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. GAP-Gen fine-tunes the transformer based language models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet, CodeSearchNet AdvTest and Code-Docstring Corpus from EdinburghNLP. Our experiments show that GAP-Gen achieves better results on automatic Python code generation task than previous works.			arxiv	['Yurun Song', 'Junlin Wang', 'Ian G. Harris']	41.0
142	Approaches to the Parallelization of Merge Sort in Python	Alexandra Yang	2022-11-26 18:26:30	http://arxiv.org/abs/2211.16479v1	The theory of divide-and-conquer parallelization has been well-studied in the past, providing a solid basis upon which to explore different approaches to the parallelization of merge sort in Python. Python's simplicity and extensive selection of libraries make it the most popular scientific programming language, so it is a fitting language in which to implement and analyze these algorithms. In this paper, we use Python packages multiprocessing and mpi4py to implement several different parallel merge sort algorithms. Experiments are conducted on an academic supercomputer, upon which benchmarks are performed using Cloudmesh. We find that hybrid multiprocessing merge sort outperforms several other algorithms, achieving a 1.5x speedup compared to the built-in Python sorted() and a 34x speedup compared to sequential merge sort. Our results provide insight into different approaches to implementing parallel merge sort in Python and contribute to the understanding of general divide-and-conquer parallelization in Python on both shared and distributed memory systems.			arxiv	[]	42.0
143	Faster or Slower? Performance Mystery of Python Idioms Unveiled with Empirical Evidence	Zejun Zhang	2023-01-30 03:28:24	http://arxiv.org/abs/2301.12633v1	The usage of Python idioms is popular among Python developers in a formative study of 101 performance-related questions of Python idioms on Stack Overflow, we find that developers often get confused about the performance impact of Python idioms and use anecdotal toy code or rely on personal project experience which is often contradictory in performance outcomes. There has been no large-scale, systematic empirical evidence to reconcile these performance debates. In the paper, we create a large synthetic dataset with 24,126 pairs of non-idiomatic and functionally-equivalent idiomatic code for the nine unique Python idioms identified in Zhang et al., and reuse a large real-project dataset of 54,879 such code pairs provided by Zhang et al. We develop a reliable performance measurement method to compare the speedup or slowdown by idiomatic code against non-idiomatic counterpart, and analyze the performance discrepancies between the synthetic and real-project code, the relationships between code features and performance changes, and the root causes of performance changes at the bytecode level. We summarize our findings as some actionable suggestions for using Python idioms.			arxiv	['Zhenchang Xing', 'Xin Xia', 'Xiwei Xu', 'Liming Zhu', 'Qinghua Lu']	43.0
144	Rapid Development of Interferometric Software Using MIRIAD and Python	Peter K. G. Williams	2012-03-01 22:07:31	http://arxiv.org/abs/1203.0330v1	"New and upgraded radio interferometers produce data at massive rates and will require significant improvements in analysis techniques to reach their promised levels of performance in a routine manner. Until these techniques are fully developed, productivity and accessibility in scientific programming environments will be key bottlenecks in the pipeline leading from data-taking to research results. We present an open-source software package, miriad-python, that allows access to the MIRIAD interferometric reduction system in the Python programming language. The modular design of MIRIAD and the high productivity and accessibility of Python provide an excellent foundation for rapid development of interferometric software. Several other projects with similar goals exist and we describe them and compare miriad-python to them in detail. Along with an overview of the package design, we present sample code and applications, including the detection of millisecond astrophysical transients, determination and application of nonstandard calibration parameters, interactive data visualization, and a reduction pipeline using a directed acyclic graph dependency model analogous to that of the traditional Unix tool ""make"". The key aspects of the miriad-python software project are documented. We find that miriad-python provides an extremely effective environment for prototyping new interferometric software, though certain existing packages provide far more infrastructure for some applications. While equivalent software written in compiled languages can be much faster than Python, there are many situations in which execution time is profitably exchanged for speed of development, code readability, accessibility to nonexpert programmers, quick interlinking with foreign software packages, and other virtues of the Python language."			arxiv	['Casey J. Law', 'Geoffrey C. Bower']	44.0
145	Toward Efficient Interactions between Python and Native Libraries	Jialiang Tan	2021-06-11 00:48:02	http://arxiv.org/abs/2107.00064v1	Python has become a popular programming language because of its excellent programmability. Many modern software packages utilize Python for high-level algorithm design and depend on native libraries written in C/C++/Fortran for efficient computation kernels. Interaction between Python code and native libraries introduces performance losses because of the abstraction lying on the boundary of Python and native libraries. On the one side, Python code, typically run with interpretation, is disjoint from its execution behavior. On the other side, native libraries do not include program semantics to understand algorithm defects. To understand the interaction inefficiencies, we extensively study a large collection of Python software packages and categorize them according to the root causes of inefficiencies. We extract two inefficiency patterns that are common in interaction inefficiencies. Based on these patterns, we develop PieProf, a lightweight profiler, to pinpoint interaction inefficiencies in Python applications. The principle of PieProf is to measure the inefficiencies in the native execution and associate inefficiencies with high-level Python code to provide a holistic view. Guided by PieProf, we optimize 17 real-world applications, yielding speedups up to 6.3$\times$ on application level.			arxiv	['Yu Chen', 'Zhenming Liu', 'Bin Ren', 'Shuaiwen Leon Song', 'Xipeng Shen', 'Xu Liu']	45.0
146	Improving Tese Case Generation for Python Native Libraries Through Constraints on Input Data Structures	Xin Zhang	2022-06-28 08:47:33	http://arxiv.org/abs/2206.13828v1	Modern Python projects execute computational functions using native libraries and give Python interfaces to boost execution speed; hence, testing these libraries becomes critical to the project's robustness. One challenge is that existing approaches use coverage to guide generation, but native libraries run as black boxes to Python code with no execution information. Another is that dynamic binary instrumentation reduces testing performance as it needs to monitor both native libraries and the Python virtual machine. To address these challenges, in this paper, we propose an automated test case generation approach that works at the Python code layer. Our insight is that many path conditions in native libraries are for processing input data structures through interacting with the VM. In our approach, we instrument the Python Interpreter to monitor the interactions between native libraries and VM, derive constraints on the structures, and then use the constraints to guide test case generation. We implement our approach in a tool named PyCing and apply it to six widely-used Python projects. The experimental results reveal that with the structure constraint guidance, PyCing can cover more execution paths than existing test cases and state-of-the-art tools. Also, with the checkers in the testing framework Pytest, PyCing can identify segmentation faults in 10 Python interfaces and memory leaks in 9. Our instrumentation strategy also has an acceptable influence on testing efficiency.			arxiv	['Xutong Ma', 'Jiwen Yan', 'Baoquan Cui', 'Jun Yan', 'Jian Zhang']	46.0
147	binary_c-python: A Python-based stellar population synthesis tool and interface to binary_c	D. D. Hendriks	2023-06-05 11:04:17	http://arxiv.org/abs/2306.02779v1	We present the software package binary_c-python which provides a convenient and easy-to-use interface to the binary_c framework, allowing the user to rapidly evolve individual systems and populations of stars. binary_c-python is available on Pip and on GitLab. binary_c-python contains many useful features to control and process the output of binary_c, like by providing binary_c-python with logging statements that are dynamically compiled and loaded into binary_c. Moreover, we have recently added standardised output of events like Roche-lobe overflow or double compact-object formation to binary_c, and automatic parsing and managing of that output in binary_c-python. binary_c-python uses multiprocessing to utilise all the cores on a particular machine, and can run populations with HPC cluster workload managers like HTCondor and Slurm, allowing the user to run simulations on large computing clusters. We provide documentation that is automatically generated based on docstrings and a suite of Jupyter notebooks. These notebooks consist of technical tutorials on how to use binary_c-python and use-case scenarios aimed at doing science. Much of binary_c-python is covered by unit tests to ensure reliability and correctness, and the test coverage is continually increased as the package is improved.			arxiv	['R. G. Izzard']	47.0
148	PyMsOfa: A Python Package for the Standards of Fundamental Astronomy (SOFA) Service	Jianghui Ji	2023-10-12 19:11:41	http://arxiv.org/abs/2310.08673v2	The Standards of Fundamental Astronomy (SOFA) is a service provided by the International Astronomical Union (IAU) that offers algorithms and software for astronomical calculations, which was released in two versions by FORTRAN 77 and ANSI C, respectively. In this work, we implement the python package PyMsOfa for SOFA service by three ways: (1) a python wrapper package based on a foreign function library for Python (ctypes), (2) a python wrapper package with the foreign function interface for Python calling C code (cffi), and (3) a python package directly written in pure python codes from SOFA subroutines. The package PyMsOfa has fully implemented 247 functions of the original SOFA routines. In addition, PyMsOfa is also extensively examined, which is exactly consistent with those test examples given by the original SOFA. This python package can be suitable to not only the astrometric detection of habitable planets of the Closeby Habitable Exoplanet Survey (CHES) mission (Ji et al. 2022), but also for the frontiers themes of black holes and dark matter related to astrometric calculations and other fields. The source codes are available via https://github.com/CHES2023/PyMsOfa.			arxiv	['Dongjie Tan', 'Chunhui Bao', 'Xiumin Huang', 'Shoucun Hu', 'Yao Dong', 'Su Wang']	48.0
149	Python for Education: Computational Methods for Nonlinear Systems	Christopher R. Myers	2007-04-24 18:55:17	http://arxiv.org/abs/0704.3182v1	We describe a novel, interdisciplinary, computational methods course that uses Python and associated numerical and visualization libraries to enable students to implement simulations for a number of different course modules. Problems in complex networks, biomechanics, pattern formation, and gene regulation are highlighted to illustrate the breadth and flexibility of Python-powered computational environments.			arxiv	['James. P. Sethna']	49.0
150	Implementation of Kalman Filter with Python Language	Mohamed Laaraiedh	2012-04-02 11:40:41	http://arxiv.org/abs/1204.0375v1	In this paper, we investigate the implementation of a Python code for a Kalman Filter using the Numpy package. A Kalman Filtering is carried out in two steps: Prediction and Update. Each step is investigated and coded as a function with matrix input and output. These different functions are explained and an example of a Kalman Filter application for the localization of mobile in wireless networks is given.			arxiv	[]	50.0
151	A Framework for Distributed Deep Learning Layer Design in Python	Clay McLeod	2015-10-25 21:04:12	http://arxiv.org/abs/1510.07303v1	In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.			arxiv	[]	51.0
152	Want Drugs? Use Python	Micha≈Ç Nowotka	2016-07-01 19:02:36	http://arxiv.org/abs/1607.00378v1	We describe how Python can be leveraged to streamline the curation, modelling and dissemination of drug discovery data as well as the development of innovative, freely available tools for the related scientific community. We look at various examples, such as chemistry toolkits, machine-learning applications and web frameworks and show how Python can glue it all together to create efficient data science pipelines.			arxiv	['George Papadatos', 'Mark Davies', 'Nathan Dedman', 'Anne Hersey']	52.0
153	Geoplotlib: a Python Toolbox for Visualizing Geographical Data	Andrea Cuttone	2016-08-05 16:39:27	http://arxiv.org/abs/1608.01933v1	We introduce geoplotlib, an open-source python toolbox for visualizing geographical data. geoplotlib supports the development of hardware-accelerated interactive visualizations in pure python, and provides implementations of dot maps, kernel density estimation, spatial graphs, Voronoi tesselation, shapefiles and many more common spatial visualizations. We describe geoplotlib design, functionalities and use cases.			arxiv	['Sune Lehmann', 'Jakob Eg Larsen']	53.0
154	Powerbox: A Python package for creating structured fields with isotropic power spectra	Steven G. Murray	2018-08-27 00:21:57	http://arxiv.org/abs/1809.05030v1	Powerbox is a pure-Python package for creating and measuring structured fields with homogeneous and isotropic power spectra.			arxiv	[]	54.0
155	TimeGym: Debugging for Time Series Modeling in Python	Diogo Seca	2021-05-04 10:36:29	http://arxiv.org/abs/2105.01404v1	We introduce the TimeGym Forecasting Debugging Toolkit, a Python library for testing and debugging time series forecasting pipelines. TimeGym simplifies the testing forecasting pipeline by providing generic tests for forecasting pipelines fresh out of the box. These tests are based on common modeling challenges of time series. Our library enables forecasters to apply a Test-Driven Development approach to forecast modeling, using specified oracles to generate artificial data with noise.			arxiv	[]	55.0
156	MontePython: Implementing Quantum Monte Carlo using Python	J. K. Nilsen	2006-09-22 12:42:42	http://arxiv.org/abs/physics/0609191v1	We present a cross-language C++/Python program for simulations of quantum mechanical systems with the use of Quantum Monte Carlo (QMC) methods. We describe a system for which to apply QMC, the algorithms of variational Monte Carlo and diffusion Monte Carlo and we describe how to implement theses methods in pure C++ and C++/Python. Furthermore we check the efficiency of the implementations in serial and parallel cases to show that the overhead using Python can be negligible.			arxiv	[]	56.0
157	HOOMD-blue: A Python package for high-performance molecular dynamics and hard particle Monte Carlo simulations	Joshua A. Anderson	2013-08-26 13:56:04	http://arxiv.org/abs/1308.5587v2	HOOMD-blue is a particle simulation engine designed for nano- and colloidal-scale molecular dynamics and hard particle Monte Carlo simulations. It has been actively developed since March 2007 and available open source since August 2008. HOOMD-blue is a Python package with a high performance C++/CUDA backend that we built from the ground up for GPU acceleration. The Python interface allows users to combine HOOMD-blue with with other packages in the Python ecosystem to create simulation and analysis workflows. We employ software engineering practices to develop, test, maintain, and expand the code.			arxiv	['Jens Glaser', 'Sharon C. Glotzer']	57.0
158	Plyades: A Python Library for Space Mission Design	Helge Eichhorn	2016-07-01 18:53:15	http://arxiv.org/abs/1607.00849v1	Plyades: A Python Library for Space Mission Design Designing a space mission is a computation-heavy task. Software tools that conduct the necessary numerical simulations and optimizations are therefore indispensable. The usability of existing software, written in Fortran and MATLAB, suffers because of high complexity, low levels of abstraction and out-dated programming practices. We propose Python as a viable alternative for astrodynamics tools and demonstrate the proof-of-concept library Plyades which combines powerful features with Pythonic ease of use.			arxiv	['Reiner Anderl']	58.0
159	A Practical Python API for Querying AFLOWLIB	Conred W. Rosenbrock	2017-09-28 20:38:47	http://arxiv.org/abs/1710.00813v1	"Large databases such as aflowlib.org provide valuable data sources for discovering material trends through machine learning. Although a REST API and query language are available, there is a learning curve associated with the AFLUX language that acts as a barrier for new users. Additionally, the data is stored using non-standard serialization formats. Here we present a high-level API that allows immediate access to the aflowlib data using standard python operators and language features. It provides an easy way to integrate aflowlib data with other python materials packages such as ase and quippy, and provides automatic deserialization into numpy arrays and python objects. This package is available via ""pip install aflow""."			arxiv	[]	59.0
160	salmon: A Symbolic Linear Regression Package for Python	Alex Boyd	2019-11-02 04:42:28	http://arxiv.org/abs/1911.00648v3	One of the most attractive features of R is its linear modeling capabilities. We describe a Python package, salmon, that brings the best of R's linear modeling functionality to Python in a Pythonic way -- by providing composable objects for specifying and fitting linear models. This object-oriented design also enables other features that enhance ease-of-use, such as automatic visualizations and intelligent model building.			arxiv	['Dennis L. Sun']	60.0
161	pySiDR: Python Event Reconstruction for SiD	C. T. Potter	2020-02-13 22:43:25	http://arxiv.org/abs/2002.05804v1	Event reconstruction in the ILC community has typically relied on algorithms implemented in C++, a fast compiled language. However, the Python package pyLCIO provides a full interface to tracker and calorimeter hits stored in LCIO files, opening up the possibility to implement reconstruction algorithms in a language uniquely well suited to working with large lists of hits built with list comprehensions. Python, an interpreted language which can perform complex tasks with minimal code, also allows seamless integration with powerful machine learning tools developed recently. We discuss pySiDR, a Python package for SiD event reconstruction.			arxiv	[]	61.0
162	FitsGeo -- Python package for PHITS geometry development and visualization	Ivan Gordeev	2020-08-08 09:54:21	http://arxiv.org/abs/2008.03298v1	An easy way to define and visualize geometry for PHITS input files introduced. Suggested FitsGeo Python package helps to define surfaces as Python objects and manipulate them conveniently. VPython assists to view defined geometry interactively which boosts geometry development and helps with complicated cases. Every class that sets the surface object has methods with some extra properties. As well as geometry generation for PHITS input, additional modules developed for material and cell definition. Any user with a very basic knowledge of Python can define the geometry in a convenient way and use it in further research related to particle transport.			arxiv	[]	62.0
163	HDPython: A High Level Python Based Object-Oriented HDL Framework	R. Peschke	2020-11-05 02:43:50	http://arxiv.org/abs/2011.02626v2	We present a High-Level Python-based Hardware Description Language (HDPython), It uses Python as its source language and converts it to standard VHDL. Compared to other approaches of building converters from a high-level programming language into a hardware description language, this new approach aims to maintain an object-oriented paradigm throughout the entire process. Instead of removing all the high-level features from Python to make it into an HDL, this approach goes the opposite way. It tries to show how certain features from a high-level language can be implemented in an HDL, providing the corresponding benefits of high-level programming for the user.			arxiv	['K. Nishimura', 'G. Varner']	63.0
164	cellanneal: A User-Friendly Deconvolution Software for Omics Data	Lisa Buchauer	2021-10-15 17:14:58	http://arxiv.org/abs/2110.08209v1	We introduce cellanneal, a python-based software for deconvolving bulk RNA sequencing data. cellanneal relies on the optimization of Spearman's rank correlation coefficient between experimental and computational mixture gene expression vectors using simulated annealing. cellanneal can be used as a python package or via a command line interface, but importantly also provides a simple graphical user interface which is distributed as a single executable file for user convenience. The python package is available at https://github.com/LiBuchauer/cellanneal , the graphical software can be downloaded at http://shalevlab.weizmann.ac.il/resources .			arxiv	['Shalev Itzkovitz']	64.0
165	Asgl: A Python Package for Penalized Linear and Quantile Regression	√Ålvaro M√©ndez Civieta	2021-10-31 11:43:10	http://arxiv.org/abs/2111.00472v1	Asg is a Python package that solves penalized linear regression and quantile regression models for simultaneous variable selection and prediction, for both high and low dimensional frameworks. It makes very easy to set up and solve different types of lasso-based penalizations among which the asgl (adaptive sparse group lasso, that gives name to the package) is remarked. This package is built on top of cvxpy, a Python-embedded modeling language for convex optimization problems and makes extensive use of multiprocessing, a Python module for parallel computing that significantly reduces computation times of asgl.			arxiv	['M. Carmen Aguilera-Morillo', 'Rosa E. Lillo']	65.0
166	Scalpel: The Python Static Analysis Framework	Li Li	2022-02-24 00:27:56	http://arxiv.org/abs/2202.11840v1	Despite being the most popular programming language, Python has not yet received enough attention from the community. To the best of our knowledge, there is no general static analysis framework proposed to facilitate the implementation of dedicated Python static analyzers. To fill this gap, we design and implement such a framework (named Scalpel) and make it publicly available as an open-source project. The Scalpel framework has already integrated a number of fundamental static analysis functions (e.g., call graph constructions, control-flow graph constructions, alias analysis, etc.) that are ready to be reused by developers to implement client applications focusing on statically resolving dedicated Python problems such as detecting bugs or fixing vulnerabilities.			arxiv	['Jiawei Wang', 'Haowei Quan']	66.0
167	Modernizing the ESRF beamline application software architecture with generic Python modules	Jorg Klora	2002-10-16 13:27:22	http://arxiv.org/abs/cond-mat/0210344v1	We report on the modernization of the ESRF beamline application software with Python modules. The current building blocks used around the SPEC data acquisition software together with the new elements are presented.			arxiv	[]	67.0
168	Multi-Agent Programming Contest 2011 - The Python-DTU Team	J√∏rgen Villadsen	2011-10-01 15:03:52	http://arxiv.org/abs/1110.0105v1	We provide a brief description of the Python-DTU system, including the overall design, the tools and the algorithms that we plan to use in the agent contest.			arxiv	['Mikko Berggren Ettienne', 'Steen Vester']	68.0
169	Proceedings of the 7th European Conference on Python in Science (EuroSciPy 2014)	Pierre de Buyl	2014-12-22 15:47:51	http://arxiv.org/abs/1412.7030v1	These are the proceedings of the 7th European Conference on Python in Science, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).			arxiv	['Nelle Varoquaux']	69.0
170	PythonFOAM: In-situ data analyses with OpenFOAM and Python	Romit Maulik	2021-03-17 01:34:41	http://arxiv.org/abs/2103.09389v2	We outline the development of a general-purpose Python-based data analysis tool for OpenFOAM. Our implementation relies on the construction of OpenFOAM applications that have bindings to data analysis libraries in Python. Double precision data in OpenFOAM is cast to a NumPy array using the NumPy C-API and Python modules may then be used for arbitrary data analysis and manipulation on flow-field information. We highlight how the proposed wrapper may be used for an in-situ online singular value decomposition (SVD) implemented in Python and accessed from the OpenFOAM solver PimpleFOAM. Here, `in-situ' refers to a programming paradigm that allows for a concurrent computation of the data analysis on the same computational resources utilized for the partial differential equation solver. In addition, to demonstrate data-parallel analyses, we deploy a distributed SVD, which collects snapshot data across the ranks of a distributed simulation to compute the global left singular vectors. Crucially, both OpenFOAM and Python share the same message passing interface (MPI) communicator for this deployment which allows Python objects and functions to exchange NumPy arrays across ranks. Subsequently, we provide scaling assessments of this distributed SVD on multiple nodes of Intel Broadwell and KNL architectures for canonical test cases such as the large eddy simulations of a backward facing step and a channel flow at friction Reynolds number of 395. Finally, we demonstrate the deployment of a deep neural network for compressing the flow-field information using an autoencoder to demonstrate an ability to use state-of-the-art machine learning tools in the Python ecosystem.			arxiv	['Dimitrios Fytanidis', 'Bethany Lusch', 'Venkatram Vishwanath', 'Saumil Patel']	70.0
171	Proceedings of the 6th European Conference on Python in Science (EuroSciPy 2013)	Pierre de Buyl	2014-05-01 14:22:09	http://arxiv.org/abs/1405.0166v1	These are the proceedings of the 6th European Conference on Python in Science, EuroSciPy 2013, that was held in Brussels (21-25 August 2013).			arxiv	['Nelle Varoquaux']	71.0
172	Py-oopsi: the python implementation of the fast-oopsi algorithm	Benyuan Liu	2014-05-06 06:05:04	http://arxiv.org/abs/1405.6181v1	Fast-oopsi was developed by Joshua Vogelstein in 2009, which is now widely used to extract neuron spike activities from calcium fluorescence signals. Here, we propose detailed implementation of the fast-oopsi algorithm in python programming language. Some corrections are also made to the original fast-oopsi paper.			arxiv	[]	72.0
173	Python Crypto Misuses in the Wild	Anna-Katharina Wickert	2021-09-02 17:32:41	http://arxiv.org/abs/2109.01109v1	Background: Previous studies have shown that up to 99.59 % of the Java apps using crypto APIs misuse the API at least once. However, these studies have been conducted on Java and C, while empirical studies for other languages are missing. For example, a controlled user study with crypto tasks in Python has shown that 68.5 % of the professional developers write a secure solution for a crypto task. Aims: To understand if this observation holds for real-world code, we conducted a study of crypto misuses in Python. Method: We developed a static analysis tool that covers common misuses of 5 different Python crypto APIs. With this analysis, we analyzed 895 popular Python projects from GitHub and 51 MicroPython projects for embedded devices. Further, we compared our results with the findings of previous studies. Results: Our analysis reveals that 52.26 % of the Python projects have at least one misuse. Further, some Python crypto libraries API design helps developers from misusing crypto functions, which were much more common in studies conducted with Java and C code. Conclusion: We conclude that we can see a positive impact of the good API design on crypto misuses for Python applications. Further, our analysis of MicroPython projects reveals the importance of hybrid analyses.			arxiv	['Lars Baumg√§rtner', 'Florian Breitfelder', 'Mira Mezini']	73.0
174	An array-oriented Python interface for FastJet	Aryan Roy	2022-02-08 14:57:31	http://arxiv.org/abs/2202.03911v1	Analysis on HEP data is an iterative process in which the results of one step often inform the next. In an exploratory analysis, it is common to perform one computation on a collection of events, then view the results (often with histograms) to decide what to try next. Awkward Array is a Scikit-HEP Python package that enables data analysis with array-at-a-time operations to implement cuts as slices, combinatorics as composable functions, etc. However, most C++ HEP libraries, such as FastJet, have an imperative, one-particle-at-a-time interface, which would be inefficient in Python and goes against the grain of the array-at-a-time logic of scientific Python. Therefore, we developed fastjet, a pip-installable Python package that provides FastJet C++ binaries, the classic (particle-at-a-time) Python interface, and the new array-oriented interface for use with Awkward Array. The new interface streamlines interoperability with scientific Python software beyond HEP, such as machine learning. In one case, adopting this library along with other array-oriented tools accelerated HEP analysis code by a factor of 20. It was designed to be easily integrated with libraries in the Scikit-HEP ecosystem, including Uproot (file I/O), hist (histogramming), Vector (Lorentz vectors), and Coffea (high-level glue). We discuss the design of the fastjet Python library, integrating the classic interface with the array oriented interface and with the Vector library for Lorentz vector operations. The new interface was developed as open source.			arxiv	['Jim Pivarski', 'Chad Wells Freer']	74.0
175	Deep Learning: From Basics to Building Deep Neural Networks with Python	Milad Vazan	2022-04-22 11:57:19	http://arxiv.org/abs/2205.01069v1	This book is intended for beginners who have no familiarity with deep learning. Our only expectation from readers is that they already have the basic programming skills in Python.			arxiv	[]	75.0
176	The Awkward World of Python and C++	Manasvi Goyal	2023-03-03 20:33:50	http://arxiv.org/abs/2303.02205v1	There are undeniable benefits of binding Python and C++ to take advantage of the best features of both languages. This is especially relevant to the HEP and other scientific communities that have invested heavily in the C++ frameworks and are rapidly moving their data analyses to Python. Version 2 of Awkward Array, a Scikit-HEP Python library, introduces a set of header-only C++ libraries that do not depend on any application binary interface. Users can directly include these libraries in their compilation rather than linking against platform-specific libraries. This new development makes the integration of Awkward Arrays into other projects easier and more portable as the implementation is easily separable from the rest of the Awkward Array codebase. The code is minimal, it does not include all of the code needed to use Awkward Arrays in Python, nor does it include references to Python or pybind11. The C++ users can use it to make arrays and then copy them to Python without any specialized data types - only raw buffers, strings, and integers. This C++ code also simplifies the process of just-in-time (JIT) compilation in ROOT. This implementation approach solves some of the drawbacks, like packaging projects where native dependencies can be challenging. In this paper, we demonstrate the technique to integrate C++ and Python by using a header-only approach. We also describe the implementation of a new LayoutBuilder and a GrowableBuffer. Furthermore, examples of wrapping the C++ data into Awkward Arrays and exposing Awkward Arrays to C++ without copying them are discussed.			arxiv	['Ianna Osborne', 'Jim Pivarski']	76.0
177	SlipCover: Near Zero-Overhead Code Coverage for Python	Juan Altmayer Pizzorno	2023-05-04 14:49:44	http://arxiv.org/abs/2305.02886v4	Coverage analysis is widely used but can suffer from high overhead. This overhead is especially acute in the context of Python, which is already notoriously slow (a recent study observes a roughly 30x slowdown vs. native code). We find that the state-of-the-art coverage tool for Python, coverage$.$py, introduces a median overhead of 180% with the standard Python interpreter. Slowdowns are even more extreme when using PyPy, a JIT-compiled Python implementation, with coverage$.$py imposing a median overhead of 1,300%. This performance degradation reduces the utility of coverage analysis in most use cases, including testing and fuzzing, and precludes its use in deployment. This paper presents SlipCover, a novel, near-zero overhead coverage analyzer for Python. SlipCover works without modifications to either the Python interpreter or PyPy. It first processes a program's AST to accurately identify all branches and lines. SlipCover then dynamically rewrites Python bytecodes to add lightweight instrumentation to each identified branch and line. At run time, SlipCover periodically de-instruments already-covered lines and branches. The result is extremely low overheads -- a median of just 5% -- making SlipCover suitable for use in deployment. We show its efficiency can translate to significant increases in the speed of coverage-based clients. As a proof of concept, we integrate SlipCover into TPBT, a targeted property-based testing system, and observe a 22x speedup.			arxiv	['Emery D Berger']	77.0
178	Cosmic Microwave Background Anisotropy Measurement From Python V	K. Coble	2001-12-21 01:09:38	http://arxiv.org/abs/astro-ph/0112506v2	We analyze observations of the microwave sky made with the Python experiment in its fifth year of operation at the Amundsen-Scott South Pole Station in Antarctica. After modeling the noise and constructing a map, we extract the cosmic signal from the data. We simultaneously estimate the angular power spectrum in eight bands ranging from large (l ~ 40) to small (l ~ 260) angular scales, with power detected in the first six bands. There is a significant rise in the power spectrum from large to smaller (l ~ 200) scales, consistent with that expected from acoustic oscillations in the early Universe. We compare this Python V map to a map made from data taken in the third year of Python. Python III observations were made at a frequency of 90 GHz and covered a subset of the region of the sky covered by Python V observations, which were made at 40 GHz. Good agreement is obtained both visually (with a filtered version of the map) and via a likelihood ratio test.			arxiv	['S. Dodelson', 'M. Dragovan', 'K. Ganga', 'L. Knox', 'J. Kovac', 'B. Ratra', 'T. Souradeep']	78.0
179	Solve the Master Equation by Python-An Introduction to the Python Computing Environment	Wei Fan	2011-03-02 00:46:20	http://arxiv.org/abs/1103.0325v4	A brief introduction to the Python computing environment is given. By solving the master equation encountered in quantum transport, we give an example of how to solve the ODE problems in Python. The ODE solvers used are the ZVODE routine in Scipy and the bsimp solver in GSL. For the former, the equation can be in its complex-valued form, while for the latter, it has to be rewritten to a real-valued form. The focus is on the detailed workflow of the implementation process, rather than on the syntax of the python language, with the hope to help readers simulate their own models in Python.			arxiv	['Yan Xu', 'Bing Chen', 'Qianqian Ye']	79.0
180	FluidFFT: common API (C++ and Python) for Fast Fourier Transform HPC libraries	Ashwin Vishnu Mohanan	2018-07-03 09:52:57	http://arxiv.org/abs/1807.01775v1	"The Python package fluidfft provides a common Python API for performing Fast Fourier Transforms (FFT) in sequential, in parallel and on GPU with different FFT libraries (FFTW, P3DFFT, PFFT, cuFFT). fluidfft is a comprehensive FFT framework which allows Python users to easily and efficiently perform FFT and the associated tasks, such as as computing linear operators and energy spectra. We describe the architecture of the package composed of C++ and Cython FFT classes, Python ""operator"" classes and Pythran functions. The package supplies utilities to easily test itself and benchmark the different FFT solutions for a particular case and on a particular machine. We present a performance scaling analysis on three different computing clusters and a microbenchmark showing that fluidfft is an interesting solution to write efficient Python applications using FFT."			arxiv	['Cyrille Bonamy', 'Pierre Augier']	80.0
181	Speeding simulation analysis up with yt and Intel Distribution for Python	Salvatore Cielo	2019-10-17 12:28:46	http://arxiv.org/abs/1910.07855v1	As modern scientific simulations grow ever more in size and complexity, even their analysis and post-processing becomes increasingly demanding, calling for the use of HPC resources and methods. yt is a parallel, open source post-processing python package for numerical simulations in astrophysics, made popular by its cross-format compatibility, its active community of developers and its integration with several other professional Python instruments. The Intel Distribution for Python enhances yt's performance and parallel scalability, through the optimization of lower-level libraries Numpy and Scipy, which make use of the optimized Intel Math Kernel Library (Intel-MKL) and the Intel MPI library for distributed computing. The library package yt is used for several analysis tasks, including integration of derived quantities, volumetric rendering, 2D phase plots, cosmological halo analysis and production of synthetic X-ray observation. In this paper, we provide a brief tutorial for the installation of yt and the Intel Distribution for Python, and the execution of each analysis task. Compared to the Anaconda python distribution, using the provided solution one can achieve net speedups up to 4.6x on Intel Xeon Scalable processors (codename Skylake).			arxiv	['Luigi Iapichino', 'Fabio Baruffa']	81.0
182	Scalene: Scripting-Language Aware Profiling for Python	Emery D. Berger	2020-06-06 14:43:09	http://arxiv.org/abs/2006.03879v2	"Existing profilers for scripting languages (a.k.a. ""glue"" languages) like Python suffer from numerous problems that drastically limit their usefulness. They impose order-of-magnitude overheads, report information at too coarse a granularity, or fail in the face of threads. Worse, past profilers---essentially variants of their counterparts for C---are oblivious to the fact that optimizing code in scripting languages requires information about code spanning the divide between the scripting language and libraries written in compiled languages. This paper introduces scripting-language aware profiling, and presents Scalene, an implementation of scripting-language aware profiling for Python. Scalene employs a combination of sampling, inference, and disassembly of byte-codes to efficiently and precisely attribute execution time and memory usage to either Python, which developers can optimize, or library code, which they cannot. It includes a novel sampling memory allocator that reports line-level memory consumption and trends with low overhead, helping developers reduce footprints and identify leaks. Finally, it introduces a new metric, copy volume, to help developers root out insidious copying costs across the Python/library boundary, which can drastically degrade performance. Scalene works for single or multi-threaded Python code, is precise, reporting detailed information at the line granularity, while imposing modest overheads (26%--53%)."			arxiv	[]	82.0
183	Extending Python for Quantum-Classical Computing via Quantum Just-in-Time Compilation	Thien Nguyen	2021-05-10 21:11:21	http://arxiv.org/abs/2105.04671v1	Python is a popular programming language known for its flexibility, usability, readability, and focus on developer productivity. The quantum software community has adopted Python on a number of large-scale efforts due to these characteristics, as well as the remote nature of near-term quantum processors. The use of Python has enabled quick prototyping for quantum code that directly benefits pertinent research and development efforts in quantum scientific computing. However, this rapid prototyping ability comes at the cost of future performant integration for tightly-coupled CPU-QPU architectures with fast-feedback. Here we present a language extension to Python that enables heterogeneous quantum-classical computing via a robust C++ infrastructure for quantum just-in-time (QJIT) compilation. Our work builds off the QCOR C++ language extension and compiler infrastructure to enable a single-source, quantum hardware-agnostic approach to quantum-classical computing that retains the performance required for tightly coupled CPU-QPU compute models. We detail this Pythonic extension, its programming model and underlying software architecture, and provide a robust set of examples to demonstrate the utility of our approach.			arxiv	['Alexander J. McCaskey']	83.0
184	An Empirical Study of Automated Unit Test Generation for Python	Stephan Lukasczyk	2021-11-09 08:54:33	http://arxiv.org/abs/2111.05003v2	Various mature automated test generation tools exist for statically typed programming languages such as Java. Automatically generating unit tests for dynamically typed programming languages such as Python, however, is substantially more difficult due to the dynamic nature of these languages as well as the lack of type information. Our Pynguin framework provides automated unit test generation for Python. In this paper, we extend our previous work on Pynguin to support more aspects of the Python language, and by studying a larger variety of well-established state of the art test-generation algorithms, namely DynaMOSA, MIO, and MOSA. Furthermore, we improved our Pynguin tool to generate regression assertions, whose quality we also evaluate. Our experiments confirm that evolutionary algorithms can outperform random test generation also in the context of Python, and similar to the Java world, DynaMOSA yields the highest coverage results. However, our results also demonstrate that there are still fundamental remaining issues, such as inferring type information for code without this information, currently limiting the effectiveness of test generation for Python.			arxiv	['Florian Kroi√ü', 'Gordon Fraser']	84.0
185	An Exploratory Study on the Predominant Programming Paradigms in Python Code	Robert Dyer	2022-09-05 08:03:20	http://arxiv.org/abs/2209.01817v1	Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.			arxiv	['Jigyasa Chauhan']	85.0
186	A Data Set of Generalizable Python Code Change Patterns	Akalanka Galappaththi	2023-04-11 04:59:26	http://arxiv.org/abs/2304.04983v1	Mining repetitive code changes from version control history is a common way of discovering unknown change patterns. Such change patterns can be used in code recommender systems or automated program repair techniques. While there are such tools and datasets exist for Java, there is little work on finding and recommending such changes in Python. In this paper, we present a data set of manually vetted generalizable Python repetitive code change patterns. We create a coding guideline to identify generalizable change patterns that can be used in automated tooling. We leverage the mined change patterns from recent work that mines repetitive changes in Python projects and use our coding guideline to manually review the patterns. For each change, we also record a description of the change and why it is applied along with other characteristics such as the number of projects it occurs in. This review process allows us to identify and share 72 Python change patterns that can be used to build and advance Python developer support tools.			arxiv	['Sarah Nadi']	86.0
187	Scalable Demand-Driven Call Graph Generation for Python	Yixuan Yan	2023-05-10 07:40:05	http://arxiv.org/abs/2305.05949v1	Call graph generation is the foundation of inter-procedural static analysis. PyCG is the state-of-the-art approach for generating call graphs for Python programs. Unfortunately, PyCG does not scale to large programs when adapted to whole-program analysis where dependent libraries are also analyzed. Further, PyCG does not support demand-driven analysis where only the reachable functions from given entry functions are analyzed. Moreover, PyCG is flow-insensitive and does not fully support Python's features, hindering its accuracy. To overcome these drawbacks, we propose a scalable demand-driven approach for generating call graphs for Python programs, and implement it as a prototype tool Jarvis. Jarvis maintains an assignment graph (i.e., points-to relations between program identifiers) for each function in a program to allow reuse and improve scalability. Given a set of entry functions as the demands, Jarvis generates the call graph on-the-fly, where flow-sensitive intra-procedural analysis and inter-procedural analysis are conducted in turn. Our evaluation on a micro-benchmark of 135 small Python programs and a macro-benchmark of 6 real-world Python applications has demonstrated that Jarvis can significantly improve PyCG by at least 67% faster in time, 84% higher in precision, and at least 10% higher in recall.			arxiv	['Kaifeng Huang', 'Bihuan Chen', 'Zixin Tao', 'Xin Peng']	87.0
188	Py-Tetrad and RPy-Tetrad: A New Python Interface with R Support for Tetrad Causal Search	Joseph D. Ramsey	2023-08-13 16:29:05	http://arxiv.org/abs/2308.07346v1	We give novel Python and R interfaces for the (Java) Tetrad project for causal modeling, search, and estimation. The Tetrad project is a mainstay in the literature, having been under consistent development for over 30 years. Some of its algorithms are now classics, like PC and FCI; others are recent developments. It is increasingly the case, however, that researchers need to access the underlying Java code from Python or R. Existing methods for doing this are inadequate. We provide new, up-to-date methods using the JPype Python-Java interface and the Reticulate Python-R interface, directly solving these issues. With the addition of some simple tools and the provision of working examples for both Python and R, using JPype and Reticulate to interface Python and R with Tetrad is straightforward and intuitive.			arxiv	['Bryan Andrews']	88.0
189	High performance Python for direct numerical simulations of turbulent flows	Mikael Mortensen	2016-02-11 08:12:37	http://arxiv.org/abs/1602.03638v1	Direct Numerical Simulations (DNS) of the Navier Stokes equations is an invaluable research tool in fluid dynamics. Still, there are few publicly available research codes and, due to the heavy number crunching implied, available codes are usually written in low-level languages such as C/C++ or Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS code that nearly matches the performance of C++ for thousands of processors and billions of unknowns. We also describe a version optimized through Cython, that is found to match the speed of C++. The solvers are written from scratch in Python, both the mesh, the MPI domain decomposition, and the temporal integrators. The solvers have been verified and benchmarked on the Shaheen supercomputer at the KAUST supercomputing laboratory, and we are able to show very good scaling up to several thousand cores. A very important part of the implementation is the mesh decomposition (we implement both slab and pencil decompositions) and 3D parallel Fast Fourier Transforms (FFT). The mesh decomposition and FFT routines have been implemented in Python using serial FFT routines (either NumPy, pyFFTW or any other serial FFT module), NumPy array manipulations and with MPI communications handled by MPI for Python (mpi4py). We show how we are able to execute a 3D parallel FFT in Python for a slab mesh decomposition using 4 lines of compact Python code, for which the parallel performance on Shaheen is found to be slightly better than similar routines provided through the FFTW library. For a pencil mesh decomposition 7 lines of code is required to execute a transform.			arxiv	['Hans Petter Langtangen']	89.0
190	PyCG: Practical Call Graph Generation in Python	Vitalis Salis	2021-02-28 18:49:25	http://arxiv.org/abs/2103.00587v1	"Call graphs play an important role in different contexts, such as profiling and vulnerability propagation analysis. Generating call graphs in an efficient manner can be a challenging task when it comes to high-level languages that are modular and incorporate dynamic features and higher-order functions. Despite the language's popularity, there have been very few tools aiming to generate call graphs for Python programs. Worse, these tools suffer from several effectiveness issues that limit their practicality in realistic programs. We propose a pragmatic, static approach for call graph generation in Python. We compute all assignment relations between program identifiers of functions, variables, classes, and modules through an inter-procedural analysis. Based on these assignment relations, we produce the resulting call graph by resolving all calls to potentially invoked functions. Notably, the underlying analysis is designed to be efficient and scalable, handling several Python features, such as modules, generators, function closures, and multiple inheritance. We have evaluated our prototype implementation, which we call PyCG, using two benchmarks: a micro-benchmark suite containing small Python programs and a set of macro-benchmarks with several popular real-world Python packages. Our results indicate that PyCG can efficiently handle thousands of lines of code in less than a second (0.38 seconds for 1k LoC on average). Further, it outperforms the state-of-the-art for Python in both precision and recall: PyCG achieves high rates of precision ~99.2%, and adequate recall ~69.9%. Finally, we demonstrate how PyCG can aid dependency impact analysis by showcasing a potential enhancement to GitHub's ""security advisory"" notification service using a real-world example."			arxiv	['Thodoris Sotiropoulos', 'Panos Louridas', 'Diomidis Spinellis', 'Dimitris Mitropoulos']	90.0
191	PyArmadillo: a streamlined linear algebra library for Python	Jason Rumengan	2021-04-22 15:13:33	http://arxiv.org/abs/2104.11120v4	PyArmadillo is a linear algebra library for the Python language, with the aim of closely mirroring the programming interface of the widely used Armadillo C++ library, which in turn is deliberately similar to Matlab. PyArmadillo hence facilitates algorithm prototyping with Matlab-like syntax directly in Python, and relatively straightforward conversion of PyArmadillo-based Python code into performant Armadillo-based C++ code. The converted code can be used for purposes such as speeding up Python-based programs in conjunction with pybind11, or the integration of algorithms originally prototyped in Python into larger C++ codebases. PyArmadillo provides objects for matrices and cubes, as well as over 200 associated functions for manipulating data stored in the objects. Integer, floating point and complex numbers are supported. Various matrix factorisations are provided through integration with LAPACK, or one of its high performance drop-in replacements such as Intel MKL or OpenBLAS. PyArmadillo is open-source software, distributed under the Apache 2.0 license; it can be obtained at https://pyarma.sourceforge.io or via the Python Package Index in precompiled form.			arxiv	['Terry Yue Zhuo', 'Conrad Sanderson']	91.0
192	Python for Smarter Cities: Comparison of Python libraries for static and interactive visualisations of large vector data	Gregor Herda	2022-02-26 10:23:29	http://arxiv.org/abs/2202.13105v1	Local governments, as part of 'smart city' initiatives and to promote interoperability, are increasingly incorporating open-source software into their data management, analysis, and visualisation workflows. Python, with its concise and natural syntax, presents a low barrier to entry for municipal staff without computer science backgrounds. However, with regard to geospatial visualisations in particular, the range of available Python libraries has diversified to such an extent that identifying candidate libraries for specific use cases is a challenging undertaking. This study therefore assesses prominent, actively-developed visualisation libraries in the Python ecosystem with respect to their suitability for producing visualisations of large vector datasets. A simple visualisation task common in urban development is used to produce near-identical thematic maps across static and an interactive 'tracks' of comparison. All short-listed libraries were able to generate the sample map products for both a small and larger dataset. Code complexity differed more strongly for interactive visualisations. Formal and informal documentation channels are highlighted to outline available resources for flattening learning curves. CPU runtimes for the Python-based portion of the process chain differed starkly for both tracks, pointing to avenues for further research. These results demonstrate that the Python ecosystem offers local governments powerful tools, free of vendor lock-in and licensing fees, to produce performant and consistently formatted visualisations for both internal and public distribution.			arxiv	['Robert McNabb']	92.0
193	An Empirical Study of Fault Localization in Python Programs	Mohammad Rezaalipour	2023-05-31 13:21:30	http://arxiv.org/abs/2305.19834v2	Despite its massive popularity as a programming language, especially in novel domains like data science programs, there is comparatively little research about fault localization that targets Python. Even though it is plausible that several findings about programming languages like C/C++ and Java -- the most common choices for fault localization research -- carry over to other languages, whether the dynamic nature of Python and how the language is used in practice affect the capabilities of classic fault localization approaches remain open questions to investigate. This paper is the first large-scale empirical study of fault localization on real-world Python programs and faults. Using Zou et al.'s recent large-scale empirical study of fault localization in Java as the basis of our study, we investigated the effectiveness (i.e., localization accuracy), efficiency (i.e., runtime performance), and other features (e.g., different entity granularities) of seven well-known fault-localization techniques in four families (spectrum-based, mutation-based, predicate switching, and stack-trace based) on 135 faults from 13 open-source Python projects from the BugsInPy curated collection. The results replicate for Python several results known about Java, and shed light on whether Python's peculiarities affect the capabilities of fault localization. The replication package that accompanies this paper includes detailed data about our experiments, as well as the tool FauxPy that we implemented to conduct the study.			arxiv	['Carlo A. Furia']	93.0
194	Does Python Smell Like Java? Tool Support for Design Defect Discovery in Python	Nicole Vavrov√°	2017-03-31 12:47:50	http://arxiv.org/abs/1703.10882v1	The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32'058'823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.			arxiv	['Vadim Zaytsev']	94.0
195	Python I, II, and III CMB Anisotropy Measurement Constraints on Open and Flat-Lambda CDM Cosmogonies	Graca Rocha	1999-05-11 18:16:35	http://arxiv.org/abs/astro-ph/9905127v1	We use Python I, II, and III cosmic microwave background anisotropy data to constrain cosmogonies. We account for the Python beamwidth and calibration uncertainties. We consider open and spatially-flat-Lambda cold dark matter cosmogonies, with nonrelativistic-mass density parameter Omega_0 in the range 0.1--1, baryonic-mass density parameter Omega_B in the range (0.005--0.029) h^{-2}, and age of the universe t_0 in the range (10--20) Gyr. Marginalizing over all parameters but Omega_0, the combined Python data favors an open (spatially-flat-Lambda) model with Omega_0 simeq 0.2 (0.1). At the 2 sigma confidence level model normalizations deduced from the combined Python data are mostly consistent with those drawn from the DMR, UCSB South Pole 1994, ARGO, MAX 4 and 5, White Dish, and SuZIE data sets.			arxiv	['Radoslaw Stompor', 'Ken Ganga', 'Bharat Ratra', 'Stephen R. Platt', 'Naoshi Sugiyama', 'Krzysztof M. Gorski']	95.0
196	PyNetMet: Python tools for efficient work with networks and metabolic models	D. Gamermann	2012-11-30 09:25:06	http://arxiv.org/abs/1211.7196v1	Background: The study of genome-scale metabolic models and their underlying networks is one of the most important fields in systems biology. The complexity of these models and their description makes the use of computational tools an essential element in their research. Therefore there is a strong need of efficient and versatile computational tools for the research in this area. Results: In this manuscript we present PyNetMet, a Python library of tools to work with networks and metabolic models. These are open-source free tools for use in a Python platform, which adds considerably versatility to them when compared with their desktop software similars. On the other hand these tools allow one to work with different standards of metabolic models (OptGene and SBML) and the fact that they are programmed in Python opens the possibility of efficient integration with any other already existing Python tool. Conclusions: PyNetMet is, therefore, a collection of computational tools that will facilitate the research work with metabolic models and networks.			arxiv	['A. Montagud', 'R. A. Jaime Infante', 'J. Triana', 'P. F. de C√≥rdoba', 'J. F. Urchuegu√≠a']	96.0
197	A Python-based Post-processing Toolset For Seismic Analyses	Steve Brasier	2014-12-19 16:09:16	http://arxiv.org/abs/1412.6410v1	This paper discusses the design and implementation of a Python-based toolset to aid in assessing the response of the UK's Advanced Gas Reactor nuclear power stations to earthquakes. The seismic analyses themselves are carried out with a commercial Finite Element solver, but understanding the raw model output this produces requires customised post-processing and visualisation tools. Extending the existing tools had become increasingly difficult and a decision was made to develop a new, Python-based toolset. This comprises of a post-processing framework (aftershock) which includes an embedded Python interpreter, and a plotting package (afterplot) based on numpy and matplotlib. The new toolset had to be significantly more flexible and easier to maintain than the existing code-base, while allowing the majority of development to be carried out by engineers with little training in software development. The resulting architecture will be described with a focus on exploring how the design drivers were met and the successes and challenges arising from the choices made.			arxiv	['Fred Pollard']	97.0
198	Rabacus: A Python Package for Analytic Cosmological Radiative Transfer Calculations	Gabriel Altay	2015-02-10 07:03:42	http://arxiv.org/abs/1502.02798v2	We describe Rabacus, a Python package for calculating the transfer of hydrogen ionizing radiation in simplified geometries relevant to astronomy and cosmology. We present example solutions for three specific cases: 1) a semi-infinite slab gas distribution in a homogeneous isotropic background, 2) a spherically symmetric gas distribution with a point source at the center, and 3) a spherically symmetric gas distribution in a homogeneous isotropic background. All problems can accommodate arbitrary spectra and density profiles as input. The solutions include a treatment of both hydrogen and helium, a self-consistent calculation of equilibrium temperatures, and the transfer of recombination radiation. The core routines are written in Fortran 90 and then wrapped in Python leading to execution speeds thousands of times faster than equivalent routines written in pure Python. In addition, all variables have associated units for ease of analysis. The software is part of the Python Package Index and the source code is available on Bitbucket at https://bitbucket.org/galtay/rabacus . In addition, installation instructions and a detailed users guide are available at http://pythonhosted.org//rabacus .			arxiv	['John Wise']	98.0
199	Weighted graph algorithms with Python	A. Kapanowski	2015-04-29 12:20:20	http://arxiv.org/abs/1504.07828v1	Python implementation of selected weighted graph algorithms is presented. The minimal graph interface is defined together with several classes implementing this interface. Graph nodes can be any hashable Python objects. Directed edges are instances of the Edge class. Graphs are instances of the Graph class. It is based on the adjacency-list representation, but with fast lookup of nodes and neighbors (dict-of-dict structure). Other implementations of this class are also possible. In this work, many algorithms are implemented using a unified approach. There are separate classes and modules devoted to different algorithms. Three algorithms for finding a minimum spanning tree are implemented: the Boruvka's algorithm, the Prim's algorithm (three implementations), and the Kruskal's algorithm. Three algorithms for solving the single-source shortest path problem are implemented: the dag shortest path algorithm, the Bellman-Ford algorithm, and the Dijkstra's algorithm (two implementations). Two algorithms for solving all-pairs shortest path problem are implemented: the Floyd-Warshall algorithm and the Johnson's algorithm. All algorithms were tested by means of the unittest module, the Python unit testing framework. Additional computer experiments were done in order to compare real and theoretical computational complexity. The source code is available from the public GitHub repository.			arxiv	['≈Å. Ga≈Çuszka']	99.0
200	Efficient Range Joins in Pandas	Unknown	2023-12-29 12:07:11	https://www.reddit.com/r/Python/comments/18tlpg0/efficient_range_joins_in_pandas/	[pyjanitor](https://pyjanitor-devs.github.io/pyjanitor/) has a [conditional_join](https://pyjanitor-devs.github.io/pyjanitor/api/functions/#janitor.functions.conditional_join.conditional_join) function that offers good performance on range joins in Pandas, avoiding the cross/cartesian join. I also wrote a [blog post](https://samukweku.github.io/data-wrangling-blog/notebooks/Fast-and-Efficient-Inequality-Joins-in-Pandas.html), as well as a [pyohio presentation](https://youtu.be/AjdBLOAhgDI?si=jWOP7QlzPpTcr7II) regarding inequality joins in pandas. Feel free to raise an issue on the [pyjanitor github issue page](https://github.com/pyjanitor-devs/pyjanitor/issues) if you encounter any difficulties.	1.0	t3_18tlpg0	reddit		
201	Stockstir is a Python project that lets you get any company stock price instantly from any script at no cost.	Unknown	2023-12-28 16:21:09	https://www.reddit.com/r/Python/comments/18sxqsc/stockstir_is_a_python_project_that_lets_you_get/	"Hello! This is Stockstir, a project I did a while ago. I just wanted to share it in case any of you have update suggestions, and also because it might be useful to all of you wanting to get stock prices from your scripts instantly. This project allows anyone to instantly get a company stock price from any of your python scripts. Not only that, but it includes other tools and features to help you gather necessary stock data, such as saving the data to a file, or even a multi-data gathering function with adjustable parameters that include antiBan, random user agent,the delay per request, and more. You can get it here: [Stockstir Link](https://github.com/PatzEdi/Stockstir) As soon as it is downloaded, you can place this line of code in your script: ``` import Stockstir price = Stockstir.Tools.getSinglePrice(""ticker/stockSymbol"") print(price) ``` Make sure to replace what is in between the quotes with the actual Ticker symbol you want to gather the price for. Edit: Based on the comments that were posted on this thread, I am currently working on a new update for Stockstir. Thank you for the great advice! Edit 2: V2 has been released! Check out the details here: [V2 Post Link](https://www.reddit.com/r/Python/s/NshHDcdeuO)"	6.0	t3_18sxqsc	reddit		
202	pytest mock	Unknown	2023-12-28 16:23:09	https://www.reddit.com/r/Python/comments/18sxsig/pytest_mock/	As per today what is the most bulletproof real life python unit testing/mocking framework? I found after some research that is pytest, but for the mocks which i need to massively use, the standard (= most used library) is unittest.mock with patch or the wrapper pytest-mock? &#x200B;	16.0	t3_18sxsig	reddit		
203	Populate template automatically from resumes	Unknown	2023-12-29 09:35:35	https://www.reddit.com/r/Python/comments/18tjccx/populate_template_automatically_from_resumes/	Hi everyone! I‚Äôm playing around with this and my goal is set up a service where I can upload any resume/cv then have it broken down into the core parts I‚Äôm interested in such as job, employers, job titles, key skills, dates of employment etc that will then populate a standardised template I‚Äôve setup. Current thoughts are to use the ChatGPT API to create a MVP/ proof of concept via python. Is the easiest ways to convert the cvs and the template to JSON‚Äôs, and pull that data across using tags or should I be considering another approach? I‚Äôve been using ChatGPT to analyse CV‚Äôs I upload into it and it s been doing pretty good so far in terms of analysing the resumes. If I get this going I would expect to train a model or something like that to increase efficiency. Obviously the template would need to be dynamic as there‚Äôs no set amount of jobs anyone has on their CV/ resume. The code I‚Äôve got is rough as so far but I‚Äôm keen on how others would approach this situation? TIA	1.0	t3_18tjccx	reddit		
204	Nur the AI empowered self actualizing documentation chat bot	Unknown	2023-12-29 20:58:03	https://www.reddit.com/r/Python/comments/18tx9fj/nur_the_ai_empowered_self_actualizing/	An AI powered system that helps you find relevant documentation, identifies gaps in its knowledge, collects feedback and develops your knowledge base based on questions, answers, feedback and unanswered questions. Open source. Open for contributions. Supports confluence and slack and developing further. Nur The self actualizing documentation framework that heals its knowledge gaps as naturally as a ray of light Custom GPT to discuss the code base Chat with the custom gpt about the solution: https://chat.openai.com/g/g-zKBLXtfrD-shams-nur Feature list Done: add a confluence space (url credentials and update interval) Pulls the confluence space and stores it in a sqlite database Vectorizes the confluence space pages and stores the embeds in a chroma db collection Uses the vectorized embeds to find the most similar pages to a question Creates an assistant with the relevant pages and allows it to engage to provide the answer Listens on specific slack channels for questions relevant to its domain Implement fast response using Gpt-4 Turbo without assistant Implemented persist queue for page content retrieval and vectorization Todo: Move slack processed ids to sqlite database setup last update date and schedule to update confluence space with log in db store embeds in database create new vector database nightly and on trigger from sql database add questions, answers and reactions (- enable confluence edit or new page recommendation) add credibility rating to database trivia question collector consider removing assistants all together https://github.com/MDGrey33/Nur	0.0	t3_18tx9fj	reddit		
205	Analytics with Python	Unknown	2023-12-29 01:18:30	https://www.reddit.com/r/Python/comments/18tacyc/analytics_with_python/	New Python data analysis tutorial website at [https://dataprep.us](https://dataprep.us) (or [https://pareek.org/ba](https://pareek.org/ba)). Includes notebooks and videos on data munging, regression, shallow ML, deep learning, time series and NLP. Just a simple read-the-docs style website with no logins, sign-ups, ads or anything. Would love feedback and suggestions. Thanks in advance.	0.0	t3_18tacyc	reddit		
206	Python lib written in rust for fast bounding box manipulation	Unknown	2023-12-28 19:45:55	https://www.reddit.com/r/Python/comments/18t2nwe/python_lib_written_in_rust_for_fast_bounding_box/	See the repo here: https://github.com/Smirkey/powerboxes Some functions available are NMS (classical or with rtree) distance metrics (IoU, GIoU) and utils (box format conversion, area computation) ! Feedback would be greatly appreciated, hope it can be useful in your computer vision projects :)	1.0	t3_18t2nwe	reddit		
207	An implementation of the Python turtle library in C++ using SDL2	Unknown	2023-12-28 18:56:25	https://www.reddit.com/r/Python/comments/18t1i5k/an_implementation_of_the_python_turtle_library_in/	[https://github.com/dafiliks/tortoise](https://github.com/dafiliks/tortoise) Check it out, read the README and give feedback, thanks!	0.0	t3_18t1i5k	reddit		
208	embuild - a small tool for embedded C/CMake project library management	Unknown	2023-12-28 12:47:46	https://www.reddit.com/r/Python/comments/18st51s/embuild_a_small_tool_for_embedded_ccmake_project/	I wrote [embuild](https://pypi.org/project/embuild/) \- a small tool. Also I hooked up a website [listing available libraries](https://embuild.dev/). For now these are mine only [from my curated library repository](https://github.com/g2labs-grzegorz-grzeda/embuild-repository). Here is the my [embuild GitHub repo link](https://github.com/g2labs-grzegorz-grzeda/embuild), though I encourage you to install it through `pip`. I was looking for a small and robust tool to handle my embedded C projects. I use CMake for project building. I know there are others ready, like `conan` or `vcpkg` but they were way to overblown. So I decided to write my own. &#x200B; Give it a shot and tell me what you think.	0.0	t3_18st51s	reddit		
209	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2023-12-29 00:01:10	https://www.reddit.com/r/Python/comments/18t8nwl/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	0.0	t3_18t8nwl	reddit		
210	Duotrigordle Practice	Unknown	2023-12-28 17:35:48	https://www.reddit.com/r/Python/comments/18szjwd/duotrigordle_practice/	The popular game Wordle has an alternative version, called Duotrigordle. In this version of the game, instead of trying to solve one word, you are trying to solve 32 words at the same time. A couple weeks ago I set out on a journey to set the world record for the fastest time to complete all 32 words. The world record, 19.62 is incredibly hard to beat, and I realized that I wouldn't be able to do it without practicing certain skills. This is why I created Duotrigordle Practice, a tool to practice specific skills related to this game. Hopefully now I will be able to break the world record. However, this practice tool could be fun for some people, and I figured I might as well share it out to the public. It is still a work in progress, but it's mostly there. I hope you enjoy, and if you have suggestions, bugs, or anything else, feel free to dm me on discord at minestone4306 or post it on github. Here is the link to the github which has all of the source code and download instructions: [https://github.com/dcjvliet/Duotrigordle](https://github.com/dcjvliet/Duotrigordle)	0.0	t3_18szjwd	reddit		
211	RuGiVi - Adult media landscape browser	Unknown	2023-12-27 17:38:11	https://www.reddit.com/r/Python/comments/18s68rl/rugivi_adult_media_landscape_browser/	I wrote a python app using pygame to fly over an image collection (in my case, an adult collection) and view thousands of images at once. Zoom in and out from one image to small thumbnails with your mousewheel in seconds. Here is the code: [https://github.com/pronopython/rugivi](https://github.com/pronopython/rugivi) &#x200B; https://preview.redd.it/mgw70srpiv8c1.jpg?width=1200&format=pjpg&auto=webp&s=03e1324c4396a621d04a8673cc23a91dc36dd66c All images are grouped as you have them on your disk and arranged in a huge landscape. RuGiVi can work with hundred thousand of images at once. It runs under Linux and Windows. \- Works with hundreds of thousands of images at the same time \- Tested with around 700.000 images (see the world map shown here), that's a RuGiVi Pixel size of 4.600.000 x 4.400.000 pixels or 20.240.000 Megapixels or 10.120.000 Full HD Screens to be scrolled through \- Dynamic view rendering - screen is updated partially when drawing takes more time \- Thumbnails are cached in a database I'm really looking forward on feedback (it's still an alpha release), ideas... :-) I have more open source apps written in python to organize and enjoy a collection, have a look at them: [https://github.com/pronopython](https://github.com/pronopython)	13.0	t3_18s68rl	reddit		
212	I made a Django webapp to create memorial pages quickly and easily	Unknown	2023-12-28 00:04:16	https://www.reddit.com/r/Python/comments/18sffil/i_made_a_django_webapp_to_create_memorial_pages/	"Hi all! After several months of having no ideas to launch or work on, I'm finally launching a small SaaS to create memorial pages for our beloved ones who have passed away. To be honest, I got this idea in the worst scenario. Here's the story... Recently, one of my family members passed away, and as a software engineer, I felt compelled to use my Python / Django skills to create something meaningful in his honor. I discovered nice platforms for creating memorial pages, like Cake, Forever Missed, Much Loved, and so forth. However, given my mental state at the time, I felt there were too many steps and overwhelming options to get this done quickly. So, I turned to **carrd.com** and created a very basic page with a link to send emails to my wife and me, where you could share condolences, support messages, book recommendations, and so forth. After that, I thought, ""Why not create a platform to build these kinds of pages in just one step and start receiving support from friends and family without feeling pressured to answer the messages?"" And after two weeks, I created [easytribute.com](http://easytribute.com/) Source code [https://github.com/mariorojas/easytribute](https://github.com/mariorojas/easytribute) Feel free to take a look, and thank you all for your support and feedback."	4.0	t3_18sffil	reddit		
213	Update on My Python Tool for OpenAI API - New Features and Improvements!	Unknown	2023-12-28 18:35:57	https://www.reddit.com/r/Python/comments/18t10k6/update_on_my_python_tool_for_openai_api_new/	Hello! A little while back, I shared [a Python tool I was working on to manage the OpenAI API](https://www.reddit.com/r/Python/comments/18f8i0y/gpt_helped_me_build_a_python_tool_to_work_with/) (‚Üê link to original post), and I'm super excited to update you all on some new features and improvements! As someone who's relatively new to Python, diving into this project has been an incredible learning experience. I've been relying on GPT's guidance to help me overcome hurdles and flesh out my ideas, and it's amazing how much progress we've made together. Click here to check it out: üëâ [OpenAI Python Tools on GitHub](https://github.com/richarddas/OpenAI-Python-Tools) Here‚Äôs what‚Äôs new in the tool: * **Enhanced Error Handling**: I've added robust error handling, so the tool is now more stable and user-friendly, especially when dealing with errors or incorrect inputs. * **Local Caching for Threads**: To streamline thread management, the tool now caches thread IDs locally, making it easier to keep track of your active threads. * **UI Clarity Improvements**: I've tweaked the user interface to be less confusing, especially when navigating between menus and making selections. These updates aim to make the tool even more intuitive and efficient for managing assistants, threads, and files within the OpenAI environment. As a beginner in Python, but with a background in other programming languages, this project has been a great challenge. Working with GPT not only helped me build a tool that I needed, but it also accelerated my learning in Python. It's empowering to realize that with the right tools and resources, we can create solutions and explore our ideas, no matter our starting skill level. I'd love for you all to try out the updated tool and share any feedback or suggestions. Your insights are invaluable and will help make this tool even better for everyone working with the OpenAI API. Thanks for all the support and encouragement so far!	1.0	t3_18t10k6	reddit		
214	Django python backend for a dating social app üêç	Unknown	2023-12-27 21:21:22	https://www.reddit.com/r/Python/comments/18sbk1v/django_python_backend_for_a_dating_social_app/	Good project to play around and explore Django REST features &#x200B; Repo -> [https://github.com/damianstone/toogether-backend](https://github.com/damianstone/toogether-backend) Frontend repo -> [https://github.com/damianstone/toogether-mobile](https://github.com/damianstone/toogether-mobile) &#x200B; **Some Django REST features used** \- Channels and websockets \- Geolocation \- Pagination \- Auth token \- ModelViewSets &#x200B; [Figma screen of the app functionalities](https://preview.redd.it/s1yf2ykxmw8c1.png?width=6801&format=png&auto=webp&s=c80e7671cc5430772830b6466cc3dd2f3276c290) **App features** \- login / register using auth token \- user profile \- matching algorithm \- swipe group and single profiles \- create group profiles using an invitation code \- group chat and 1-1 chats \- report and block \- recovery password &#x200B;	10.0	t3_18sbk1v	reddit		
215	ML Program using face recognition which analyze your face structure and measure how close it is to golden ratio	Unknown	2023-12-28 15:45:01	https://www.reddit.com/r/Python/comments/18swvoo/ml_program_using_face_recognition_which_analyze/	I know it's pretty simple project but if you can add some feature or implementation I am open to contributions. [https://github.com/Crevils/faceGoldenRatio](https://github.com/Crevils/faceGoldenRatio) (Give a star)	0.0	t3_18swvoo	reddit		
216	Blackline + Python	Unknown	2023-12-28 00:50:57	https://www.reddit.com/r/Python/comments/18sginp/blackline_python/	Are there any accountants that have used Python in conjunction with Blackline or even replaced some functionality such as Blackline matching and automated journal entry processes? If so what and how?	4.0	t3_18sginp	reddit		
217	baozi- a dataclass alternative with a greater emphasis on the 'class' aspect.	Unknown	2023-12-27 23:57:34	https://www.reddit.com/r/Python/comments/18sf9jy/baozi_a_dataclass_alternative_with_a_greater/	"github repo: [baozi-github](https://github.com/raceychan/baozi) Hi guys, I am here to introduce my new project 'baozi' to you. baozi aims to be a strictly superset of dataclass, so that user can have a very flat learning curve. besides what datalcass offers, baozi adds extra features like: * Inheritance * Immutability & Immutability check * Keyword-only arguments and random order attributes * Support user-defined __pre_init__ method that gets executed before class instantiation you can use baozi like this: ```python from baozi import Struct, FrozenStruct, field from datetime import datetime class Event(FrozenStruct): name: str create_at: datetime = field(default_factory=datetime.now) >> e = Event(name=""event"") >> assert isinstance(e.created_at, datetime) >> e.created_at = datetime.now() dataclasses.FrozenInstanceError: cannot assign to field 'created_at' ``` or ``` from baozi import Struct from dataclasses import asdict class B(Struct): name: str age: int @classmethod def __pre_init__(cls, **data): data[""age""] = int(data[""age""]) return data assert asdict(B(name=""name"", age=""15"")) == {""name"": ""name"", ""age"": 15} ``` The very first motivation for this project was that during developement I was getting really tired of duplicated @dataclass(...) decorators all over my codebase, so I decided to implement a more class-based dataclass alternative. since I am also a big fan of DDD, I added some extra features for improvements on immutability, hasing and memory efficiency. baozi is still in its early deveolopment stage, but it now has 100% test coverage, I hope you would be nice enough to give it a try, any feedback is highly appreciated. There is also a rationale for the existence of this project in the github README, I hope it will help explain why not go stright to pydantic or similar projects."	2.0	t3_18sf9jy	reddit		
218	Raycasting game in Python and Pygame Part 3	Unknown	2023-12-27 16:52:20	https://www.reddit.com/r/Python/comments/18s55am/raycasting_game_in_python_and_pygame_part_3/	I made a raycasting game using Python and Pygame Code: [https://github.com/DataWizual/Raycasting\_Part\_3](https://github.com/DataWizual/Raycasting_Part_3) Here's the video explaining how I did it: [https://youtu.be/znIAg\_6\_Od4](https://youtu.be/znIAg_6_Od4)	0.0	t3_18s55am	reddit		
219	Looking for some packages similar to Laravel's Jetstream and Cashier for subscriptions	Unknown	2023-12-27 13:10:35	https://www.reddit.com/r/Python/comments/18s0d46/looking_for_some_packages_similar_to_laravels/	For a new project at my job we're debating our new project's framework. We're in between Django and Laravel. Since I read Django is much faster and more secure (plus I'd like to do a Python project) we'd like to give that a shot. That said: it is a subscription based project that allows users to call API's, embed chat widgets etc. Laravel has some great plugins/packages for that and that's creating doubt. Is anybody aware of any such similar Python modules that can handle subscription management (subscribe, pause, trials, cancel, refund etc.) using Stripe (and others). Our current projects are in Slim 4 (PHP) and FastAPI (Python) mostly, so we're new to both Laravel and Django. Any suggestions are welcome, thank you. &#x200B;	2.0	t3_18s0d46	reddit		
220	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2023-12-28 00:00:09	https://www.reddit.com/r/Python/comments/18sfbsg/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	0.0	t3_18sfbsg	reddit		
221	I created a web application that solves hidato puzzles from images!	Unknown	2023-12-27 23:32:46	https://www.reddit.com/r/Python/comments/18seo9w/i_created_a_web_application_that_solves_hidato/	Hidato is a logic puzzle containing a hexagonal grid of cells. The goal is to fill the grid with numbers such that consecutive numbers are in cells adjacent to each other. The solver uses image processing and deep learning to extract the grid from the image, then it uses CSP-based techniques to solve the puzzle. web application link: [https://assafvol-hidatosolverwebapp-app-1dptmp.streamlit.app/](https://assafvol-hidatosolverwebapp-app-1dptmp.streamlit.app/) Github repo - [https://github.com/assafvol/HidatoSolverWebApp](https://github.com/assafvol/HidatoSolverWebApp)	1.0	t3_18seo9w	reddit		
222	FinderZ V 2.1.2 Released	Unknown	2023-12-27 19:33:28	https://www.reddit.com/r/Python/comments/18s90lz/finderz_v_212_released/	Hello! FinderZ V2.1.2 has been released. FinderZ is a file management library in python that can do many things (in order to save time by not having to do the functions yourself). It has many features including Synchronization of folders, backing up of multiple folders, a full GatherInfo class as well as a fileOperands class. The latest release 2.1.2 includes new GatherInfo Functions as well as fileOperands functions, such as XOR encryption/decryption of files/directories, and a new data gathering function that gets all the file extensions in a directory and their specific amount in order, and returns them in a dictionary. Check out the project here: [FinderZ](https://github.com/PatzEdi/FinderZ) The documentation will be updated in the next release (2.1.5) as it is outdated as of now. I hope you find FinderZ useful! In the link provided above you will be able to find the installation instructions as well as a changelog which has started from version 2.1.2 onward. If you have any comments or concerns on what I should improve, please let me know.	3.0	t3_18s90lz	reddit		
223	Refactor using SOLID principles amounted to good speedup	Unknown	2023-12-27 03:19:17	https://www.reddit.com/r/Python/comments/18rqeev/refactor_using_solid_principles_amounted_to_good/	I've been reading about SOLID principles and used this knowledge to refactor 2 out of the 4 functions provided in PyPi's unexpected-isaves, and this amounted to a \~1.3x speedup, even though making things run faster was not my first intention. I just wanted to make the project better for the new stargazers arriving. This shows the importance of good and simple code. [Eric-Mendes/unexpected-isaves: A Python library that paints an image on a spreadsheet, builds its pixel art in Minecraft, makes its ascii art, or makes a rubik's cube art out of it. (github.com)](https://github.com/Eric-Mendes/unexpected-isaves)	2.0	t3_18rqeev	reddit		
224	Increase details of videos (from üå± to ü™¥)	Unknown	2023-12-26 12:28:44	https://www.reddit.com/r/Python/comments/18r7487/increase_details_of_videos_from_to/	&#x200B; [Drive through rain](https://i.redd.it/nkkm6x0skm8c1.gif) I have been working on an interesting project for a while. The aim of the project is to implement some of the known ways of augmenting image details and enhance their contrast. I would absolutely appreciate it if you checkout my code repository and share your opinion. This video is enhanced using the following code base üêç: [Two-dimensional histogram equalization and contrast enhancement](https://github.com/Mamdasn/im2dhisteq) Full video link: [https://youtu.be/7LrzX2ZpLAQ](https://youtu.be/7LrzX2ZpLAQ) Other image/video quality/contrast enhancers that I have implemented: * [Histogram-Based Locality-Preserving Contrast Enhancement](https://github.com/Mamdasn/imhblpce) * [Fast Image/Video Contrast Enhancement Based on Weighted Thresholded Histogram Equalization](https://github.com/Mamdasn/imWeightedThresholdedheq) All these modules strive to make details in images/videos more prominent and remove haziness in them as much as possible. &#x200B;	3.0	t3_18r7487	reddit		
225	Wednesday Daily Thread: Beginner questions	Unknown	2023-12-27 00:00:08	https://www.reddit.com/r/Python/comments/18rm8k7/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	2.0	t3_18rm8k7	reddit		
226	Automaticly type a specific word when the chat is spamming it on kick.com. multiple accounts and threats at the same time.	Unknown	2023-12-27 20:24:12	https://www.reddit.com/r/Python/comments/18sa7fa/automaticly_type_a_specific_word_when_the_chat_is/	is the idea that I just typed out a good idea to make in python? I think I could also make it in JS as it is on a browser but I feel like pixel detection could be a good idea too. what is your ideas on it? (you get points when you type a specific word in the chat and you get money for those points)	3.0	t3_18sa7fa	reddit		
227	Data Structures and Algorithms	Unknown	2023-12-26 14:44:28	https://www.reddit.com/r/Python/comments/18r9kw7/data_structures_and_algorithms/	Hello everyone! For some time on this community, there was a complete google drive style playlist regarding Data Structures and Algorithms in Python. I followed it for some months but I did not finish the entire classes. I guess it was the one attached to this thread on this community [https://www.reddit.com/r/Python/comments/lyux3w/the\_complete\_data\_structures\_and\_algorithms/](https://www.reddit.com/r/Python/comments/lyux3w/the_complete_data_structures_and_algorithms/) Is there anyone that has the correct link to the referred playlist? Thanks in advance!	3.0	t3_18r9kw7	reddit		
228	py-cachify - cache-based locks and handy caching decorators	Unknown	2023-12-26 16:43:01	https://www.reddit.com/r/Python/comments/18rc6e6/pycachify_cachebased_locks_and_handy_caching/	Hey everybody! Sharing [**py-cachify**](https://github.com/EzyGang/py-cachify) which I just wrote and published that provides cache-based locks and decorators that come in handy around the locks and caching. Inspiration to start it came from [douglasfarinelli's python-cachelock](https://github.com/douglasfarinelli/python-cachelock) library since it's no longer maintained but the tool has proven to be very useful in my past projects. Check it out!	2.0	t3_18rc6e6	reddit		
229	Financial-Analyzer CLI App: an argument for use	Unknown	2023-12-26 04:19:13	https://www.reddit.com/r/Python/comments/18qzi6d/financialanalyzer_cli_app_an_argument_for_use/	"Hello r/Python, With Mint closing as an option for many to track personal finances and some renewed interest on my end for finance app....... I'm writing this post on this Christmas in regards to a [Github project](https://github.com/andersbandt/Financial-Analyzer/blob/main/README.md) based around financial analysis. The app in function is written in Python and is meant to analyze one's personal finances, with a main window page looking like so &#x200B; https://preview.redd.it/8cw1xyn0ek8c1.png?width=836&format=png&auto=webp&s=4fc99bdebe4b2e6cefe2910dbb3e22303e29648e The application analysis your financial spending based on data from a directory of monthly statements (typically in .csv format) from various credit cards / banks one has. It is quite easy for one to pull this data from any account like Apple Card, Venmo, Wells Fargo, U.S. Bank, etc. So each month 15-20 mins of effort is required from user to get data, but it's not that hard. My analysis on spending is performed against a preset ""tree of categories"" which allows one to get quite granular with spending data. &#x200B; [a printout of my personal category tree](https://preview.redd.it/e7zo4qrydk8c1.png?width=794&format=png&auto=webp&s=bfdca5b52bb4c3600a37f81eef16e1438bc24284) This project was brought up in [this Reddit post](https://www.reddit.com/r/Python/comments/zrv4hx/would_anyone_be_interested_in_collaborating_on_a/) when the main focus was on a GUI based version of the app (originally used Tkinter library as GUI framework). However, it is **my belief** that a CLI based version of this application is best for an open-source direction. Coding for a GUI with a full-fledged API like Flask or Django and a proper front-end like React is **not worth the time**. CLI applications (like the Linux bash shell) are perfectly functional, and honestly better than most clunky GUI applications for their linearity) My personal progress on this project has increased exponentially since switching to a CLI based application. So with Mint closing their doors feel free to try and get this app running on your machine. Likely you will encounter program bugs. But they should be easily fixable. I also setup a Discord about a year ago for discussion. Check it out [here at this Discord link](https://discord.gg/WEPzzJhs)"	6.0	t3_18qzi6d	reddit		
230	Seeking Suggestions for Enhancing my PyPI Package eagelview - Image Dataset Visualization	Unknown	2023-12-26 14:12:24	https://www.reddit.com/r/Python/comments/18r8y6d/seeking_suggestions_for_enhancing_my_pypi_package/	Hey everyone, I've been developing a PyPI package called eagelview aimed at visualizing image datasets by printing images from folder(s) and adding labels from .csv(s) files, facilitating image dataset visualization. I'm eager to expand its functionality and make it more versatile. Any ideas, suggestions, or features you think would be valuable to include in eagelview would be greatly appreciated! Looking forward to hearing your thoughts. Thanks in advance! \[Check out eagleview on GitHub\](https://github.com/hexronuspi/eagleview)	2.0	t3_18r8y6d	reddit		
231	Unofficial reverse-engineered ChatGPT API in Python	Unknown	2023-12-26 13:58:49	https://www.reddit.com/r/Python/comments/18r8oj0/unofficial_reverseengineered_chatgpt_api_in_python/	I wanted to create a discord bot that I could use to chat with ChatGPT through discord (for fun and to learn how discord bots worked). but the issue was that I couldn't afford to buy the ChatGPT Plus plan to get access to the API. So, I tried to find a way around this problem and found [reverse-engineered ChatGPT API by Antonio Cheong](https://github.com/acheong08/ChatGPT). But as you can see, Antonio Cheong stopped maintaining the API, so it doesn't work anymore (at least not for me). So, I decided to try and make my own for fun. I would really appreciate it if you check it out and tell me what you think (and advise me on how I can make it better if possible). thanks!!!! repo link: [https://github.com/Zai-Kun/reverse-engineered-chatgpt](https://github.com/Zai-Kun/reverse-engineered-chatgpt)	2.0	t3_18r8oj0	reddit		
232	cookiecutter-python-cli-app -- a cookiecutter template for creating a new Python command-line application with Click	Unknown	2023-12-26 14:38:05	https://www.reddit.com/r/Python/comments/18r9g70/cookiecutterpythoncliapp_a_cookiecutter_template/	While there exist many great [cookiecutter](https://github.com/cookiecutter/cookiecutter) templates for Python packages, web applications and data science & machine learning, for example, I was surprised to find virtually none for command-line applications. [The one I did find](https://github.com/simonw/click-app) is pretty basic, and does not have the feature set I was looking for. As such, I took it upon myself to create and publish a great cookiecutter template for CLI's, called **[cookiecutter-python-cli-app](https://github.com/sgraaf/cookiecutter-python-cli-app)**. The template supports Python 3.8, 3.9, 3.10, 3.11 and 3.12 and uses the venerate [Click](https://click.palletsprojects.com/) package to create beautiful and powerful command-line interfaces. It has many other features, like: - Linting with autofix (i.e. removing unused imports, formatting and Python syntax upgrades) with [ruff](https://beta.ruff.rs/docs/) - Code formatting with [ruff](https://beta.ruff.rs/docs/) and [Prettier](https://prettier.io/) - Static type-checking with [mypy](http://www.mypy-lang.org/) - Checks and fixes before every commit with [pre-commit](https://pre-commit.com/) - Testing with [pytest](https://docs.pytest.org/en/stable/index.html) - Project automation with [Nox](https://nox.thea.codes/en/stable/) - Continuous Integration with [GitHub Actions](https://github.com/features/actions) and [pre-commit.ci](https://pre-commit.ci/) - Automated version updates for GitHub Actions with [Dependabot](https://docs.github.com/en/code-security/dependabot/working-with-dependabot/keeping-your-actions-up-to-date-with-dependabot) - Documentation with [Sphinx](https://www.sphinx-doc.org/en/master/), [MyST](https://myst-parser.readthedocs.io/en/latest/), and [Read the Docs](https://readthedocs.org/) using the [Furo](https://pradyunsg.me/furo/) theme - Automated release builds and uploads to [PyPI](https://pypi.org/) Thank you for taking the time to read this! Please give it a try and let me know your thoughts. :)	0.0	t3_18r9g70	reddit		
233	A few questions and concerns about the dataclass PEP	Unknown	2023-12-26 12:13:00	https://www.reddit.com/r/Python/comments/18r6v5v/a_few_questions_and_concerns_about_the_dataclass/	"/u/Mugalari graciously linked us to [the PEP for dataclasses](https://peps.python.org/pep-0557/#rationale) in a thread about [a library intended to fix an issue with standard dataclasses](https://www.reddit.com/r/Python/comments/18q8w6v/dataclassy_fixing_dataclass_inheritance_hell/). This led to a few questions/thoughts/concerns for me: [We read](https://peps.python.org/pep-0557/#abstract): > A class decorator is provided which inspects a class definition for variables with type annotations as defined in PEP 526, ‚ÄúSyntax for Variable Annotations‚Äù. In this document, such variables are called fields. Ok, so why not call the decorator `FieldFactory`? Next, [we read](https://peps.python.org/pep-0557/#rationale) > There have been numerous attempts to define classes which exist primarily to store values which are accessible by attribute lookup. I think this is incorrectly worded first of all. I would've said ""There have been numerous attempts to define classes which provide more thorough support for defining and manipulating the attributes of a class"". The reason I would say it that way is based on [the example in the abstract](https://peps.python.org/pep-0557/#abstract) where we have a class and a large number of methods are automatically generated to guarantee complete and correct generation of auxilliary methods often needed for `field` usage. Next, and most importantly what class does **NOT** exist to store values accessible by attribute lookup? (Simula-based) Object-oriented programming is about unifying values and the methods that operate on them. Next, [we read](https://peps.python.org/pep-0557/#rationale) > There have been numerous attempts to define classes which exist primarily to store values which are accessible by attribute lookup. Why did the author leave out [Enthought Traits](https://docs.enthought.com/traits/) and [Traitlets](https://traitlets.readthedocs.io/en/stable/)? Next [we read](https://peps.python.org/pep-0557/#rationale) > Where is it not appropriate to use Data Classes? and here they give 2 scenarios where it is not appropriate (API compatibility with tuples or dicts and Type validation beyond that provided by PEPs 484 and 526 is required, or value validation or conversion is required) but here is where I have a serious problem. Remember in [the Rationale](https://peps.python.org/pep-0557/#rationale) where this PEP said ""there have been numerous attempts to define classes which exist primarily to store values which are accessible by attribute lookup""? OK, so they are implying that this PEP is tailored to help build classes which exist primarily to store values which are accessible by attribute lookup and therefore this PEP is not appropriate for other types of classes... but that is my central point: if there really were such a large number of classes outside of the ones the PEP mentions in the rationale then why were they not also mentioned when answering the question `Where is it not appropriate to use Data Classes?`."	2.0	t3_18r6v5v	reddit		
234	Building a decentralized key-value store on top of IRC (>= Python 3.6)	Unknown	2023-12-25 10:18:31	https://www.reddit.com/r/Python/comments/18qg1rq/building_a_decentralized_keyvalue_store_on_top_of/	Recently, I've been working on a design for building a decentralized, permissioned key-value store across a threshold of IRC servers using channel names to store keys and topics to store values. The system can be used for a variety of purposes but my intended use is for DNS. My code is written to target Python >= 3.6 using async networking. The write up is on my blog [https://roberts.pm/irc\_kvs/](https://roberts.pm/irc_kvs/)	5.0	t3_18qg1rq	reddit		
235	Tuesday Daily Thread: Advanced questions	Unknown	2023-12-26 00:00:08	https://www.reddit.com/r/Python/comments/18qujkd/tuesday_daily_thread_advanced_questions/	# Weekly Wednesday Thread: Advanced Questions üêç Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices. ## How it Works: 1. **Ask Away**: Post your advanced Python questions here. 2. **Expert Insights**: Get answers from experienced developers. 3. **Resource Pool**: Share or discover tutorials, articles, and tips. ## Guidelines: * This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday. * Questions that are not advanced may be removed and redirected to the appropriate thread. ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **How can you implement a custom memory allocator in Python?** 2. **What are the best practices for optimizing Cython code for heavy numerical computations?** 3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?** 4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?** 5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?** 6. **What are some advanced use-cases for Python's decorators?** 7. **How can you achieve real-time data streaming in Python with WebSockets?** 8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?** 9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?** 10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)** Let's deepen our Python knowledge together. Happy coding! üåü	1.0	t3_18qujkd	reddit		
236	PNLS: An offensive tool that captures and displays SSIDs from device's Preferred Network List in the nearby vicinity.	Unknown	2023-12-25 12:52:00	https://www.reddit.com/r/Python/comments/18qi3u0/pnls_an_offensive_tool_that_captures_and_displays/	Hi everyone, I was tinkering with this idea for a while and it's finally presentable. PNLS is an open-source tool that captures and displays SSIDs from device's Preferred Network List. This is achieved by sniffing out Probe Requests in the nearby vicinity which are then parsed for SSID and other information, and finally propagated to the web UI. The tool is implemented on the Raspberry Pi. More details about the project, its architecture and the technologies it uses is available on the GitHub ([https://github.com/AleksaMCode/Preferred-Network-List-Sniffer](https://github.com/AleksaMCode/Preferred-Network-List-Sniffer)). Because the backend part is written using Python I would appreciate feedback, but more importantly I would love some suggestions on how the code and this tool could be improved.	4.0	t3_18qi3u0	reddit		
237	Introducing Pypeanuts: Monetize Your APIs with Ease!	Unknown	2023-12-25 20:57:37	https://www.reddit.com/r/Python/comments/18qqy33/introducing_pypeanuts_monetize_your_apis_with_ease/	Hello Python enthusiasts and developers! I'm excited to share a project I've been working on: Pypeanuts. It's a Python package designed to help you effortlessly monetize your APIs. While it's still a work in progress, I believe it has great potential for developers looking to generate revenue from their APIs. The code is openly available on GitHub, and I encourage you to check it out and contribute: [Pypeanuts on GitHub](https://github.com/yachty66/pypeanuts) Additionally, I've created a landing page that explains the concept in more detail. Visit [Pypeanuts Landing Page](https://www.pypeanuts.cash/) to learn more about how it works and the benefits it offers. I'm eager to hear your feedback, suggestions, and thoughts on this project. Let's discuss how we can make API monetization simpler and more accessible for everyone!	2.0	t3_18qqy33	reddit		
238	8 Levels of Using Structure Pattern Matching in Python	Unknown	2023-12-24 22:40:45	https://www.reddit.com/r/Python/comments/18q5mp1/8_levels_of_using_structure_pattern_matching_in/	There is one feature that Python developers waiting for so long: structural pattern matching. It finally became possible since Python 3.10. [This article](https://medium.com/techtofreedom/8-levels-of-using-structural-pattern-matching-in-python-d76282d5630f?sk=bc75658e9c10fc24789bd4479c358f86) will show you all tricks of it in 8 levels of difficulty.	4.0	t3_18q5mp1	reddit		
239	Best TTS API for Japanese in Python?	Unknown	2023-12-25 09:36:21	https://www.reddit.com/r/Python/comments/18qfj00/best_tts_api_for_japanese_in_python/	What's the best text-to-speech API for Japanese? I was thoroughly impressed by some of the new AI voice generation techniques but it looks like most of the work it's happening in English. The only real good Japanese AI voice I could find is Speechify's but they don't have an API you can use.	3.0	t3_18qfj00	reddit		
240	Dataclassy - fixing dataclass inheritance hell	Unknown	2023-12-25 01:49:01	https://www.reddit.com/r/Python/comments/18q8w6v/dataclassy_fixing_dataclass_inheritance_hell/	While [researching how to get out of dataclass inheritance hell](https://stackoverflow.com/questions/51575931/class-inheritance-in-python-3-7-dataclasses) I came across [dataclassy](https://github.com/biqqles/dataclassy). It's a (mostly) drop in replacement for the standard library \`dataclasses\` with a number of improvements. Whilst I haven't extensively used it so far it seems to have solved my problems and there's a good chance I'll default to using it instead of \`dataclasses\`. I thought I'd share since it's not a super well known library but seem nice üëå	4.0	t3_18q8w6v	reddit		
241	entertaining / informative youtube channels for keeping up with the nerds	Unknown	2023-12-25 04:12:04	https://www.reddit.com/r/Python/comments/18qb250/entertaining_informative_youtube_channels_for/	I'm after someone entertaining like primeagen but with a python focus/bias. I haven't come across anyone with a great personality (that arjan guy is weird/boring, etc). Any good sources you know of?	3.0	t3_18qb250	reddit		
242	Monday Daily Thread: Project ideas!	Unknown	2023-12-25 00:00:10	https://www.reddit.com/r/Python/comments/18q72ja/monday_daily_thread_project_ideas/	"# Weekly Thread: Project Ideas üí° Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you. ## How it Works: 1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced. 2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code. 3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration. ## Guidelines: * Clearly state the difficulty level. * Provide a brief description and, if possible, outline the tech stack. * Feel free to link to tutorials or resources that might help. # Example Submissions: ## Project Idea: Chatbot **Difficulty**: Intermediate **Tech Stack**: Python, NLP, Flask/FastAPI/Litestar **Description**: Create a chatbot that can answer FAQs for a website. **Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM) # Project Idea: Weather Dashboard **Difficulty**: Beginner **Tech Stack**: HTML, CSS, JavaScript, API **Description**: Build a dashboard that displays real-time weather information using a weather API. **Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8) ## Project Idea: File Organizer **Difficulty**: Beginner **Tech Stack**: Python, File I/O **Description**: Create a script that organizes files in a directory into sub-folders based on file type. **Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/) Let's help each other grow. Happy coding! üåü"	2.0	t3_18q72ja	reddit		
243	Driver's Attention Monitoring System:	Unknown	2023-12-24 00:46:12	https://www.reddit.com/r/Python/comments/18pjhk8/drivers_attention_monitoring_system/	Since accidents have been one of the factors that cause the most deaths in world the main objective of this project is to develop a system that monitors the driver and that through artificial intelligence models can predict when this is showing signs of tiredness, and can also quickly notify and ask for something feedback, thus avoiding accidents due to distraction and possible deaths. With this system it is possible to help reduce this problem, and make driving safer. https://github.com/cousintiz/Driver-s-Attention-Monitoring-System	5.0	t3_18pjhk8	reddit		
244	Debugging dockerized Python apps in VSCode	Unknown	2023-12-23 13:19:36	https://www.reddit.com/r/Python/comments/18p5j92/debugging_dockerized_python_apps_in_vscode/	Finally, set aside the time to configure VScode debugger to peek into web apps running inside docker containers. I use the debugger with pretty much everything but containers. Not sure why I didn‚Äôt bother to do it earlier. Huge productivity boost. TIL: https://rednafi.com/python/debug_dockerized_apps_in_vscode/	9.0	t3_18p5j92	reddit		
245	DSAlgo repository on GitHub	Unknown	2023-12-24 14:27:57	https://www.reddit.com/r/Python/comments/18pw8em/dsalgo_repository_on_github/	Hey everyone, I just wanted to share this amazing data structures and algorithms repository that I found on Github: [**https://github.com/SamirPaulb/DSAlgo**](https://github.com/SamirPaulb/DSAlgo). It's seriously one of the best resources I've come across for learning DSA.	1.0	t3_18pw8em	reddit		
246	Sunday Daily Thread: What's everyone working on this week?	Unknown	2023-12-24 00:00:10	https://www.reddit.com/r/Python/comments/18pikkd/sunday_daily_thread_whats_everyone_working_on/	# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to! ## How it Works: 1. **Show & Tell**: Share your current projects, completed works, or future ideas. 2. **Discuss**: Get feedback, find collaborators, or just chat about your project. 3. **Inspire**: Your project might inspire someone else, just as you might get inspired here. ## Guidelines: * Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome. * Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here. ## Example Shares: 1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate! 2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better. 3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier! Let's build and grow together! Share your journey and learn from others. Happy coding! üåü	12.0	t3_18pikkd	reddit		
247	A simple game made with the graphics.py library	Unknown	2023-12-23 16:18:24	https://www.reddit.com/r/Python/comments/18p91rg/a_simple_game_made_with_the_graphicspy_library/	[https://github.com/sirus-the-beaver/tree-collector](https://github.com/sirus-the-beaver/tree-collector) &#x200B; &#x200B; [Starting menu](https://preview.redd.it/hz6c7nexk28c1.png?width=912&format=png&auto=webp&s=0b35eafa0d114d9ae021dfa887718a51eb2f116c) [Mid gameplay](https://preview.redd.it/7j63soexk28c1.png?width=912&format=png&auto=webp&s=a3011271d34ee2adc48d73fc976c431832e2ed4e) [Pause menu](https://preview.redd.it/6fte0qexk28c1.png?width=912&format=png&auto=webp&s=c9efc2944835538b281590d6b952aec55d7c5d02)	2.0	t3_18p91rg	reddit		
248	Why do so many online gambling sites sponsor PyDev?	Unknown	2023-12-22 21:34:29	https://www.reddit.com/r/Python/comments/18opjgt/why_do_so_many_online_gambling_sites_sponsor_pydev/	"&#x200B; https://preview.redd.it/bezgyjvmzw7c1.png?width=1332&format=png&auto=webp&s=03c2291f559fc6ef66561c1588983888a3f2be71 So, I was looking at the [PyDev homepage](https://www.pydev.org/) (""Eclipse but for Python"") and noticed that most of their sponsors are from online gambling sites. I may be looking into things here, but this seems to be VERY shady, as I don't exactly see this kind of company as the ""generous"" type, especially for a not-so-well-known (and used) python IDE"	16.0	t3_18opjgt	reddit		
249	Gymhero - FastAPI project example	Unknown	2023-12-23 14:26:50	https://www.reddit.com/r/Python/comments/18p6rox/gymhero_fastapi_project_example/	"Hello guys, A couple of weeks ago, I got the itch to build something with FastAPI. As I am a Data Engineer I didn't have a ton of experience with API development - In the past, I only developed the ""reading"" part of API to expose some database tables, KPIs, etc. But I've always wanted to give it a try to build a full CRUD app. So I thought that it would be a fun project to learn the basics of building a backend app with full CRUD functionality. I was originally planning on using Flask, but when I saw FastAPI and all its nifty features, like typing hints, Pydantic, and Depends, I knew I had to give it a go. Turns out, it was a great decision. FastAPI is a really powerful framework, and it's a joy to work with, I highly recommend using it. It's a great choice and it has great documentation and the developer experience is awesome. Here is GitHub with the project: [https://github.com/JakubPluta/gymhero](https://github.com/JakubPluta/gymhero) My project is pretty simple and it's still in development - probably there are some mistakes and some ""better ways"" to do something, but still, I am happy that I managed to write it from scratch. I just only regret I didn't start with async, so it will be harder to migrate it, but I have it in plans :) To give a short description of my project there are a couple of words:*Gymhero is a simple application to manage your gym training workouts. You have the flexibility to create your own exercises, you can develop custom training units and these units can be easily integrated into personalized training plans. You can manage your training units by adding or removing exercises as needed. By default application contains database od more than 1000 exercises*. Core technologies * FastAPI - web framework for building APIs with Python 3.8+ based on standard Python type hints. * SQLAlchemy - Object Relational Mapper * Pydantic - Data validation library for Python and FastAPI models * Uvicorn - ASGI web server implementation for Python * Alembic - lightweight database migration tool for usage with the SQLAlchemy Database Toolkit for Python. * Docker - tool to package and run an application in a loosely isolated environment * Docker Compose - tool for defining and running multi-container Docker applications * Postgres - open source object-relational database * For testing: * pytest * pytest-cov * pytest-mock * For development * precommit-hook * pylint * black * ruff * poetry * venv Some implemented functionalities: * JWT Authentication * Password Hashing * Login & Register Endpoints * ORM Objects representing SQL tables and relationships * Pydantic schemas * CRUD module for reading, updating, and deleting objects in/from the database * Pagination * Dependencies - superuser, active user, database * Initialization scripts * Separate database and env for testing You can find more in Readme [https://github.com/JakubPluta/gymhero/blob/main/README.md](https://github.com/JakubPluta/gymhero/blob/main/README.md) To run the project locally in docker container simply clone the repository, navigate to cloned directory, and run the: `make dev` or `make install` command, or if you don't have make installed just use docker commands docker compose build docker compose up -d docker exec -it app alembic upgrade head docker exec -it app python -m scripts.initdb --env=dev For more details just go through the README file.I would love to get some opinions from you, especially from experienced FastAPI users :). Please let me know what should I improve, what I did wrong, and what could be done in a better way. &#x200B;"	1.0	t3_18p6rox	reddit		
250	Azure DevOps connection to Toggl	Unknown	2023-12-24 00:27:27	https://www.reddit.com/r/Python/comments/18pj4h4/azure_devops_connection_to_toggl/	Anyone have success connecting azure DevOps with Toggl? I currently use a notebook to sync my outlook with my toggl but would really like to cut out a step and connect directly to dev ops. I use another notebook to pull some info down from dev ops to generate reports. I know its possible but before I go down that rabit hole, thought I would check here. A quick google turned up nothing useful. We can‚Äôt install from visual studio marketplace except approved plugins so the typical route wont work. Just want to take my laziness to the next level while not officially sanctioned	0.0	t3_18pj4h4	reddit		
251	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2023-12-23 00:00:19	https://www.reddit.com/r/Python/comments/18osm7b/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	2.0	t3_18osm7b	reddit		
252	Snake game in Python and Pygame	Unknown	2023-12-22 15:57:48	https://www.reddit.com/r/Python/comments/18oi4a6/snake_game_in_python_and_pygame/	I made a Snake game using Python and Pygame Code: [https://github.com/DataWizual/Snake-game-in-python-and-Pygame](https://github.com/DataWizual/Snake-game-in-python-and-Pygame) Here's the video explaining how I did it: [https://youtu.be/f5jWChprIGU](https://youtu.be/f5jWChprIGU)	1.0	t3_18oi4a6	reddit		
253	[wip] Automating with a Python-based BPM	Unknown	2023-12-22 16:03:44	https://www.reddit.com/r/Python/comments/18oi9gu/wip_automating_with_a_pythonbased_bpm/	Hi everyone! We're building Abstra [Workflows](https://www.abstra.io/workflows), a tool to help Python devs build process automation in a faster, yet equally powerful way. The idea is to code it all in Python (this means versionable and no lock-in) while using out-of-box features (such as access control and audit logs) that reduce tech overhead and busywork that usually comes with high-code. I'd love to understand if anyone here in the community would benefit from a tool like this. I'm on the lookout on how to make the tool as best as possible for Python devs needing to automate. Here's the [Github link](https://github.com/abstra-app/abstra-lib) also. We'll chat in the comments, thank you :)	1.0	t3_18oi9gu	reddit		
254	VT100 coloring logging utility	Unknown	2023-12-22 14:20:48	https://www.reddit.com/r/Python/comments/18og01h/vt100_coloring_logging_utility/	PyPI: https://pypi.org/project/vt100logging/ GitHub: https://github.com/g2labs-grzegorz-grzeda/vt100logging This is my first Python package. It adds basic VT100 escape codes coloring to the STDOUT.	0.0	t3_18og01h	reddit		
255	What is a low overhead ETL pipeline?	Unknown	2023-12-21 21:53:31	https://www.reddit.com/r/Python/comments/18nyeki/what_is_a_low_overhead_etl_pipeline/	I need to do some pipelines for crawling,cleaning, indexing from a flask app, expecting them to be long running and want to run outside of flask. The project is a POC/prototype for a pitch to determine if it‚Äôs worth moving forward. So looking for low overhead, minimal setup. Celery & Airflow are just too big for something like this, Luigi seems to fit the bill but looks like it‚Äôs in rough shape Spotify seems to have moved away from Luigi, but is two commands to get it up and running. Anybody have suggestions for a quick and simple etl framework?	19.0	t3_18nyeki	reddit		
256	The Decimator, or how to plot a lot of points in Python	Unknown	2023-12-21 15:48:42	https://www.reddit.com/r/Python/comments/18nq0s8/the_decimator_or_how_to_plot_a_lot_of_points_in/	"The decimator is a function that removes the points but keeps the ""information"" of a chart. The post features examples on Times series and also for clustering. [https://www.taipy.io/posts/big-data-charting-strategies-in-python](https://www.taipy.io/posts/big-data-charting-strategies-in-python)"	3.0	t3_18nq0s8	reddit		
257	How to implement DDD Entities in Python	Unknown	2023-12-21 12:59:38	https://www.reddit.com/r/Python/comments/18nmhc2/how_to_implement_ddd_entities_in_python/	üë®‚Äçüíª Recently, I delved into the world of DDD Entities in Python. üöÄ Shared my experience and some basics in my latest blog post. Check it out if you're curious: [https://blog.szymonmiks.pl/p/basic-building-blocks-ddd-entities/](https://blog.szymonmiks.pl/p/basic-building-blocks-ddd-entities/)	4.0	t3_18nmhc2	reddit		
258	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2023-12-22 00:01:09	https://www.reddit.com/r/Python/comments/18o19gc/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	1.0	t3_18o19gc	reddit		
259	Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial	Unknown	2023-12-21 15:28:11	https://www.reddit.com/r/Python/comments/18npkd0/exploring_3d_terrain_visualization_with_python_a/	&#x200B; [Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://preview.redd.it/uwfu1e682o7c1.jpg?width=1024&format=pjpg&auto=webp&s=640029aa56265addfe86f152c9778f34c7df03e0) [Exploring 3D Terrain Visualization with Python: A DEM and PyVista Tutorial](https://spatial-dev.guru/2023/12/17/exploring-3d-terrain-visualization-with-python-a-dem-and-pyvista-tutorial/)	0.0	t3_18npkd0	reddit		
260	existing package for directory sharding?	Unknown	2023-12-21 14:58:25	https://www.reddit.com/r/Python/comments/18nox0v/existing_package_for_directory_sharding/	Is there an existing package that shards filenames to create subdirectory names? e.g. shard('doggy') -> 'a7/doggy' yes I know it's fairly easy to code. *pip install <some package name>* is easier iff *some package name* exists ;) Edit: So this happened: [https://pypi.org/project/directoryshard/](https://pypi.org/project/directoryshard/)	7.0	t3_18nox0v	reddit		
261	Open Source Django Admin Template - Sneat	Unknown	2023-12-21 11:04:22	https://www.reddit.com/r/Python/comments/18nkjae/open_source_django_admin_template_sneat/	Hi All, Sharing here an open-source resource: Sneat Open Source & Free Bootstrap 5 Admin Template. Incredibly versatile, the Sneat ‚Äì [Sneat ‚Äì Free Bootstrap Django Admin Template](https://themeselection.com/item/sneat-free-bootstrap-django-admin-template/) also allows you to build any type of web application. For instance, you can create: * SaaS platforms * Project management apps * Ecommerce backends * CRM systems * Analytics apps * Banking apps * Education apps * Fitness apps & many more. **Features:** * Based on **Django 5** * **Bootstrap 5** * **Vertical** layout * Unique Dashboard * 1 Chart library * Authentication Pages * Fully Responsive Layout * Organized Folder Structure * Clean & Commented Code * Well Documented I hope you all find this resource useful.	1.0	t3_18nkjae	reddit		
262	Looking for contributers for writing a plugin for calibre	Unknown	2023-12-21 22:12:45	https://www.reddit.com/r/Python/comments/18nyu9m/looking_for_contributers_for_writing_a_plugin_for/	Hi all, so I currently found out that there is barely any way to fetch metadata when it comes to books written in Bulgarian through calibre. I have therefore written a web scraper that will fetch metadata of a book from a couple of websites, and it seems to work as intended. The thing is ‚Äî I'm a complete noob when it comes to plugins and was hoping to find someone who might be interested in helping out on the project - to turn these web scrapers into a calibre plugin. The functionality is already there, I just need someone to create the interface in calibre and to map the data to the correct fields. [https://github.com/kbkozlev/BGBookMeta](https://github.com/kbkozlev/BGBookMeta)	2.0	t3_18nyu9m	reddit		
263	The hand-picked selection of the best Python libraries and tools of 2023	Unknown	2023-12-20 20:33:05	https://www.reddit.com/r/Python/comments/18n4guw/the_handpicked_selection_of_the_best_python/	Hello Python Community! We're thrilled to present our 9th edition of the **Top Python Libraries and tools**, where we've scoured the Python ecosystem for the most innovative and impactful developments of the year. This year, it‚Äôs been the boom of Generative AI and Large Language Models (LLMs) which have influenced our picks. Our team has meticulously reviewed and categorized over 100 libraries, ensuring we highlight both the mainstream and the hidden gems. **Explore the entire list with in-depth descriptions here**: [https://tryolabs.com/blog/top-python-libraries-2023](https://tryolabs.com/blog/top-python-libraries-2023) Here‚Äôs a glimpse of our top 10 picks: 1. [**LiteLLM**](https://github.com/BerriAI/litellm) ‚Äî Call any LLM using OpenAI format, and more. 2. [**PyApp**](https://github.com/ofek/pyapp) ‚Äî Deploy self-contained Python applications anywhere. 3. [**Taipy**](https://github.com/Avaiga/taipy) ‚Äî Build UIs for data apps, even in production. 4. [**MLX**](https://github.com/ml-explore/mlx) ‚Äî Machine learning on Apple silicon with NumPy-like API. 5. [**Unstructured**](https://github.com/Unstructured-IO/unstructured) ‚Äî The ultimate toolkit for text preprocessing. 6. [**ZenML**](https://github.com/zenml-io/zenml) and [**AutoMLOps**](https://github.com/GoogleCloudPlatform/automlops) ‚Äî Portable, production-ready MLOps pipelines. 7. [**WhisperX**](https://github.com/m-bain/whisperX) ‚Äî Speech recognition with word-level timestamps & diarization. 8. [**AutoGen**](https://github.com/microsoft/autogen) ‚Äî LLM conversational collaborative suite. 9. [**Guardrails**](https://github.com/guardrails-ai/guardrails) ‚Äî Babysit LLMs so they behave as intended. 10. [**Temporian**](https://github.com/google/temporian) ‚Äî The ‚ÄúPandas‚Äù built for preprocessing temporal data. Our selection criteria prioritize innovation, robust maintenance, and the potential to spark interest across a variety of programming fields. Alongside our top picks, we've put significant effort into the long tail, showcasing a wide range of tools and libraries that are valuable to the Python community. A huge thank you to the individuals and teams behind these libraries. Your contributions are the driving force behind the Python community's growth and innovation. üöÄüöÄüöÄ **What do you think of our 2023 lineup? Did we miss any library that deserves recognition?** Your feedback is vital to help us refine our selection each year.	14.0	t3_18n4guw	reddit		
264	Snappy (Candid Photos)	Unknown	2023-12-21 06:32:21	https://www.reddit.com/r/Python/comments/18nghyl/snappy_candid_photos/	Do you ever spend hours coding and wish you could capture your agony? Now you can! Snappy is a little program for the macOS menu bar that uses cv2 to take pictures on a customizable timer with the option for random filters. This is my first time combining rumps and pyqt6. Let me know what you think! [https://www.mediafire.com/file/dii30gc8oxwbzul/Snappy.zip/file](https://www.mediafire.com/file/dii30gc8oxwbzul/Snappy.zip/file) [https://github.com/CoderEgloo/Snappy](https://github.com/CoderEgloo/Snappy) &#x200B; [options](https://preview.redd.it/x1qpvxiddl7c1.png?width=215&format=png&auto=webp&s=0cf3c309f7ceb1d9ec0c8b0bcf84ebeb65086042) [settings page](https://preview.redd.it/t383fi1kdl7c1.png?width=810&format=png&auto=webp&s=53892bed52e7b1b13b8258dffcd96c12e99ac75e) &#x200B; &#x200B; &#x200B;	0.0	t3_18nghyl	reddit		
265	Have you wasted hours tweaking a plot for a presentation or academic paper, like searching StackOverflow on how to change the font size of the labels?	Unknown	2023-12-21 17:45:43	https://www.reddit.com/r/Python/comments/18nsorf/have_you_wasted_hours_tweaking_a_plot_for_a/	[LLM automatically updating plot](https://i.redd.it/cr2wcxpoqo7c1.gif) In this blog post, we will build an AI chatbot with Panel and Mixtral 8x7b that will help you generate code and execute code to tweak an Matplotlib plot. It has two functionalities: 1. You can chat with the AI assistant to do small tweaks of a Matplotlib plot or ask it to ‚Äúmake this figure ready for a poster presentation‚Äù. This is especially helpful when we need help with styling but don‚Äôt know where to start. This AI chatbot will not only generate ideas, but also runnable code to improve your plot directly. 2. You can also check the code of a figure, edit the code directly, and get the updated version of the plot. This is helpful when you would like to start with your own plot. You can copy and paste the code of your own plot here as a starting point for AI to improve. * Try out the app [here](https://huggingface.co/spaces/ahuang11/tweak-mpl-chat) (we will keep this app live for a week) * Check the code [here](https://huggingface.co/spaces/ahuang11/tweak-mpl-chat/blob/main/app.py)	1.0	t3_18nsorf	reddit		
266	Any mypyc users have any organization tips?	Unknown	2023-12-21 13:17:27	https://www.reddit.com/r/Python/comments/18nmti4/any_mypyc_users_have_any_organization_tips/	I've been using to download and process a bunch of data. Noticed that some of the unit tests I made ran up to 3x faster when compiled using \`mypyc\`. Some of the scripts depend on packages that don't take to well to being c-compiled, so I'm still having to interface my c-compiled scripts with vanilla python. Best organization I've come up with so far has been (showing only the stuff under version control): \`\`\`bash scripts/ |--> [file1.py](https://file1.py) |--> [file2.py](https://file2.py) |-> compiled\_scripts/ |--> mypy.ini |-> stubs/ |--> file1.pyi |-> module1/ \`\`\` And the procedure has been to navigate to \`compiled\_scripts\`, run \`mypyc ../file1.py\` and then in \`file2.py\` put a \`try/except\` to look for the package in the subdirectory: \`\`\`python try: import compiled\_scripts.file1 except: import file1 \`\`\` This seems to keep things fairly clean, but I'm wondering how other people organize things.	1.0	t3_18nmti4	reddit		
267	Ive been a python hobbyist for a couple years - am I ready to start applying? -Looking for feedback on my most recent project - A library wrapping AIOSQLite to abstract away writing SQL (for smaller projects)	Unknown	2023-12-20 17:35:40	https://www.reddit.com/r/Python/comments/18n07wj/ive_been_a_python_hobbyist_for_a_couple_years_am/	"As the title implies, I'm looking for feedback on my code and potential hire-ability. [https://github.com/sockheadrps/AIODesa](https://github.com/sockheadrps/AIODesa) This project wraps AIOSQLite using built-ins and data classes to provide an abstraction layer for dealing with SQLite databases. I definitely prefer back end web development, and know just enough HTML CSS and JS to be dangerous, but Im wondering if this project is ""professional"" enough to put on my CV, and if it accurately conveys my level of understanding. Background on myself: Im 30 years old, been programming as a hobby for like 5 years or so, always been interested in computer science, but went the route of trade school and have been a commercial/industrial electrician for 10 years. I have no formal education or training other than a HS diploma and my trade certificate. Do you think it would be unlikely for me to be hired at this stage in my life and at this stage in my code quality?"	8.0	t3_18n07wj	reddit		
268	Project: Render Cellular Automaton simulations in the terminal	Unknown	2023-12-20 22:38:46	https://www.reddit.com/r/Python/comments/18n7cbb/project_render_cellular_automaton_simulations_in/	Hey all, I wanted to share this project I've been working on for the last few days. I've written a cellular automaton program that renders cells directly to the terminal using \`rich\`, and I think the results look pretty cool. Do note the example in the README is not how the cells *actually* render -- this is some weird graphical artifacts with certain character's background/foreground colors overlapping each other. Feedback is appreciated! [https://github.com/noprobelm/terminal-cellular-automaton](https://github.com/noprobelm/terminal-cellular-automaton)	1.0	t3_18n7cbb	reddit		
269	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2023-12-21 00:00:08	https://www.reddit.com/r/Python/comments/18n9501/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	0.0	t3_18n9501	reddit		
270	HoloViews Cheat Sheet for Bokeh Backend	Unknown	2023-12-20 17:25:14	https://www.reddit.com/r/Python/comments/18mzyof/holoviews_cheat_sheet_for_bokeh_backend/	"HoloViews simplifies high-level data visualization in Python, excelling in interactive and declarative visualizations, especially when paired with pandas and xarray. [hvplot](https://hvplot.holoviz.org/), which is built on top of HoloViews, makes it even easier with pandas-like plotting syntax, e.g. \`ds.hvplot(""x"", ""y"")\`. https://preview.redd.it/8iilcl2ohh7c1.png?width=1911&format=png&auto=webp&s=a229f8ed9ee8219a670e462c4207853641c41a23"	0.0	t3_18mzyof	reddit		
271	Alarm-Clock-Tkinter-GUI-project-IOS-Design	Unknown	2023-12-20 17:57:41	https://www.reddit.com/r/Python/comments/18n0r5q/alarmclocktkinterguiprojectiosdesign/	Alarm Clock with GUI is a Python project that utilizes tkinter and other libraries to create an intuitive alarm clock with a graphical user interface resembling IOS Alarm Application [https://github.com/YatoVoid/Alarm-Clock-Tkinter-GUI-project-IOS-Design.git](https://github.com/YatoVoid/Alarm-Clock-Tkinter-GUI-project-IOS-Design.git) &#x200B; https://preview.redd.it/i0pajyzqfi4c1.png?width=508&format=png&auto=webp&s=da4a8337a8b176339dac3a3fc96ea30f73f939f5 https://preview.redd.it/7q2e3qisfi4c1.png?width=504&format=png&auto=webp&s=060bf29092bd7407e30589ce0001676c133049d9 https://preview.redd.it/pnu2r6yufi4c1.png?width=503&format=png&auto=webp&s=7bf91532d85c29299ba5380d8ab227b78d115d99	0.0	t3_18n0r5q	reddit		
272	Declarative GUI for Python	Unknown	2023-12-19 21:56:26	https://www.reddit.com/r/Python/comments/18mdpig/declarative_gui_for_python/	Today, we at Slint ([https://slint.dev](https://slint.dev)) kicked off support for Python with an initial PR - [https://github.com/slint-ui/slint/pull/4155](https://github.com/slint-ui/slint/pull/4155). We invite your suggestions, feedback, and contributions to achieve the initial milestone - [https://github.com/slint-ui/slint/milestone/18](https://github.com/slint-ui/slint/milestone/18). Slint is an open-source graphical user interface toolkit to design, develop, and deploy native user interfaces on desktop and embedded systems. One of our goals is to support multiple programming languages. This project to provide native Python APIs has been made possible by the NLNet Foundation - [https://nlnet.nl/project/PythonicSlint/](https://nlnet.nl/project/PythonicSlint/).	8.0	t3_18mdpig	reddit		
273	Python-oracledb 2.0 for Oracle Database introduces asyncio support	Unknown	2023-12-20 03:02:55	https://www.reddit.com/r/Python/comments/18mk7ud/pythonoracledb_20_for_oracle_database_introduces/	Python-oracledb 2.0 for Oracle Database introduces asyncio support Python-oracledb is the Python driver for Oracle Database. The main changes in 2.0 are: * Support for asynchronous concurrent coding * Support for ‚ÄòSuccess With Info‚Äô warnings * Support for configuring the SDU in Thin mode * The future \`oracledb.\_\_future\_\_.old\_json\_col\_as\_ob\`j has been removed * New Connection object attributes * New SQL Domain and Annotation attributes * Some obsolete, long deprecated parameters like \`encoding\` and \`nencoding\` have been desupported * Support for the obsolete Python 3.6 release has been dropped (you can use Python 3.7, 3.8, 3.9, 3.10, 3.11 and 3.12 !) &#x200B; Check the [release announcement](https://cjones-oracle.medium.com/python-oracledb-2-0-has-asyncio-support-2b913e40f9ca) &#x200B;	2.0	t3_18mk7ud	reddit		
274	TSAlign: A simple and fast python library to align two 1D time-series data using FFT based convolution.	Unknown	2023-12-20 06:11:23	https://www.reddit.com/r/Python/comments/18mnpoy/tsalign_a_simple_and_fast_python_library_to_align/	[TSAlign](https://github.com/nexus1203/TSAlign/tree/main) is a Python library for fast and straightforward alignment of 1D time series data, using FFT-based convolution. Ideal for signal processing and time series analysis, it offers functions for calculating distances like Euclidean, mean-adjusted, and z-normalized. [Github Link](https://github.com/nexus1203/TSAlign) Choose a distance metric ('sdist', 'mdist', 'zdist') and align your series. The package includes an example aligning sine and cosine waves, with visualization capabilities for comparing original, aligned, and difference series. **Here's a quick example:** import numpy as np import tsalign as tsa # Example time series data t = np.linspace(0, 3, 1000) Q = np.sin(2 * np.pi * 5 * t)[:500] S = np.cos(2 * np.pi * 5 * t) + 0.1 * np.random.randn(len(t)) # Align using z-normalized distance aligned_series, difference = tsa.align_timeseries(Q, S, distance='zdist') **Visualization:** https://preview.redd.it/nmabau7z5e7c1.png?width=1000&format=png&auto=webp&s=967d4fb8935d07807fb8a7dc01715d21d0dc5e20 **Benchmark** |Array Size|Mean Time (s)|Standard Deviation (s)| |:-|:-|:-| |1.0e+01|0.000265|0.000068| |1.0e+02|0.000272|0.000072| |1.0e+03|0.000330|0.000121| |1.0e+04|0.000841|0.000303| |1.0e+05|0.010003|0.001382| |1.0e+06|0.108963|0.014162| |1.0e+07|1.150634|0.203717| &#x200B;	1.0	t3_18mnpoy	reddit		
275	I recorded a crash course on Polars library of Python (Great library for working with big data) and uploaded it on Youtube	 	2023-12-19 03:44:14	https://www.reddit.com/r/Python/comments/18lsb7i/i_recorded_a_crash_course_on_polars_library_of/	Hello everyone, I created a crash course of Polars library of Python and talked about data types in Polars, reading and writing operations, file handling, and powerful data manipulation techniques. I am leaving the link, have a great day!! [https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra\_5PGH&index=6&t=689s](https://www.youtube.com/watch?v=aiHSMYvoqYE&list=PLTsu3dft3CWiow7L7WrCd27ohlra_5PGH&index=6&t=689s)	10.0	t3_18lsb7i	reddit		
276	MiniLang - C succesor	Unknown	2023-12-20 00:12:28	https://www.reddit.com/r/Python/comments/18mgqrb/minilang_c_succesor/	Link: [https://github.com/NICUP14/MiniLang](https://github.com/NICUP14/MiniLang) # Mini Lang A type-safe C successor that compiles directly to x86\_64 assembly. ## Features * Minimal * Compiled * Typed * Functional\* * Inter-op with C functions Minimal - As close as possible to actual assembly code while maintaining as many high-level features as possible.	2.0	t3_18mgqrb	reddit		
277	Wednesday Daily Thread: Beginner questions	Unknown	2023-12-20 00:00:09	https://www.reddit.com/r/Python/comments/18mgh2m/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	2.0	t3_18mgh2m	reddit		
278	Convenient SQL databases terminal client	Unknown	2023-12-19 09:13:20	https://www.reddit.com/r/Python/comments/18lxq5l/convenient_sql_databases_terminal_client/	"I spend most of my time in the terminal while developing, and I'm used to using terminal-based database clients. For example, all application logs are stored in ClickHouse, but there is no convenient client with a user-friendly data representation and SQL query storage, like DBeaver or DataGrip, but for the terminal. Since I'm a programmer, I took on 2 projects - kaa editor and visidata, both written in Python, and created ""pineapple apple pen"" - a terminal-based simplified (and in some cases superior, thanks to the capabilities of visidata) alternative to DBeaver. GitHub: [https://github.com/Sets88/dbcls](https://github.com/Sets88/dbcls) Please star üåü repo if you liked what i created"	8.0	t3_18lxq5l	reddit		
279	Questions regarding OpenAPI Generator CLI and Swagger Codegen with Python and Flask	Unknown	2023-12-19 16:17:39	https://www.reddit.com/r/Python/comments/18m5oia/questions_regarding_openapi_generator_cli_and/	So, I have been trying to look into OpenAPI Generator CLI but I got the feeling from the documentation and after trying to set up just a very basic example server that it: 1. does not work put of the box 2. the documentation is nearly non-existent or not specific to any generator (neither python-flask nor any other generator) 3. the code generated also indicates that the mentioned python-flask generator is not up to date when (python version 3.6+, does not run with an environment set up with py-3.6 either) Did anyone try to use this yet or has an alternative to generating a working server stub? Or is there someone who would like to share his customizaton of the openapi generator?	2.0	t3_18m5oia	reddit		
280	Questions about a TensorFlow Plate recognition project i plan on doing.	Unknown	2023-12-20 09:04:15	https://www.reddit.com/r/Python/comments/18mqb4t/questions_about_a_tensorflow_plate_recognition/	"Hello, so i have a python project i plan on doing for school and thought about training a TensorFlow model for License Plates recognition (also recognize the character), then retrieving the characters, and automatically put them in a french ""plate checker"" website via a little script, which'll give me the model/brand of the car, and use the brand/model to give info on the car via an API using the said brand/model. So now that i've explained that, does anyone know if following a 2 year old tutorial is fine for this ? I really want to dive in but am afraid i'll run into errors due to a deprecated way of installing/configuring TensorFlow. [This is the tutorial i thought about following](https://www.youtube.com/watch?v=0-4p_QgrdbE&list=LL&index=10&t=2105s&ab_channel=NicholasRenotte), seems pretty complete and he goes over a lot of details. Also want to know if i'll be able to put all of this in a app.exe with a little window that pops up that asks us to choose an image and then analyze it to do everything i want it to do. Thanks for your time and the help."	1.0	t3_18mqb4t	reddit		
281	python-decouple vs confiGOAT || Why you need to use confiGOAT to manage configurations and environment variables of your python projects	Unknown	2023-12-20 09:01:36	https://www.reddit.com/r/Python/comments/18mq9nm/pythondecouple_vs_configoat_why_you_need_to_use/	Some weeks ago, one of my colleagues and I were given a task to ‚Äúfix‚Äù the management of the configuration and environment variables used in some of our mission-critical projects. While reviewing the existing implementation of the configuration management module using the python-decouple package, we identified multiple issues, 1. With python-decouple, we were restricted to using only .ini or .env files to define the configurations or settings variables. 2. Everything in python-decouple is a string. What if we wanted to use a tuple or dict without casting from <class str>? 3. We could not load configurations that required complex calculations. 4. What if we wanted all different configurations loaded from multiple sources to be available from a single interface? 5. What if we wanted to create and then access variables following a nested hierarchy? 6. What if we wanted to reference variables from anywhere in a bidirectional way? 7. We had to manually create separate configuration files and copy the entire folder architecture of Python scripts for different environments. 8. What if we wanted to access the environment variables using dynamic modules and attributes? As we started listing down the scope and functionalities of this new module, we realized that others might also benefit from having a highly extensive module where they could, 1. Define configuration parameters using both YAML and Python scripts. 2. Manage all environment variables or configuration parameters from a single setup. Define configurations once, and use them everywhere. 3. Cast values before using them. 4. Use a powerful reference mechanism to reuse variables from any level at any level in any direction. 5. Use multiple resource types in the YAML to support the vast majority of use cases. 6. Support both simple use cases and complex, multi-layered nested configurations. 7. Use dynamic modules to access the parameters through the import interface in Python. 8. Use a single exposed API to interact with the layered configurations. We are delighted to release the first version of confiGOAT, a powerful, flexible, and developer-friendly management tool for all your environment variables and configurations. We have created a detailed guide on how to set up and use this package in your projects. (https://github.com/aag13/configoat) Please take a look and let us know if you would like any enhancements or report any issues. Oh, and don‚Äôt forget to ‚≠ê the GitHub repository (https://github.com/aag13/configoat) and spread the news! ü•≥ https://pypi.org/project/configoat/ ‚ú®‚ú®‚ú®	3.0	t3_18mq9nm	reddit		
282	How could i improve This Monitoring system?	Unknown	2023-12-19 14:33:29	https://www.reddit.com/r/Python/comments/18m39nq/how_could_i_improve_this_monitoring_system/	Driver's Attention Monitoring System: Since accidents have been one of the factors that cause the most deaths in world the main objective of this project is to develop a system that monitors the driver and that through artificial intelligence models can predict when this is showing signs of tiredness, and can also quickly notify and ask for something feedback, thus avoiding accidents due to distraction and possible deaths. With this system it is possible to help reduce this problem, and make driving safer. https://github.com/cousintiz/Driver-s-Attention-Monitoring-System	0.0	t3_18m39nq	reddit		
283	Looking to buy the script of the unit cell of a TPMS structure	Unknown	2023-12-19 21:30:04	https://www.reddit.com/r/Python/comments/18md345/looking_to_buy_the_script_of_the_unit_cell_of_a/	Hi there, Anyone ever used the PyScaffolder module for creating gyroid or Schwartz structures for 3D shoe sole lattice purposes? If you have a portfolio of unit cell of such structure, then I‚Äôd like to buy one. Unless you can build it on demand for me. Thanks.	0.0	t3_18md345	reddit		
284	[blog] Convert data streams to Parquet Files in Python	Unknown	2023-12-19 20:42:58	https://www.reddit.com/r/Python/comments/18mbz1c/blog_convert_data_streams_to_parquet_files_in/	Using Bytewax (Stream processing purely in Python) take simulated streaming web event data and put it in partitioned Parquet files in 5 second intervals: \* Define a custom data source connector to our fake web events simulator \* Deserialize the data and reformat it \* Batch the records into a list and convert to an Apache Arrow Table \* Write the events to partitioned Parquet Files [https://bytewax.io/blog/data-pipelines-streams-to-parquet](https://bytewax.io/blog/data-pipelines-streams-to-parquet)	1.0	t3_18mbz1c	reddit		
285	Create Beautiful Rocks with Dynamic Lighting using PyRock2D | Made with Python and Pygame-CE	Unknown	2023-12-19 01:12:32	https://www.reddit.com/r/Python/comments/18lp80z/create_beautiful_rocks_with_dynamic_lighting/	Introducing PyRock2D - A 2D rock generator that uses real-time dynamic lighting to give a pseudo-3D effect. Export to PNG or JSON formats for use in your projects or your personal collection : ) I also made a trailer video for this app [here](https://youtu.be/Qn6LuhRiYT0) Source code is found [here](https://github.com/tank-king/PyRock2D) https://preview.redd.it/k193r7ujj57c1.png?width=1875&format=png&auto=webp&s=f27838e4b298f001b9a4c79309bf61ebc7610a6d &#x200B; https://preview.redd.it/lecfnncmj57c1.png?width=828&format=png&auto=webp&s=0f7f38528590df2e6096c9156864a9b92f3a74eb https://preview.redd.it/0ryujyypj57c1.png?width=828&format=png&auto=webp&s=f17139fe9b4268e2e22e581198a2bfaa2c2e2bae	5.0	t3_18lp80z	reddit		
286	Panel ChatInterface lets you create chat interfaces with just Python	Unknown	2023-12-19 00:04:42	https://www.reddit.com/r/Python/comments/18lnrx5/panel_chatinterface_lets_you_create_chat/	"No Javascript knowledge required. Here's a minimal example that you can use to get started! import panel as pn def callback(contents: str, user: str, instance: pn.chat.ChatInterface): message = f""Echoing {user}: {contents}"" return message chat_interface = pn.chat.ChatInterface(callback=callback) chat_interface.servable() See [https://holoviz-topics.github.io/panel-chat-examples/](https://holoviz-topics.github.io/panel-chat-examples/) for recipes, including interfacing with OpenAI, Mistral, Llama, Langchain, and LlamaIndex! https://i.redd.it/u3gfst0j757c1.gif"	0.0	t3_18lnrx5	reddit		
287	Matplotlib's subplot_mosaic()	Unknown	2023-12-19 00:12:44	https://www.reddit.com/r/Python/comments/18lny57/matplotlibs_subplot_mosaic/	I'm thrilled to share that I've been working on some fascinating content around Matplotlib's powerful features, like subplot\_mosaic(). This function is a game-changer when it comes to creating intricate subplot layouts and organizing multiple subgraphs seamlessly within a single diagram. Link: [https://colab.research.google.com/drive/1XK9K9dP\_CC3WfzimIabBZV8-8mx1aWH8?usp=sharing](https://colab.research.google.com/drive/1XK9K9dP_CC3WfzimIabBZV8-8mx1aWH8?usp=sharing)	1.0	t3_18lny57	reddit		
288	Tuesday Daily Thread: Advanced questions	Unknown	2023-12-19 00:00:07	https://www.reddit.com/r/Python/comments/18lnnzo/tuesday_daily_thread_advanced_questions/	# Weekly Wednesday Thread: Advanced Questions üêç Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices. ## How it Works: 1. **Ask Away**: Post your advanced Python questions here. 2. **Expert Insights**: Get answers from experienced developers. 3. **Resource Pool**: Share or discover tutorials, articles, and tips. ## Guidelines: * This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday. * Questions that are not advanced may be removed and redirected to the appropriate thread. ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **How can you implement a custom memory allocator in Python?** 2. **What are the best practices for optimizing Cython code for heavy numerical computations?** 3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?** 4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?** 5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?** 6. **What are some advanced use-cases for Python's decorators?** 7. **How can you achieve real-time data streaming in Python with WebSockets?** 8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?** 9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?** 10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)** Let's deepen our Python knowledge together. Happy coding! üåü	2.0	t3_18lnnzo	reddit		
289	Tool which helps you to keep passwords inside your shell scripts(or python code) safely	Unknown	2023-12-18 07:56:30	https://www.reddit.com/r/Python/comments/18l3g42/tool_which_helps_you_to_keep_passwords_inside/	I hate entering passwords, and I'm not very fond of the concept of passwords itself, but such is life; one has to adapt to circumstances. That's why I wrote this utility that allows storing passwords in an encrypted form inside scripts, using an SSH key located in your SSH agent for encryption/decryption. ‚ÄºÔ∏èNever keep your secrets in public places(like git repos accessible to multiple people) GitHub: https://github.com/Sets88/ssh-crypt Please star üåü repo if you liked what i created	13.0	t3_18l3g42	reddit		
290	XetCache Library: Improving the Jupyter Notebook Rerun Experience	Unknown	2023-12-18 22:34:52	https://www.reddit.com/r/Python/comments/18llosd/xetcache_library_improving_the_jupyter_notebook/	For Python-speaking data users, Jupyter notebooks are the go-to tool for data exploration. We created a plugin to easily cache the results of functions in Jupyter notebook cells. The intermediate results are stored in a pickle file in the same folder. This helps solve a few common pains we've experienced: \- **accidentally overwriting variables**: You can re-run a given cell and re-populate any variable (e.g. if you reassigned \`df\` to some other value)\_ \- **sharing notebooks for others to rerun / reproduce**: Many collaborators don't have access to all the same clients / tokens, or all the datasets. Using xetcache, notebook authors can cache any cells / functions that they know are painful for others to reproduce / recreate. \- **speed up rerunning**: even in single player mode, being able to rerun through your entire notebooks in seconds instead of minutes or hours is really really fun The syntax is super minimal and looks like: **%%xetmemo input=df,clean\_data output=model** Here's a link to the project on GitHub: [https://github.com/xetdata/xetcache](https://github.com/xetdata/xetcache) And you can install it via pip: **pip install xetcache** Let us know what you think and what feedback you have! Happy data scienc-ing &#x200B;	0.0	t3_18llosd	reddit		
291	Creating a Voice Virtual Assistant in Python (OpenAI, ElevenLabs, Deepgram)	Unknown	2023-12-18 13:35:34	https://www.reddit.com/r/Python/comments/18l8u1m/creating_a_voice_virtual_assistant_in_python/	Hey guys! I spent the weekend creating a Voice Virtual Assistant (a bit like Jarvis in Iron Man) in Python using OpenAI's GPT, ElevenLabs' TTS, Deepgram's transcription and Taipy's front-end. I figured I would share it here: GitHub repository: [https://github.com/AlexandreSajus/JARVIS](https://github.com/AlexandreSajus/JARVIS) Video demo: [https://youtu.be/aIg4-eL9ATc?si=R6aqJfe7T1fQMqMA](https://youtu.be/aIg4-eL9ATc?si=R6aqJfe7T1fQMqMA)	0.0	t3_18l8u1m	reddit		
292	Interactive TUI app for Python regex exercises	üìö learnbyexample	2023-12-18 10:23:24	https://www.reddit.com/r/Python/comments/18l5irq/interactive_tui_app_for_python_regex_exercises/	Hello! I wrote a TUI app (using `textual`) that has beginner to advanced level exercises for Python regular expressions. There are more than 100 exercises covering both the builtin `re` and third-party `regex` module. Installation: `pip install regexexercises` See https://github.com/learnbyexample/TUI-apps/tree/main/PyRegexExercises for source code, installation details, app guide, screenshot, etc. I'd highly appreciate your feedback.	0.0	t3_18l5irq	reddit		
293	Monday Daily Thread: Project ideas!	Unknown	2023-12-18 00:00:09	https://www.reddit.com/r/Python/comments/18kv3oh/monday_daily_thread_project_ideas/	"# Weekly Thread: Project Ideas üí° Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you. ## How it Works: 1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced. 2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code. 3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration. ## Guidelines: * Clearly state the difficulty level. * Provide a brief description and, if possible, outline the tech stack. * Feel free to link to tutorials or resources that might help. # Example Submissions: ## Project Idea: Chatbot **Difficulty**: Intermediate **Tech Stack**: Python, NLP, Flask/FastAPI/Litestar **Description**: Create a chatbot that can answer FAQs for a website. **Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM) # Project Idea: Weather Dashboard **Difficulty**: Beginner **Tech Stack**: HTML, CSS, JavaScript, API **Description**: Build a dashboard that displays real-time weather information using a weather API. **Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8) ## Project Idea: File Organizer **Difficulty**: Beginner **Tech Stack**: Python, File I/O **Description**: Create a script that organizes files in a directory into sub-folders based on file type. **Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/) Let's help each other grow. Happy coding! üåü"	1.0	t3_18kv3oh	reddit		
294	FluidFrames.RIFE 2.11 - video AI fluidifier app	Unknown	2023-12-17 08:47:48	https://www.reddit.com/r/Python/comments/18kd2zx/fluidframesrife_211_video_ai_fluidifier_app/	&#x200B; https://preview.redd.it/25x6q0aigt6c1.png?width=1459&format=png&auto=webp&s=c290e2960c6376020ce884d1cfb487dee1015b7e * Github. [https://github.com/Djdefrag/FluidFrames.RIFE](https://github.com/Djdefrag/FluidFrames.RIFE) **FluidFrames.RIFE** is a Windows app powered by **RIFE AI** to fluidify videos and to create slowmotion videos. **Changelog version 2.4 -> 2.11** **NEW** * Updated **RIFE AI** model to version **4.13** * better **interpolation quality** * Added a new **RIFE\_Lite AI model** * **high interpolation quality** * **25 % faster** than full RIFE model * **25 % less VRAM** usage * suitable for **low-end GPUs** and high definition videos * Added new option to **save generated video frames** (default is enabled) &#x200B; **GUI / UI** * Completely **re-designed GUI**, now cleaner and more elegant * Added new **info-message** for **info buttons** and in case of **error** * The app will now report the **remaining time to complete video fluidify** * The app will now report the **fluidify progress in %** * Added widget to **select AI model** to use * Updated **info texts**, giving more information * Input Resolution % default value changed to **60%** &#x200B; **BUGFIXES / IMPROVEMENTS** * When the application is closed **during fluidify process**, now the process is **stopped correctly** * Drastically reduced **CPU utilization by 90%** without impacting fluidify speed * Completely revised how the app is built, using **Nuitka** (reducing **antivirus false positive**) * For **AMD gpu** users, it is recommended to update to **driver > 23.11.1** * **performance improvements for DirectML**\-based applications * Improvements for **video processing** * for .mp4 output is possibile to select the **codec (x264 or x265)** * improved video bitrate from 4M (default value for ffmpeg) **to 16M** * in future will be the possibility to select the **desired bitrate** * Better support for file path names **with special characters** * Fixed upscaled video **incorrect colorspace** * Redesigned how the app reports **fluidify progress** * General bugfixes and performance improvements * Updated dependencies	0.0	t3_18kd2zx	reddit		
295	messing around with ui's	Unknown	2023-12-17 01:11:51	https://www.reddit.com/r/Python/comments/18k5mvx/messing_around_with_uis/	i just modified a old shitty encrypted notes app to make a fancy ui as a bit of a test to see how to make better ui's and it turned out pretty good. [heres](https://github.com/popcornman209/sf-notes) github link the code is absolutely horrendous its a mix of super old code and new code (joking ofc but it was just a test so dont be suprised when your eyes start bleeding out) (when it comes to the encryption it was my own very bad algorithm that i made for fun, DONT use it, i was just bored and i know of many ways you crack the code to it decently easily) https://preview.redd.it/3xwm10yk9r6c1.jpg?width=960&format=pjpg&auto=webp&s=d02117f6198e4727b0d9da5ac4315f4e4599a617	1.0	t3_18k5mvx	reddit		
296	üöÄ PYGGESTER	Unknown	2023-12-16 23:12:22	https://www.reddit.com/r/Python/comments/18k38yk/pyggester/	Pyggester is a dynamic/static analyzer used to suggest improvements to suboptimal data structure usages in Python. üîóGitHub: [https://github.com/ValdonVitija/pyggester](https://github.com/ValdonVitija/pyggester) üì¶ pip installable (pip install pyggester) ‚úÖ Super easy to use üñ•Ô∏è Intuitive CLI interface üìö Fully documented: üìò User guide üìñ Code documentation (abstract, docstrings) üìù Extremely detailed contribution guide üß© Modular project structure üîù More than 30+ potential data structure suggestions üåü Your Support Matters! Your star on GitHub would be greatly appreciated. Thanks in advance!	3.0	t3_18k38yk	reddit		
297	Sunday Daily Thread: What's everyone working on this week?	Unknown	2023-12-17 00:00:09	https://www.reddit.com/r/Python/comments/18k47x7/sunday_daily_thread_whats_everyone_working_on/	# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to! ## How it Works: 1. **Show & Tell**: Share your current projects, completed works, or future ideas. 2. **Discuss**: Get feedback, find collaborators, or just chat about your project. 3. **Inspire**: Your project might inspire someone else, just as you might get inspired here. ## Guidelines: * Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome. * Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here. ## Example Shares: 1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate! 2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better. 3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier! Let's build and grow together! Share your journey and learn from others. Happy coding! üåü	10.0	t3_18k47x7	reddit		
298	Documenting my experience building a Python CLI tool to deploy my builds to AWS EC2 instance	Unknown	2023-12-17 05:37:02	https://www.reddit.com/r/Python/comments/18ka9gx/documenting_my_experience_building_a_python_cli/	Hey Folks, Hope everyone is doing good. I recently started blogging on Medium and I would appreciate your feedback on my given blog which would help me further improve. Just to set the context straight , this is blog is a practical guide documenting my experience on building a Python based CLI command which deploys the provided build to a Amazon EC2 instance on click of one simple command. Blog link : https://medium.com/@anubhavsanyal/from-code-to-cloud-automating-ec2-deployments-with-python-cli-e262396559a9	1.0	t3_18ka9gx	reddit		
299	Banish state-mutating methods from data classes	Unknown	2023-12-17 13:31:11	https://www.reddit.com/r/Python/comments/18kheee/banish_statemutating_methods_from_data_classes/	I've always found mutable data classes or record types in programming languages quite weird. This holds true for any language that supports mutable data classes, like Python, Kotlin, Swift, etc. Self-mutating methods are even weirder. They break the semantics of the types that these data classes or records represent. I've expanded my thoughts here. Although it mostly talks about Python, the message remains valid for any other language that allows you to add self-mutating methods to data classes. https://rednafi.com/python/dataclasses_and_methods/	14.0	t3_18kheee	reddit		
300	[Tkinter] TkClassWizard - Graphically define objects	Unknown	2023-12-17 01:08:25	https://www.reddit.com/r/Python/comments/18k5khj/tkinter_tkclasswizard_graphically_define_objects/	Hello guys, I've recently created a new library called TkClassWizard. Originally it was part of a different library, but I decided to split it for reusage. Features: * Graphical definition of any class that has been type annotated. * Validation of defined values * Ability to register / overwrite additional annotations to classes that cannot be modified * Save the defined data into fake objects and then into real Python objects (or vice-versa) [https://github.com/davidhozic/TkClassWizard](https://github.com/davidhozic/TkClassWizard)	0.0	t3_18k5khj	reddit		
301	Do you guys use any accessory libraries to manage or expedite your web scraping?	Unknown	2023-12-16 00:59:20	https://www.reddit.com/r/Python/comments/18jfasl/do_you_guys_use_any_accessory_libraries_to_manage/	I'm mainly referring to additional libraries that aren't the big popular primary HTML or JS parsers (b4s, selenium, scrapy). I'm no stranger to writing some truly heinous Regex parsing scripts with some sinful nested loops iterating over hundreds of web-pages on sites where there was clearly no templated format being adhered to, essentially non-existent HTML tree structure patterns to design around. I have an upcoming project where I may end up needing to write scripts such as this for a very small company where shelling out the hundreds/low thousands per month for proper API access is not likely to take priority over the other pricy budgeted items. How do you guys go about dealing with either poorly structured webpages or, at the other end, highly dynamic high foot traffic websites where the front end is probably getting script breaking updates at least monthly? Anyway to make this stuff more resilient/easier to repair or is it still the same PITA to deal with as it has been for the last 15+ years	14.0	t3_18jfasl	reddit		
302	Feud: Build simple CLIs based on Pydantic for typing and Click for argument parsing.	Unknown	2023-12-15 16:31:09	https://www.reddit.com/r/Python/comments/18j41fv/feud_build_simple_clis_based_on_pydantic_for/	"- **GitHub repository**: https://github.com/eonu/feud/ - **API reference**: https://docs.feud.wiki/ --- Hello! I've spent the last three months or so working on a new package, [**Feud**](https://github.com/eonu/feud/), for building CLIs in Python, and thought I'd share it here for anyone interested. While existing solutions such as [Click](https://github.com/pallets/click) and [Typer](https://github.com/tiangolo/typer) often provide most of the necessary functionality for the majority of CLIs, I've found that there are still some things missing from these packages, such as validation, support of common input types and high ease-of-use. *Feud combines the best parts of the following packages to make building CLIs as smooth as possible*: - **Click**: Argument parsing library used either: - directly to implement CLIs, - or as the core of other CLI-building packages like Typer and Feud itself. - [**Pydantic**](https://github.com/pydantic/pydantic): Popular data validation library for Python. ### Basic features #### Relies on Python language features To make it as easy as possible for even beginner Python developers to quickly create sophisticated CLIs, Feud aims to rely on basic core language features as much as possible, e.g.: - using type annotations from the function signature to validate CLI inputs (via Pydantic), - using the function signature structure (e.g. default values, the `*` operator to separate arguments and options) to determine arguments, options, required options and defaults. - using docstrings to define CLI help commands and parameter descriptions. ```python # serve.py import feud from typing import Literal def serve(port: int, *, watch: bool = True, env: Literal[""dev"", ""prod""] = ""dev""): """"""Start a local HTTP server. Parameters ---------- port: Server port. watch: Watch source code for changes. env: Environment mode. """""" if __name__ == ""__main__"": feud.run(serve) ``` Usage: - `python serve.py 8080` - `python serve.py 3000 --watch --env dev` - `python serve.py 4567 --no-watch --env prod` **See the [README.md](https://github.com/eonu/feud/blob/master/README.md) and [API reference](https://docs.feud.wiki/) for more examples.** #### Easy validation and extensive type support (via Pydantic) Essentially all types supported by Pydantic are also supported by Feud, meaning that it is very straightforward to perform validation and type conversion of CLI inputs. ```python # command.py import feud from pydantic import PositiveInt, FutureDate, conlist def command(ids: conlist(PositiveInt, max_length=3), *, date: FutureDate): """"""Command that accepts up to three ID arguments (must be positive integers), and an optional date that must be in the future. """""" if __name__ == ""__main__"": feud.run(command) ``` e.g. calling `$ python command.py 2 5 8 --date 2029-12-02` would make: - the list of integers `[2, 5, 8]` accessible as the `ids` argument, - the date `datetime.date(2029, 12, 02)` accessible as the `date` argument. And of course, an appropriate error would be shown if the validation of any parameter failed. #### Simple grouping of commands To create a group of commands, you can simply define functions in a `feud.Group` subclass. ```python # post.py import feud from datetime import date class Post(feud.Group): """"""Manage blog posts."""""" def create(id: int, *, title: str, desc: str | None = None): """"""Create a blog post."""""" def delete(*ids: int): """"""Delete blog posts."""""" def list(*, between: tuple[date, date] | None = None): """"""View all blog posts, optionally filtering by date range."""""" if __name__ == ""__main__"": feud.run(Post) ``` Usage: ```bash $ python post.py --help Usage: post.py [OPTIONS] COMMAND [ARGS]... Manage blog posts. ‚ï≠‚îÄ Options ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ --help Show this message and exit. ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ‚ï≠‚îÄ Commands ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚îÇ create Create a blog post. ‚îÇ ‚îÇ delete Delete blog posts. ‚îÇ ‚îÇ list View all blog posts, optionally filtering by date range. ‚îÇ ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ ``` ### Comparison with similar packages - [`argparse`](https://docs.python.org/3/library/argparse.html) (and [`optparse`](https://docs.python.org/3/library/optparse.html) ‚Äì deprecated): The standard library argument parsing libraries are perfectly fine to use for simple CLIs, and even complex CLIs as long as you are careful in maintaining them, and happy to deal with the somewhat verbose code required to build a CLI. Feud may not provide every feature offered by these more fundamental packages, but it is designed to have a much more user-friendly experience and better input validation. - [`click`](https://github.com/pallets/click): Click forms the basis of Feud, as all Feud-defined commands and groups are 'compiled' into Click applications. While Click itself is a huge upgrade from `argparse`, it can sometimes still be a bit verbose to use when building CLIs. - [`typer`](https://github.com/tiangolo/typer): Typer is a more complete CLI-building package that is also based on Click, but currently lacks support for more complex types such as those offered by Pydantic."	4.0	t3_18j41fv	reddit		
303	Abstract Blockchain: designed to streamline and simplify interactions with blockchain networks and smart contracts.	Unknown	2023-12-16 14:44:25	https://www.reddit.com/r/Python/comments/18jsnyc/abstract_blockchain_designed_to_streamline_and/	Abstract Blockchain Abstract Blockchain is a Python package designed to streamline and simplify interactions with blockchain networks and smart contracts. It consists of various utilities that enable users to manage RPC parameters, work with smart contract ABIs, and facilitate user-friendly interactions using graphical user interfaces (GUIs). [GITHUB-Abstract\_Blockchain](https://github.com/AbstractEndeavors/abstract_blockchain/tree/main)[PYPI-Abstract\_Blockchain](https://pypi.org/project/abstract-blockchain/) ## Available Modules * **abstract\_abis.py**: This module provides the `ABIBridge` class, an interface to Ethereum smart contract ABIs. It allows interactions with contract functions and retrieves read-only functions. Additionally, it categorizes RPC parameters for easier blockchain interaction. * **abstract\_apis.py**: Houses the `APIBridge` class for managing API URL creation and their respective calls. It contains GUI-enabled tools to build API URLs or fetch preselected call parameters. * **abstract\_rpcs.py**: This module offers the `RPCBridge` class that manages the RPC parameters for different blockchain networks. It provides a GUI for filtering and selecting RPC parameters and organizes them for easy use. * **abstract\_accounts.py**: The `ACCTBridge` class in this module allows interfacing with your personal wallet. You can build transaction information, derive public keys, and send/verify transactions. * **abstract\_contract\_console.py**: This section of the module integrates all classes for a harmonious interaction with smart contracts. * **abstract\_gui.py** : This submodule provides utilities for creating GUIs that enhance user interaction with blockchain-related features. * **main.py**: This is the entry point of the package where files are uploaded. ## Installation The package is available on [PyPI](https://pypi.org/project/abstract-blockchain/). You can install it using pip with `pip install abstract-blockchain`. [providing a Pythonic interface to their Application Binary Interfaces \(ABIs\).](https://preview.redd.it/hm5vhud55o6c1.png?width=604&format=png&auto=webp&s=654f1011dd48b0230dfbd5c175d243502fe9f683) ## Example Usage from abstract_abis import ABIBridge from abstract_apis import Choose_RPC_Parameters_GUI, RPCData # Example usage of ABIBridge abi_manager = ABIBridge(contract_address='0x3dCCeAE634f371E779c894A1cEa43a09C23af8d5', rpc=default_rpc()) read_only_functions = abi_manager.get_read_only_functions() for each in read_only_functions: inputs = abi_manager.get_required_inputs(each) if len(inputs) == 0: result = abi_manager.call_function(each) print(each, result) else: print(each, inputs) # Example usage of RPCData and GUI rpc_data = Choose_RPC_Parameters_GUI() rpc_manager = RPCData(rpc_data) w3 = rpc_manager.w3 # Your blockchain interactions using w3... ## Installation The `abstract_blockchain` package can be installed using pip: pip install abstract_blockchain ## Module - abstract_accounts.py This module, under the `abstract_blockchain` package, includes the `ACCTBridge` class, providing an interface to the user's personal wallet. It interacts with Ethereum accounts, allowing the user to perform transactions, estimate gas, retrieve transaction counts, sign and send transactions, and handle Ethereum addresses. The module primarily leverages other modules and classes from `abstract_rpcs.py` and `abstract_apis.py`, requiring the `eth_account` package to interact with Ethereum accounts. The critical methods within the `ACCTBridge` class include: * `__init__`: Initializes the `ACCTBridge` object, establishing an RPC bridge for interaction and retrieves the private key and account address. * `check_priv_key` & `get_address_from_private_key`: Manages operations related to the private key and converts it into an Ethereum address. * `build_txn` & `get_txn_info`: Allows the user to build a transaction, accounting for multiple variables. * `check_sum` & `try_check_sum`: Manages the conversion of the address to a checksum address. * `get_transaction_count`: Fetches the transaction count of the Ethereum account. * `sign_transaction` & `send_transaction`: Handles the signing and sending of transactions. * `estimate_gas`: Estimates the gas fee for Ethereum transactions. The module employs a rate limiting manager to manage the frequency of requests, preventing the exceeding of API rate limits. ## Module - abstract_abis.py Part of the `abstract_blockchain` package, the `abstract_abis.py` module is a critical component intended to streamline interactions with Ethereum smart contracts, providing a Pythonic interface to their Application Binary Interfaces (ABIs). The core of this module is the `ABIBridge` class, which offers an encompassing interface for managing Ethereum contract ABIs. Just like the `abstract_accounts.py` module, `abstract_abis.py` performs various tasks in collaboration with other modules, principally `abstract_rpcs.py` and `abstract_apis.py`. It houses several methods responsible for: * `Validating` Ethereum addresses * Creating `ABI bridges` * `Enumerating` contract functions * `Accessing` read-only functions from contract ABIs * `Invoking` contract functions * `Acquiring` and `categorizing` RPC parameters that are crucial for interaction with the blockchain * `Managing` rate limiting for API requests The `ABIBridge` class also emphasizes contracts' functions, offering tools to list all functions, obtain necessary input details, and invoke functions smoothly. It also provides mechanisms to create functions ready to be executed in future operations. A default\_rpc() function is also included, providing a default RPC configuration used when an instance of ABIBridge is created. This function underlines how to utilize the ABIBridge class for tasks like interacting with Ethereum contracts, managing user interaction, retrieving read-only functions, obtaining required input details, and invoking contract functions. # Module - abstract_api_gui.py Part of the `abstract_blockchain` package, `abstract_api_gui.py` is a module that builds a Graphical User Interface (GUI) to simplify interactions with APIs. This module utilises `PySimpleGUI` for creating the GUI, and `abstract_utilities.list_utils` for utility functions. The primary features of this module include: * Declaring hard-coded API descriptions through `apiCallDesc` * Defining options for API actions using `options` * Streamlining API parameters with `inputs` Additionally, it introduces several functions to generate API GUI and manage user interactions: 1. `get_revised_dict()`: This function modifies a dictionary based on required keys. 2. `generate_api_variables()`: This function produces API variables based on user input. 3. `generate_api_gui(api_desc)`: Generates the GUI layout with the given API description. 4. `make_invisible_unless(window,values,visible_list,example)`: Manages UI elements' visibility based on user input. 5. `choose_api_gui()`: Renders the main GUI displaying available APIs, parameters, and execution results. In the main application execution, `choose_api_gui` function gets called to display the user interface. As user actions are detected in the dropdown or interaction buttons, relevant changes are made in the GUI, providing a user-friendly environment for API interaction. # The abstract_apis.py Script The `abstract_apis.py` script is a core component of the abstract-blockchain module. This script houses the `APIBridge` class which serves as the robust engine for API URL construction and execution, and efficient GUI management for RPC parameters. ## Key Components * `APIBridge` class: This primary class initializes with optional parameters, `api_data`, `rpc`, and `address`. To ensure smooth functionality and improve efficiency, the class imports from external modules namely, `abstract_webtools` and `abstract_utilities`. ## Methods of the APIBridge class include: * `get_api_data_string`: This method prepares the appropriate API data string based on the provided `api_data_type` and `address`. * `api_keys`: An essential method that retrieves API keys from the environment settings contingent on the scanner in use. * `get_http_variants`, `get_api_variants`: These methods lend a hand in navigating different versions of the API URL based on request requirements. * `try_api_url`: As the name suggests, this method obtains the API request URL. * `get_try`, `get_request`: Key in sending a request to the API URL with a focus on 'Rate-Limiting'. * `get_response`: This method parses the JSON response from the API. * `check_sum`, `try_check_sum`: Integral in ensuring the validity of an Ethereum address and converting it to a checksum address. * `safe_json_loads`: A safety-oriented method that loads JSON data as a dictionary or a list. Remember to study the functional activities of this module and understand its relevance in the larger abstract-blockchain module. Join us in the next section where we detail another integral part of the module, the abstract-api-gui.py script. # abstract_contract_console.py The `abstract_contract_console.py` module within the `abstract-blockchain` package offers a Graphical User Interface(GUI). This GUI is ideal for users interested in engaging interactively with the Smart Contracts on the Ethereum Blockchain. At its core, it is designed to provide a user-friendly console for interacting with Ethereum Smart Contract functions. The module imports required dependencies and presents helper functions to perform data conversions and error checks. This setup lays the foundation for an interface that allows users to input a contract address on the Ethereum Blockchain. Once the address is provided, the interface makes a connection with a network node that will process the desired transactions and fetch available contract functions. All available functions are displayed in the GUI, which features input fields and buttons to facilitate interaction. The GUI shows the return value for functions having 'read' or 'pure' mutability; for transaction-triggering functions, the transaction hash is displayed. Notably, this module also permits users to manually input their account details, vital for signing transactions when calling non-read-only functions. Determining the Endpoint URL to connect to an Ethereum node is made simple with the `get_account_layout` and `determine_correct_rpc` functions. Operating within a GUI Event Loop, the interface maintains responsiveness and interactive capabilities. When a button associated with a contract function is clicked, input values are collected, the function is executed, and the output is displayed. In summary, `abstract_contract_console.py` emulates a local Ethereum interaction console, simplifying users' interactions with the Ethereum Blockchain and providing an all-in-one abstraction service. Furthermore, the module features an `abstract_contract_console_main()`. This function leverages various classes from the package for smart contract interaction and accepts an optional 'rpc\_list' argument. If no argument is provided, it defaults to the RPC list from the RPCBridge class. To launch a new window titled 'New Blockchain Console,' it retrieves the RPC layout and values, constructs a final layout consisting of the account, ABI, and RPC layouts. The new\_window\_mgr object's `while_basic()` method responsibly handles the main application loop, managing events and rendering. ## Overview The `abstract_contracts.py` script is a part of the `abstract_blockchain` module which is designed to facilitate the interaction of users with the Ethereum network. The script is built around a suite of utility functions which assist users in communicating with smart contracts on the blockchain, providing an easy and effective way to perform a variety of tasks from simple data conversions to transaction generation. ## Key Features 1. **Versatile Utility Functions:** The script offers utility functions which handle a wide range of features including estimation of average gas price for transactions, verification and handling of different input types (e.g., boolean, integers, and addresses), and user interaction management through GUIs. 2. **Data Type Conversion:** It supports conversion of different data types (e.g., addresses, integers, bytes, boolean values, and strings) to their required format for interaction with the Ethereum blockchain. 3. **User-friendly Interaction:** The script provides a comfortable user interface, allowing for inputs confirmation for a smart contract function or generation of a new transaction. 4. **Error Handling and Validation:** It manages various situations and edge cases, such as checking the validity of a potential Ethereum address and dealing with batch inputs. It provides intuitive feedback and correction options throughout the code execution. ## abstract_contract Module The `abstract_contracts.py` script within the abstract\_blockchain module is designed to enhance user interaction with blockchain contracts associated with the Ethereum network. This script extensively uses the web3 module to enable these interactions while also leveraging PySimpleGUI to deliver a simple yet effective graphical user interface. The script initiates with the importation of needed modules and subsequently defines several utility functions. These functions specialize in aiding users in contract communication and boosting their utilization of smart contracts within the Ethereum network. Several features are encompassed by these functions such as average gas price estimation for transactions, verification and management of multiple input types including boolean, integers, and addresses, user interaction facilitation via GUIs, and tasks like check-summing for Ethereum addresses. Further, they empower users to interact directly with the blockchain, such as confirming inputs for a smart contract function or even producing a new transaction. The script introduces functions to convert a diverse range of data types to a format that is suitable for the interaction with the Ethereum blockchain. Data types like addresses, integers, bytes, boolean values, and strings are all catered to. They also support list processing of these data types, allowing for batch operations. The functions are coded to handle a broad array of situations and edge cases such as validating potential Ethereum addresses and managing batch inputs. The code regularly provides intuitive feedback and correction options. Overall, this script serves as a comprehensive suite of tools for users interested in interacting with smart contract careers on the Ethereum network. ## abstract_rpcs Module The abstract\_rpcs.py script is part of the Abstract Blockchain package and it offers the RPCBridge class that is designed to manage RPC parameters for different blockchain networks. RPC parameters are necessary for remote procedure calls (RPC), a protocol that one program can use to request a service from a program located in another computer on a network without having to understand the network's details. In the context of blockchain, RPC parameters are used to interact with the blockchain network. The RPCBridge class in abstract\_rpcs.py provides several key functionalities. It allows users to filter and select RPC parameters through a graphical user interface (GUI), streamlining the process of defining these parameters. This makes it user-friendly even for those without deep knowledge of RPC parameters. The RPCBridge class also categorizes and organizes RPC parameters, so that they can easily be used in blockchain interactions. Information is thus made more accessible, and the usage of blockchain technology is simplified as a result. Overall, the abstract\_rpcs.py script plays a crucial role in the Abstract Blockchain package by providing efficient and streamlined ways to manage RPC parameters, one of the essential parts of interacting with blockchain networks. In conclusion, `abstract_blockchain` is a powerful tool that allows developers to interact with Ethereum networks and contracts. While the package is easy to install and use, we always recommend carefully reading the documentation and understanding how blockchain technologies work before getting started. Happy coding! For more info regarding license, please visit [here](https://github.com/AbstractEndeavors/abstract_blockchain/blob/main/LICENSE).	2.0	t3_18jsnyc	reddit		
304	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2023-12-16 00:00:19	https://www.reddit.com/r/Python/comments/18je37p/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	1.0	t3_18je37p	reddit		
305	Personal Telegram Bot with ChatGPT, Stable Diff, Kandinsky and other useful tools	Unknown	2023-12-15 15:03:37	https://www.reddit.com/r/Python/comments/18j23f9/personal_telegram_bot_with_chatgpt_stable_diff/	Hello everyone, I have created a Telegram bot with a lot of useful features that can be helpful for you too: \- Most OpenAI models (GPT4, Dalle3, TTS, Whisper) \- A large number of free models hosted in Replicate (Kandinsky, Stable Diffusion, Blip, LLama) \- Ability to download media content from various sources Please leave stars if you liked my project Github: [https://github.com/Sets88/sets88\_telegram\_bot](https://github.com/Sets88/sets88_telegram_bot)	1.0	t3_18j23f9	reddit		
306	PyCon US 2024 Registration Now Open	:litestar-logo: Litestar Maintainer & :ruff-logo: Ruff Fanboy 	2023-12-15 08:20:39	https://www.reddit.com/r/Python/comments/18ivdl7/pycon_us_2024_registration_now_open/	# PyCon US 2024 Registration Opens &#x200B; * [Register Now](https://us.pycon.org/2024/registration/register) * [PyCon Blog Announcement](https://pycon.blogspot.com/2023/12/pycon-us-2024-registration-launch.html) You have 30 days until the early bird pricing is gone! ### Early Bird Registration prices PyCon US offers discounted early bird rates for Corporate, Individual, and Student ticket types **for the first 30 days** that registration is open. All rates will switch to the regular rate prices on January 13, 2024. |Cost|Rate| |:-|:-| |650|Corporate| |350|Individual| |75|Student| |100|Online (not included in Early Bird Discount)| &#x200B; ## Details May 15 - May 23, 2024 - Pittsburgh, Pennsylvania Conference breakdown: * Tutorials: May 15 - 16, 2024 * Main Conference and Online: May 17 - 19, 2024 * Job Fair: May 19, 2024 * Sprints: May 20 - May 23, 2024 &#x200B;	1.0	t3_18ivdl7	reddit		
307	Simplifying django-rest-framework Testing with drf-api-action	Unknown	2023-12-15 15:21:32	https://www.reddit.com/r/Python/comments/18j2hjy/simplifying_djangorestframework_testing_with/	Hi all! I‚Äôm proud to share my new first open-source project drf-api-action. https://github.com/Ori-Roza/drf-api-action This project was built as a side project at work in which we had to tweak DRF for our own needs, this project was successful back then so I wanted to publish it to everyone :) The drf-api-action Python package is designed to elevate your testing experience for Django Rest Framework (DRF) REST endpoints. With the custom decorator api-action, this package empowers you to effortlessly test your REST endpoints as if they were conventional functions. Features: Simplified Testing: Testing DRF REST endpoints using the api-action decorator, treating them like regular functions. Seamless Integration: Replacing DRF's action decorator with api-action in your WebViewSet seamlessly. Clear Traceback: Instead of getting a response with error code, get the real traceback that led to the error. Pagination Support: Paginating easily through pages by a single kwarg. Please let me know what you think/ any feedback. It means a lot since it's my first open-source project :) \#feedback #feedbackwelcome #djangorestframework #python #django	0.0	t3_18j2hjy	reddit		
308	Implementing 3.12 nested strings to build device configurations without Jinja	Unknown	2023-12-14 23:49:51	https://www.reddit.com/r/Python/comments/18imcvy/implementing_312_nested_strings_to_build_device/	"So I was playing around with 3.12's newly expanded f-string capability to see how it well could work for building network device configurations. I was able to come up with something pretty cool that worked but it used an eval statement which I am not a fan of. Can anyone think of a clean way to achieve this without using eval? #!/usr/bin/python3.12 def repeat(input_data): response = """" input_data = ""f\""\""\"""" + input_data + ""\""\""\"""" for i in range(5): interface = f""1/{i}"" description = f""test user interface {i}"" response = response + eval(input_data) + ""\n"" return response.strip() config = f"""""" This is a test of {repeat(""""""\ interface {interface} description {description}"""""")} The test is ending """""" print(config) result: This is an f-string test interface 1/0 description test user interface 0 interface 1/1 description test user interface 1 interface 1/2 description test user interface 2 interface 1/3 description test user interface 3 interface 1/4 description test user interface 4 The test is ending &#x200B; Keep in mind that in the real world `config` is more like 300 lines long and not 6. This is why I am trying to keep `config` as clean and compact as possible sacrificing complexity elsewhere. What do you think about this complexity trade offs, is eval a mortal sin if I always control the input?"	4.0	t3_18imcvy	reddit		
309	Python course for Duality Views (JSON + Relational at the same time)	Unknown	2023-12-15 16:49:17	https://www.reddit.com/r/Python/comments/18j4ggg/python_course_for_duality_views_json_relational/	Hi Python devs! I had created a workshop for learning how to use Python with JSON and Oracle Duality Views. The Duality Views allows to work with JSON and Relational and the same time, combining the benefits of both! If you want to have a short overview, you can go to this linkedin post with a short video summary [Duality Views Video](https://www.linkedin.com/posts/javier-de-la-torre-medina_oracle-json-python-activity-7136277598828904448-7a5c) which points to [dev.to](https://dev.to) blogs where all the details and code are there. Willing to hear your feedback!	0.0	t3_18j4ggg	reddit		
310	A simple library to write and read disk based datasets for ML training	Unknown	2023-12-15 08:00:48	https://www.reddit.com/r/Python/comments/18iv3l1/a_simple_library_to_write_and_read_disk_based/	"If you don't have enough RAM to load the whole dataset, it is probably required to load them during training. Here is a simple library to help with that (link at the end). The main class of the library is `SplitDataLoader`. It supports len() and indexing. The ""split"" here is for splitting the dataset into tiny files. This seems to help with a number of issues. One of them is the shuffling of data during loading. Additionally, the library also has a helper routine to run a generator in another process and get the result via a queue. This will make sure that the training loop does not have to wait for data loading. Writing data looks like this: from splitdataloader import write_split_data def example_writer(...): # Get the data source data_source: Iterable[bytes] = some_source() target_dir = ""tmp/training_data"" write_split_data(target_dir, data_source, splits=128) And reading would look like this: from splitdataloader import SplitDataLoader def example_loader(...): # Get the data source data_dir = ""tmp/training_data"" loader = SplitDataLoader(data_dir) # Supports len() print(len(loader)) # Supports indexing data = loader[2] # Supports iteration for data in loader.iterate_binwise(shuffle=True): do_something(data) This is the library: [https://github.com/charstorm/split-data-loader](https://github.com/charstorm/split-data-loader)"	0.0	t3_18iv3l1	reddit		
311	Staying Excited about Python!	Unknown	2023-12-14 21:09:35	https://www.reddit.com/r/Python/comments/18iirrf/staying_excited_about_python/	Hey all, I was just wondering what kinds of resources do you use to stay updated about current trends in the Python community? Websites, podcasts, YouTube channels?!? I joined a few meetups in my local area and they‚Äôre awesome (but only once a month or so)! Looking for a more condensed/valuable r/Python. Thanks!	5.0	t3_18iirrf	reddit		
312	Registration is open for PyCon US 2024	:pythonLogo: Python Software Foundation Staff	2023-12-14 23:13:54	https://www.reddit.com/r/Python/comments/18illcu/registration_is_open_for_pycon_us_2024/	We'll be in Pittsburgh PA this year, May 16-23 2024, and early bird prices are in effect now through January 12. [Lots more information on the PyCon US site](https://us.pycon.org/2024/attend/information/).	1.0	t3_18illcu	reddit		
313	GPT-4 Advanced Data Analysis: A Beginner‚Äôs Guide to Charts and Maps	Unknown	2023-12-15 15:46:13	https://www.reddit.com/r/Python/comments/18j31fc/gpt4_advanced_data_analysis_a_beginners_guide_to/	üåç Calling all data science mapping enthusiasts! The #GPT4 Advanced Data Analysis tool simplifies the data visualization process. This article shows beginners how to create on-the-fly maps and charts from just a CSV file.üìä üëá üëá **Medium Friend Link (No Paywall)**: [https://pub.towardsai.net/gpt-4-advanced-data-analysis-a-beginners-guide-to-charts-and-maps-d59763487750?source=friends\_link&sk=faceba7fdb89a4363f3dfae5043b02d6](https://pub.towardsai.net/gpt-4-advanced-data-analysis-a-beginners-guide-to-charts-and-maps-d59763487750?source=friends_link&sk=faceba7fdb89a4363f3dfae5043b02d6) [Dall-E 2 image: impressionist painting in oil colors of a map of earth](https://preview.redd.it/5tvx1tqybh6c1.png?width=1008&format=png&auto=webp&s=52791016fc70496ca025719aa58150c7ae8da907)	0.0	t3_18j31fc	reddit		
314	Monadic-Error Package	Unknown	2023-12-14 15:21:53	https://www.reddit.com/r/Python/comments/18iaz0x/monadicerror_package/	Hey everyone, I just recently published a new version of a python package called `monadic-error`. This library implements two error handling monads: Option and Attempt (or if you're familiar with Haskell Maybe and Either), along with some helper functions and decorators to make integration into current code easy. I'd love some feedback on improvements for the library, and on the coding style in general. The library is written using Python 3.12 type syntax. PyPI: https://pypi.org/project/monadic-error/ Source Code: https://github.com/ikollipara/monadic-error	4.0	t3_18iaz0x	reddit		
315	Check wrong private function calls pre-commit hook	Unknown	2023-12-14 17:15:55	https://www.reddit.com/r/Python/comments/18idhhz/check_wrong_private_function_calls_precommit_hook/	Python doesn't actually complain about a private function being called outside of the module it was defined, but to do that is bad practice. As a code owner of several begginer friendly open source projects I decided to create a pre-commit hook that checks that and throws an exception if one of those bad function calls is found. [Eric-Mendes/private-calls-pre-commit: pre-commit hook that checks whether a private function is being called outside of its module. (github.com)](https://github.com/Eric-Mendes/private-calls-pre-commit) Not everything is working as expected, but I'm open for contributions!	2.0	t3_18idhhz	reddit		
316	Implementing Role-Based Access Control in Django	Unknown	2023-12-14 15:46:41	https://www.reddit.com/r/Python/comments/18ibhyh/implementing_rolebased_access_control_in_django/	Hi folks, We've written a brief guide to demonstrate how to implement Role-Based Access Control (RBAC) in a Django app üîí In particular, we've utilized Django's built-in authentication system to enable or restrict user access to resources and actions based on the permissions assigned to their roles. Here's the post if you're interested: [https://www.permify.co/post/rbac-in-django/](https://www.permify.co/post/rbac-in-django/) Looking forward for your feedbacks!!	0.0	t3_18ibhyh	reddit		
317	De4py toolkit for python RE v1.0.4 has been released	Unknown	2023-12-14 16:45:35	https://www.reddit.com/r/Python/comments/18icssg/de4py_toolkit_for_python_re_v104_has_been_released/	[De4py v1.0.4](https://github.com/Fadi002/de4py/releases/tag/v1.0.4-stable) update has been released and includes an advanced feature that can be used for analysis of python files by monitoring it's behavior. in this update a major feature have taken place, memory analyzing (WinAPI hooking). Monitoring Files: * Monitoring File Handles Creation Monitoring Processes: * Monitoring Process handle * Monitoring if the process tried to write/read to a process * Monitoring Termination of other processes Monitoring Connections: * Monitoring Socket creation * Monitoring sending/receiving of data with the size of data sent/recieved. &#x200B; more to come soon.	0.0	t3_18icssg	reddit		
318	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2023-12-15 00:01:13	https://www.reddit.com/r/Python/comments/18imlqn/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	2.0	t3_18imlqn	reddit		
319	Beam: Run Python functions on the cloud in seconds	Unknown	2023-12-13 15:02:59	https://www.reddit.com/r/Python/comments/18hiqyq/beam_run_python_functions_on_the_cloud_in_seconds/	"Hi r/python! I‚Äôm Eli, and my co-founder and I built [Beam](https://beam.cloud) to quickly get your Python code running on the cloud. All you need to do is install a Python SDK, add a decorator to your code, and run a CLI command to get a cloud endpoint for your function. Here is an extremely simple example of a function that can be deployed onto a GPU, as a REST API: from beam import App, Runtime, Image # Register your function to Beam app = App( name=""quickstart"", runtime=Runtime(gpu=""A10G"", image=Image(python_packages=[""requests"", ""pandas""])), ) # Add a decorator to any function to run it remotely @app.rest_api() def hello_world(): return {""This is running on the cloud remotely!""} This function can be deployed as a REST API by running only one command: `beam deploy app.py:hello_world` When you run this command, you‚Äôll get a web endpoint with load balancing, authentication, and autoscaling. There‚Äôs also a web dashboard to view logs, metrics, and other data you‚Äôd want to monitor for your app. We also have GPU support, so it‚Äôs super easy to run compute-heavy workloads on the cloud using Beam. **The Pricing Model** Pricing is pay-for-what-you-use. Beam is serverless, so your apps will turn off when you‚Äôre not using them. If you don‚Äôt use your API, you don‚Äôt pay anything. It‚Äôs pretty simple. Also, Beam has a 10 hour free tier! You can sign up and immediately start running workloads for free. **Things you can build with Beam** * [GPU-accelerated APIs for AI inference](https://docs.beam.cloud/examples/stable-diffusion-gpu) * [Web scrapers](https://docs.beam.cloud/examples/reddit-posts) * [ASGI apps](https://docs.beam.cloud/examples/gradio) * [Cron jobs](https://docs.beam.cloud/deployment/schedule) This is a relatively new platform, so it would be great to hear your thoughts and feedback. Thanks for checking it out! [https://docs.beam.cloud](https://docs.beam.cloud)"	21.0	t3_18hiqyq	reddit		
320	The Evolution of GPT-4: Crafting Python Plotly Dashboards With Ease	Unknown	2023-12-14 16:23:02	https://www.reddit.com/r/Python/comments/18icaki/the_evolution_of_gpt4_crafting_python_plotly/	A few months ago, I wrote a series of (moderately successful) articles on how to prompt GPT-4 for Python ***plotly*** dashboard creation. Back then, I ran into some limitations on the capabilities available - namely GPT-4 would cough, cry, and complain about creating multi-visual Plotly dashboard code. And so would I. It was frustrating. With the added functionality in the main chat window, can GPT-4 NOW handle the task of ***seamlessly creating*** complex plotly dashboard code from an uploaded CSV? The answer is yes! Here‚Äôs how it works. **MEDIUM FRIEND LINK (no paywall):** [https://pub.towardsai.net/the-evolution-of-gpt-4-crafting-python-plotly-dashboards-with-ease-7216952c476d?source=friends\_link&sk=3dd90dbc4647840ed15c141a4d8be7da](https://pub.towardsai.net/the-evolution-of-gpt-4-crafting-python-plotly-dashboards-with-ease-7216952c476d?source=friends_link&sk=3dd90dbc4647840ed15c141a4d8be7da) [Dall-E 2 image: impressionist painting of a global dashboard](https://preview.redd.it/i2jj8majda6c1.png?width=1008&format=png&auto=webp&s=8322ad767bcd1a52942cc893ab49f91363d4f730) &#x200B; &#x200B;	0.0	t3_18icaki	reddit		
321	I made a Windows Notepad Alternative with PyQt6!	Unknown	2023-12-13 15:12:25	https://www.reddit.com/r/Python/comments/18hiyes/i_made_a_windows_notepad_alternative_with_pyqt6/	ZenNotes is a minimalistic Notepad app with a sleek design inspired by [Fluent Design](https://fluent2.microsoft.design/). It offers the familiar look of the Windows Notepad while having much more powerful features like Translation, TTS, etc. GitHub: [https://github.com/rohankishore/ZenNotes](https://github.com/rohankishore/ZenNotes) Please star üåü the repo if you like my project. Also, visit Aura Text ([https://github.com/rohankishore/Aura-Text](https://github.com/rohankishore/Aura-Text)), my IDE project [UI](https://preview.redd.it/i9zm39ywv26c1.png?width=1444&format=png&auto=webp&s=0aa157effcacaa2f9eaaf66b621eb54e81258e23)	10.0	t3_18hiyes	reddit		
322	Cyclopts: A CLI library that fixes 13 annoying issues in Typer	Unknown	2023-12-13 18:09:46	https://www.reddit.com/r/Python/comments/18hn2t1/cyclopts_a_cli_library_that_fixes_13_annoying/	Like many users here, I've used Typer because of its popularity and how it seemingly removed a lot of boilerplate from CLI creation. However, after getting past the shiny exterior, I quickly found it unnecessary bloated function signatures, lacked a lot of features (such as supporting Literal types!), and the author is famous for not addressing issues and merging in pull-requests. So, I created my own CLI library, Cyclopts, that addresses all of these issues. Please check it out! Typer Comparison: [https://cyclopts.readthedocs.io/en/latest/vs\_typer/README.html](https://cyclopts.readthedocs.io/en/latest/vs_typer/README.html) Project Page: [https://github.com/BrianPugh/cyclopts](https://github.com/BrianPugh/cyclopts)	8.0	t3_18hn2t1	reddit		
323	Good cheat sheet for beginners	Unknown	2023-12-13 13:20:22	https://www.reddit.com/r/Python/comments/18hgn7c/good_cheat_sheet_for_beginners/	So I am writing an exam next week in python and R and we are allowed to have all kinds of cheat sheets. Chat bots are not allowed though which is kinda fucking me over because Im only somewhat good at coding in R and I would normally use ChatGPT to translate R code to python. The exam is very basic. The hardest part is knowing the commands for tidying and manipulating data and just general stuff. Is anyone aware of a good cheat sheet like a HTML file where you could use the search function for example to look up specific code? Because I have looked for something like this and failed to find anything. Any help would be greatly appreciated! Thanks	4.0	t3_18hgn7c	reddit		
324	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2023-12-14 00:00:09	https://www.reddit.com/r/Python/comments/18hv5qi/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	1.0	t3_18hv5qi	reddit		
325	Gsplat	Unknown	2023-12-13 11:02:57	https://www.reddit.com/r/Python/comments/18hedwu/gsplat/	gsplat is an open-source library for CUDA accelerated rasterization of gaussians with python bindings. It is inspired by the SIGGRAPH paper [3D Gaussian Splatting for Real-Time Rendering of Radiance Fields](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/). This library contains the neccessary components for efficient 3D to 2D projection, sorting, and alpha compositing of gaussians and their associated backward passes for inverse rendering. [https://github.com/nerfstudio-project/gsplat](https://github.com/nerfstudio-project/gsplat) &#x200B;	0.0	t3_18hedwu	reddit		
326	Finally released my first Open Source project and wondering how good/bad it reflects my skills	Unknown	2023-12-13 06:56:10	https://www.reddit.com/r/Python/comments/18hayoc/finally_released_my_first_open_source_project_and/	Hello !! I‚Äôm happy to introduce you my first ever released framework: `athena`. This project exists to simplify the creation and management of different sanity checking procedures, it offers an abstract way to communicate feedbacks and potentially select them (when embedded in a software with selection) The idea was to allow the creation of many independent checks that can have different kind of feedback or potentially different inputs. Then, with an abstraction layer (the framework) make them all able to communicate together and report to a single entity, for use in a tool or post/pre process for example (e.g. before publishing an asset) With the creation of configuration files, x independent checks can then be run all together and even be configured. (This allow to define different rule set for different teams or persons) One use case is as follows: - I have a team of 10 artists (3D modelers) - I need to ensure that everything they publish is conform to a standard I‚Äôve set so the next departments can know what to expect. - I need to ensure the model respect some rules, but also the scene, the published files, the task status (in project management) etc‚Ä¶ a lot of different things to ensure with different strategies. - I could write different functions and checks those things independently, but my goal is to give feedback to the user and make sure he understands what to fix (if I can‚Äôt fix it myself) - Using this framework I can write all checks in a similar way and a config file. Now, whether I have an interface or not, all my checks can be run together following the options in my configuration. Once done I have a status that allows to know if it‚Äôs successful or not and what to fix. Here‚Äôs the link to the project: https://github.com/gpijat/athena It‚Äôs also available on [PyPi](https://pypi.org/project/athena-sanity/1.0.0b0/), it was my first time releasing a project here as well as publishing documentation on [ReadTheDocs](https://athena-sanity.readthedocs.io/en/latest/) Here‚Äôs a video that shows the capabilities of the tool and therefore, of the framework: https://vimeo.com/880496852 It has a clear focus on vfx/game as it‚Äôs what it was created for originally. But at the end it can be used for anything else. Any feedback will be greatly appreciated, if you want to use it or have questions I‚Äôll be happy to answer! :) PS: I‚Äôve edited the introduction, I hope now it make more sense what the project is about ü§û Also please, don‚Äôt focus your attention on stupid typos, I know in the README the licence is referred as GPT instead of GPL, it‚Äôs already fixed and will be released soon with more meaningful changes on the doc.	10.0	t3_18hayoc	reddit		
327	I've made a CLI Tool to integrate's with Notion P.A.R.A Method	Unknown	2023-12-13 12:48:55	https://www.reddit.com/r/Python/comments/18hg2rr/ive_made_a_cli_tool_to_integrates_with_notion/	Hello. Recently I discovered the P.A.R.A Method, which is a system to organize your life. I decided to give it a try and structured the system on my Notion. But before that i used another app to keep track of my tasks which was really simple to use and I didn't had to have constantly the app open on my computer, and if a did leave open the app do not use as much memory as notion. So I came with the idea of using a CLI tool to integrate's with my system. Here is the link to the project [https://github.com/eoBattisti/proj-notionhubpy](https://github.com/eoBattisti/proj-notionhubpy) . I wanted to share this project not only for showcasing, but also to people use it and modify it by their own ideas. All feedbacks, questions and suggestions are welcome, i'd love to disscuss and improve this project. &#x200B; Note: I'll constantly add more features to the project as i'll use the CLI	0.0	t3_18hg2rr	reddit		
328	Are there any PEPs about type imports?	Unknown	2023-12-12 18:31:00	https://www.reddit.com/r/Python/comments/18gsr42/are_there_any_peps_about_type_imports/	"If you use type hinting in your python code, you probably use this pattern a lot: ``` from __future__ import annotations from typing import TYPE_CHECKING if TYPE_CHECKING: # pylint: disable = ungrouped-imports from module_z import A def foo(p0:A): ... ``` I know I use it a lot - literally every one of my python files. And it kind of sucks. It could be much more succinct: ``` from module_z import type A def foo(p0:A): ... ``` PEP 563 and 649 standardization pending. It was supposed to be mandatory in 3.10, yet here we are, two versions later and still TBD. Anyway, lately cpython developers seems to love copying stuff from Typescript, so how about [doing it once more](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-8.html#type-only-imports-and-export), since at least this time it makes sense? The syntax is already a given: In Typescript: import type { SomeThing } from ""./some-module.js""; In Python: from module_z import type A import type module_z.A This would definitely be a lot more useful than a walrus operator."	10.0	t3_18gsr42	reddit		
329	My first open-source python package: please star/share :)	Unknown	2023-12-13 16:17:15	https://www.reddit.com/r/Python/comments/18hkei7/my_first_opensource_python_package_please/	Hi, everyone! I‚Äôve just released my first open-source python package focused on making it easier to predict biological age. I‚Äôm looking for feedback and critiques to improve it! Link to GitHub: https://github.com/rsinghlab/pyaging	1.0	t3_18hkei7	reddit		
330	FastStream adds support for Redis	Unknown	2023-12-12 19:13:42	https://www.reddit.com/r/Python/comments/18gtrv0/faststream_adds_support_for_redis/	[FastStream](https://github.com/airtai/faststream) is a **free open-source** framework for building **asynchronous services** interacting with event streams such as [**Apache Kafka**](https://kafka.apache.org/), [**RabbitMQ**](https://www.rabbitmq.com/), [**NATS**](https://nats.io/) and, since the latest 0.3 release, [**Redis**](https://redis.io/). FastStream simplifies writing producers and consumers for message queues, automatically handling all the parsing, networking and documentation generation. It enables you to write your service once and run it on every streaming protocol with minimal change in setup code. Please check it out and let us know how to improve it: [https://github.com/airtai/faststream](https://github.com/airtai/faststream)	3.0	t3_18gtrv0	reddit		
331	Perform Proper Financial Analysis with Python through the Finance Toolkit	Unknown	2023-12-12 15:12:44	https://www.reddit.com/r/Python/comments/18go3iw/perform_proper_financial_analysis_with_python/	A little over 4 months ago I've shared a project that I've been working on for quite some time titled the [**Finance Toolkit**](https://github.com/JerBouma/FinanceToolkit). The purpose of this project was to write down as many formulas as used in Finance in the most transparent and simple way to prevent the thing I see so often, the same model, Excel spreadsheet or discussion being done again. This has led to over 150+ different ratios, technical indicators, performance metrics, risk metrics, economic indicators and more written down in a very simplistic fashion ([proof](https://github.com/JerBouma/FinanceToolkit/blob/main/financetoolkit/ratios/valuation_model.py)) while letting any kind of data be fed directly into the Toolkit through using the MVC architecture. See what I mean in the GIF below that demonstrates some of the data that can be obtained. Take a look at the repository here: [https://github.com/JerBouma/FinanceToolkit/](https://github.com/JerBouma/FinanceToolkit/) https://i.redd.it/w394wqszov5c1.gif Since this GIF I've made a lot of new improvements which are both technical and finance orientated. First and foremost, I've had multiple requests to improve the speed of data collection. I've therefore experimented with threading to speed up the data collection which you can see gets you financial statements of 345 companies within 40 seconds (and any ratio collection is then done within a couple seconds). https://preview.redd.it/6vimwe9epv5c1.png?width=2300&format=png&auto=webp&s=bb402b1f8dd07e89c6a1c7f4306118e3021ed220 I've also worked on integrating the Fama-and-French 5 Factor model which makes it possible to understand how each company inputted is influenced by each factor and how these factors correlate over time. Basically what this model does is it lets you understand how companies are influenced by key factors such as momentum, their tendency to perform R&D and more. This is quite an interesting topic as the correlations between factors fluctuate while being used quite a lot in Finance. https://i.redd.it/qnk8snfupv5c1.gif Next to that, I've added in many performance and risk metrics such as Jensen's Alpha, (Conditional) Value at Risk and the GARCH Model which was designed by another developer including many other Risk metrics (and he is not done yet he says!) Today, I've extended the Finance Toolkit with Key Economic Indicators, new additions that can really help in financial analysis to better understand the economical climate of a country (with support for 60+ countries). For example the Unemployment Rates: https://preview.redd.it/9ns15nnfqv5c1.png?width=630&format=png&auto=webp&s=92b7bae7ee46c3fb2e8594d31dcc62ca42ab6625 You can find a complete list of all the metrics I currently have [**here**](https://github.com/JerBouma/FinanceToolkit#available-metrics). Oh and it is good to note that all of this is **FREE**. I do all of this because it's my hobby, I enjoy thinking about these calculations and just programming in general. It will always remain a hobby since I enjoy my full-time job in the financial sector just as much!	1.0	t3_18go3iw	reddit		
332	Python 3D simulation library	Unknown	2023-12-12 07:27:42	https://www.reddit.com/r/Python/comments/18gggvi/python_3d_simulation_library/	Hi all, Just wondering if there is any 3d physics simulation library that i can use, im planning to simulate the drainage plane of some structures using some particles, similar to a pool table with balls if the table was inclined in a particular direction. I have seen pymunk but it looks like it doesnt work with 3d objects.. Thanks in advance Alwx	11.0	t3_18gggvi	reddit		
333	I've added WhisperX support to my Audiotext program to transcribe audio from multiple sources	Unknown	2023-12-12 17:30:41	https://www.reddit.com/r/Python/comments/18grby6/ive_added_whisperx_support_to_my_audiotext/	&#x200B; [Main screen](https://preview.redd.it/llnky6uufw5c1.png?width=1506&format=png&auto=webp&s=8fec922a553f87af63cc59a828f1f1c8e99e3a0d) # What's Audiotext? It's a program that transcribes the audio from an audio file, video file, or microphone input into one of the 74 different languages it supports, along with some of their dialects. You can transcribe using the [**Google Speech-to-Text API**](https://cloud.google.com/speech-to-text) or [**WhisperX**](https://github.com/m-bain/whisperX) , which can even translate the transcription or generate subtitles! You can also choose the theme you like best. It can be dark, light, or the one configured in the system. # What have you changed in it? Originally, **Audiotext** only supported transcriptions via the Google API, which mostly gave poor results and was limited by the 60 minutes per month usage limit offered by the free tier. Although you can add your API key to the program, I found this very limiting and costly, so I had a try at implementing the WhisperX package into the program to support fast, unlimited audio transcription that doesn't depend on a remote service. The results were better than expected, as I was able to transcribe and subtitle movies in a matter of a few minutes (depending on the hardware of your system!). It also supports translation, which along with the features above, makes this method by far the best when compared to the Google API. # What have you used to make it? The GUI is built using CustomTkinter. Apart from the aforementioned WhisperX package, it uses the SpeechRecognition package to use the Google API transcription method and get the audio from the microphone, among some other packages credited in the [GitHub repo](https://github.com/HenestrosaDev/audiotext) where you can get more insight into the project. Any and all suggestions are most welcome! :)	0.0	t3_18grby6	reddit		
334	Build a Python music player using libVLC	Unknown	2023-12-12 14:53:19	https://www.reddit.com/r/Python/comments/18gnnl8/build_a_python_music_player_using_libvlc/	I wrote a C program that dynamically loads the `libvlc` shared libraries, exposes some functions to create a player object, load music files, and control playback. I then wrote a Cython wrapper to the C library as a Python extension. Then a jupyter notebook to load and control playback music. The music file can come from local storage, any URL, or even a bytestream in memory that we construct. There isn't a source code posted yet as this is part of a bigger project. In this video I walkthrough the steps to implement a new feature into the player and you will understand how the layers interact. ### Why did I use dlopen and not link at compile time? So that the module can be built and installed even if VLC is not installed. When it is, the library will use VLC. Also, we might choose to implement a different backend than VLC. ### Why did I not use python-vlc package to implement the application? The VLC player can be built using the existing python-vlc package. I wanted to showcase the utility of using Cython to call into a C library. You can take this knowledge and apply it to any other C library, or write your own performant C library and call it from Python. **Watch it here:** [Cython VLC player tutorial video](https://youtu.be/c8tRMHWfGCQ?si=1eVsF7jEEmWNAr7c) I will be happy to answer any of your questions	0.0	t3_18gnnl8	reddit		
335	T Rex runner game in python and Pygame	Unknown	2023-12-12 16:58:25	https://www.reddit.com/r/Python/comments/18gqjqg/t_rex_runner_game_in_python_and_pygame/	I made a T Rex runner game using Python and Pygame Code: [https://github.com/DataWizual/T-Rex-Runner](https://github.com/DataWizual/T-Rex-Runner) Here's the video explaining how I did it: [https://www.youtube.com/watch?v=pZySIXSelCA](https://www.youtube.com/watch?v=pZySIXSelCA)	0.0	t3_18gqjqg	reddit		
336	Wednesday Daily Thread: Beginner questions	Unknown	2023-12-13 00:00:08	https://www.reddit.com/r/Python/comments/18h1nd0/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	1.0	t3_18h1nd0	reddit		
337	Visualization Libraries for Polars	Unknown	2023-12-12 03:17:25	https://www.reddit.com/r/Python/comments/18gc8rn/visualization_libraries_for_polars/	Hey folks, I‚Äôm looking for a library that that easily plot directly from polars library. I know I can convert to_pandas() and plot using matplotlib or seaborn but was wondering if you know any libraries that can plot directly. If you know upcoming releases of libraries which will, feel free to share as well. Thanks!	2.0	t3_18gc8rn	reddit		
338	Introducing Code Prompt Assist: A Revolutionary VS Code Extension for Developers!	Unknown	2023-12-12 18:37:30	https://www.reddit.com/r/Python/comments/18gswkq/introducing_code_prompt_assist_a_revolutionary_vs/	Hi r/Python community! I'm excited to share a tool I've been working on that I believe can significantly enhance your coding experience in VS Code. It's called **Code Prompt Assist**, and it's designed to streamline your workflow, improve code security, and boost your productivity. Here's a quick rundown of what it offers: 1. **Effortless Prompt Redaction**: Automatically redact sensitive information like API keys from your code, enhancing security and compliance. 2. **Effortless Code Solutions**: Generate code solutions automatically based on your file paths and ticket information, saving you time and effort. 3. **Automated Unit Test Generation**: Create comprehensive unit tests through thorough code analysis, improving reliability and reducing errors. 4. **Intelligent Code Optimizations**: Receive suggestions for performance enhancements and best coding practices, tailored to your unique codebase. I developed Code Prompt Assist to address common challenges we face as developers. It's still in its MVP phase, and I'm looking for feedback to make it even better. **I'd love for you to try it out and share your thoughts:** * [https://marketplace.visualstudio.com/items?itemName=NickolasTheodoulou.code-prompt-assist](https://marketplace.visualstudio.com/items?itemName=NickolasTheodoulou.code-prompt-assist) Your feedback will be invaluable in shaping the future of this tool. Let's make coding in VS Code even more powerful and efficient together! Thanks for your time, and I can't wait to hear what you think!	2.0	t3_18gswkq	reddit		
339	Introducing UniDep: Unified Conda and Pip Dependency Management	Unknown	2023-12-12 04:43:10	https://www.reddit.com/r/Python/comments/18gdt83/introducing_unidep_unified_conda_and_pip/	Hello r/python! I'm excited to share a tool I've been working on called [**UniDep**](https://github.com/basnijholt/unidep) - a tool designed to streamline Python project dependency management by unifying Conda and Pip dependencies in a single `requirements.yaml` file. This approach allows for the creation of a unified Conda `environment.yaml`, while also integrating seamlessly with `setup.py` or `pyproject.toml`. **Key Features of UniDep:** - üîó **Unified Management**: Manage Conda and Pip dependencies together in a single file. - ‚öôÔ∏è **Project Tool Integration**: Seamless integration with `pyproject.toml` and `setup.py`. - üè¢ **Monorepo Support**: Merge multiple `requirements.yaml` files into one Conda environment `environment.yaml`. - üåç **Platform-Specific Support**: Specify dependencies for different operating systems or architectures. - üõ†Ô∏è **Conflict Resolution**: Automated version conflict resolution for complex dependency chains. - üîÑ **`unidep install` CLI**: A single command to install all Conda, Pip, and local dependencies. - üîß **`pip-compile` Integration**: Generate fully pinned `requirements.txt` files from `requirements.yaml`. **Getting Started:** - Installation: Easily install UniDep via pip (`pip install unidep`) or Conda (`conda install -c conda-forge unidep`). - [GitHub Repo](https://github.com/basnijholt/unidep): Check out the code, examples, and detailed documentation. **Example:** ```yaml name: example_environment channels: - conda-forge dependencies: - numpy # same name on conda and pip - conda: python-graphviz # When names differ between Conda and Pip pip: graphviz - pip: slurm-usage # pip-only - conda: mumps # conda-only # Use platform selectors; below only on linux64 - conda: cuda-toolkit # [linux64] platforms: # (Optional) specify platforms that are supported (like conda-lock) - linux-64 - osx-arm64 includes: - ../other-project-using-unidep # include other projects that use unidep - ../common-requirements.yaml # include other requirements.yaml files ``` **Why UniDep?** Managing dependencies can be a challenge, especially in larger projects or monorepos. Maintaining a global `environment.yml`, sub `environment.yml` files for subprojects, and `requirements.txt` per project is a hassle and keeping all files in sync is a pain. UniDep was born out of the need to manage this complexity more effectively. I would love for the community to try it out, contribute, and share feedback. Your insights and contributions are invaluable in making UniDep even better! Looking forward to your thoughts!	0.0	t3_18gdt83	reddit		
340	Blog: Ibis + DuckDB geospatial: a match made on Earth	Unknown	2023-12-11 23:18:30	https://www.reddit.com/r/Python/comments/18g7dwi/blog_ibis_duckdb_geospatial_a_match_made_on_earth/	[Ibis](https://ibis-project.org/) now has support for DuckDB geospatial functions! I've been working on this and I wrote a blog about it. Check it out! [https://ibis-project.org/posts/ibis-duckdb-geospatial/](https://ibis-project.org/posts/ibis-duckdb-geospatial/)	1.0	t3_18g7dwi	reddit		
341	CLI tool & library for real-time monitoring/filtering of all page edits across wikipedia	Unknown	2023-12-12 07:35:43	https://www.reddit.com/r/Python/comments/18ggkxp/cli_tool_library_for_realtime_monitoringfiltering/	"This package is inspired by Tom Scott's ""WikiParliament"" project ([https://www.tomscott.com/wikiparliament/](https://www.tomscott.com/wikiparliament/)) &#x200B; Github URL: [https://github.com/eriknyquist/wikichangewatcher](https://github.com/eriknyquist/wikichangewatcher) &#x200B; This package allows real-time monitoring of all page edits across wikipedia, with filtering features that allow you to see (for example) only page edits made by specific IPv4/IPv6 address ranges, or by specific usernames. You can also filter based on the value of any field described in [https://www.mediawiki.org/wiki/Manual:RCFeed](https://www.mediawiki.org/wiki/Manual:RCFeed) &#x200B; I posted an earlier version of this package a while back: [https://www.reddit.com/r/Python/comments/1581yje/wikichangewatcher\_realtime\_monitoring\_of\_all/](https://www.reddit.com/r/Python/comments/1581yje/wikichangewatcher_realtime_monitoring_of_all/) &#x200B; Since then I've added unit tests, fixed several bugs, and added a CLI tool which provides a nice way to easily monitor wikipedia page edits without writing a python script."	0.0	t3_18ggkxp	reddit		
342	I made a basic python client and ORM for XTDB	Unknown	2023-12-11 19:33:17	https://www.reddit.com/r/Python/comments/18g1xmc/i_made_a_basic_python_client_and_orm_for_xtdb/	"[XTDB](https://xtdb.com/) is a *bitemporal and dynamic relational database for SQL and Datalog*, written in Clojure. The Python application I work on uses XTDB for its bitemporal and schema-less nature. There were a few Python clients that looked unmaintained and lacked some features we needed, so I tried to build something that would have fit our own requirements in hindsight. This includes: * A thin language layer around [edn](https://github.com/edn-format/edn)/datalog, the query language * A client to talk edn to [XTDB's HTTP API](https://v1-docs.xtdb.com/clients/http/) * A session object that does transaction management * A base class for creating an ORM * A Query object that lets you easily write edn queries using these ORM classes. The latter two aim to make using XTDB feel like using PostgreSQL with SQLAlchemy: >>> query = Query(TestEntity).where(TestEntity, name=""test"") >>> result = session.query(query) >>> >>> result[0].dict() # {""TestEntity/name"": ""test"", ""type"": ""TestEntity"", ""xt/id"": ""fe2a3ee0-9254-41dc-91cc-74ad9e2a16db""} Source code: [https://github.com/Donnype/xtdb-py](https://github.com/Donnype/xtdb-py)"	1.0	t3_18g1xmc	reddit		
343	Exploring the Cosmos in 3D: StarScholar3D & StellarPhysicsHub Astronomy Projects	Unknown	2023-12-11 18:38:33	https://www.reddit.com/r/Python/comments/18g0knz/exploring_the_cosmos_in_3d_starscholar3d/	Hi all, I'm excited to share with you two projects that I have created with the aim of educating about constellations and asterisms. # Background: The first project, [StarScholar3D](https://github.com/WDoyle123/StarScholar3D), forms the core of this work, serving as a foundational tool for 3D visualisation. This tool is integrated into my second project, [StellarPhysicsHub](https://github.com/WDoyle123/StellarPhysicsHub), enhancing the accessibility and engagement of astronomical visualisations for a broader audience. Recently I haven't had much luck in the job market for a Python developer role, so I have spent the past couple weeks developing projects for my portfolio. My background is in Physics so I thought it would make good sense to put that to use! # Project 1 [StarScholar3D](https://github.com/WDoyle123/StarScholar3D): [StarScholar3D](https://github.com/WDoyle123/StarScholar3D) is a dynamic 3D visualisation project focusing on stars, constellations, and asterisms, based on the Yale Bright Star catalogue. It transforms equatorial coordinates into Cartesian coordinates for 3D plotting. The plots can then be made into GIFs or used interactively. Key features include multiprocessing to enhance performance, data utilisation from reputable astronomical sources, interactive 3D visualisations, and GIF capture capabilities. It also employs the B-V colour index and V magnitude for representing how stars are seen from Earth. [Big Dipper](https://i.redd.it/ckyqs3w4ap5c1.gif) # Project 2 [StellarPhysicsHub](https://github.com/WDoyle123/StellarPhysicsHub): [StellarPhysicsHub](https://github.com/WDoyle123/StellarPhysicsHub) is a Flask-based web application designed for astronomy enthusiasts and researchers, offering an interactive platform to explore constellations and asterisms. The app uses Flask for backend development and SQLAlchemy for database interactions, backed by a SQLite database. It features a user-friendly interface with search functionality, dynamic content rendering using Jinja2, and employs [StarScholar3D](https://github.com/WDoyle123/StarScholar3D) for interactive 3D visualisations. The responsive design ensures accessibility across devices however it is best viewed on a PC as the zoom functionality struggles with touch controls. [www.stellarphysicshub.com](https://i.redd.it/3jxqzyfjap5c1.gif) [https://www.stellarphysicshub.com/](https://www.stellarphysicshub.com/) is a live website so you do not have to download the repository to test it! &#x200B; Thank you for reading all of that! I hope to hear back from you and welcome suggestions.	0.0	t3_18g0knz	reddit		
344	I made a library to solve Physics equations	Unknown	2023-12-11 04:13:49	https://www.reddit.com/r/Python/comments/18flzf5/i_made_a_library_to_solve_physics_equations/	PhysiPy is a Python Library that calculates all types of Physics Formulae for calculations and research. It consists of formulas from basic Kinematics to higher-order quantum mechanics. It is made to make equation-solving a lot faster. You can find examples in the GitHub. GitHub: [https://github.com/rohankishore/PhysiPy](https://github.com/rohankishore/PhysiPy)	12.0	t3_18flzf5	reddit		
345	Tuesday Daily Thread: Advanced questions	Unknown	2023-12-12 00:00:11	https://www.reddit.com/r/Python/comments/18g8a2m/tuesday_daily_thread_advanced_questions/	# Weekly Wednesday Thread: Advanced Questions üêç Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices. ## How it Works: 1. **Ask Away**: Post your advanced Python questions here. 2. **Expert Insights**: Get answers from experienced developers. 3. **Resource Pool**: Share or discover tutorials, articles, and tips. ## Guidelines: * This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday. * Questions that are not advanced may be removed and redirected to the appropriate thread. ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **How can you implement a custom memory allocator in Python?** 2. **What are the best practices for optimizing Cython code for heavy numerical computations?** 3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?** 4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?** 5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?** 6. **What are some advanced use-cases for Python's decorators?** 7. **How can you achieve real-time data streaming in Python with WebSockets?** 8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?** 9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?** 10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)** Let's deepen our Python knowledge together. Happy coding! üåü	0.0	t3_18g8a2m	reddit		
346	Squarify with matplotlib , nice combination to create heatmaps	Unknown	2023-12-11 08:53:37	https://www.reddit.com/r/Python/comments/18fq6z8/squarify_with_matplotlib_nice_combination_to/	&#x200B; https://preview.redd.it/1u8p2x9pqm5c1.png?width=2378&format=png&auto=webp&s=2642107c97b0dd663918b3aaaef4b69f29c8d10a	1.0	t3_18fq6z8	reddit		
347	I just created a LLM Terminal App, Clara, which does literally everything for you	Unknown	2023-12-12 13:43:12	https://www.reddit.com/r/Python/comments/18gm5ry/i_just_created_a_llm_terminal_app_clara_which/	Note: I doesn‚Äôt literally does everything, my bad for using that word, it can basically do everything you can achieve with a python script, creates and tries to run it ü§∑üèª I can't remember stuff or I usually work with lot of automation code, so I thought why not and created this app which does everything from putting you laptop to sleep to editing, browsing and getting stuffs done. Have a Look at: [https://github.com/badboysm890/Clara-Term](https://github.com/badboysm890/Clara-Term) Or Just Install using `pip install claraterm` &#x200B;	4.0	t3_18gm5ry	reddit		
348	SCRABBLE IN TERMINAL	Unknown	2023-12-10 18:25:49	https://www.reddit.com/r/Python/comments/18f9tb6/scrabble_in_terminal/	Hey everyone, this is my first serious python project. I (hope) it works in terminal after cloning it and running rework file. All other info is in **README** so make sure you check how to play it before you do. Hope you all like it! I'm planning to advance it and add some graphics. Any piece of advice would be appreciated! [Scrabble repository on github](https://github.com/phillosz/scrabble.git) In case you find any error or anything to improve you can fork it and make pull requests. &#x200B; [Scrabble](https://preview.redd.it/91f62p3ifi5c1.png?width=500&format=png&auto=webp&s=ed7e41706fc19072d3855149e300c6601aec940b)	16.0	t3_18f9tb6	reddit		
349	I made Arrest, a small utility to wrap your API calls from your Python application	Unknown	2023-12-10 08:08:06	https://www.reddit.com/r/Python/comments/18eys56/i_made_arrest_a_small_utility_to_wrap_your_api/	Hey guys! I'm super excited to announce [arrest](https://github.com/s-bose/arrest). It is a small library that you can use to define the structure of the REST Apis your Python application will be interacting with. it provides a straightforward way to call the different routes in your API and additionally enriches them with Pydantic classes for data validation. And it is also backward compatible with pydantic@v1.10.13. I would greatly appreciate your feedback and useful opinions/improvements on this. Here are the [docs](https://s-bose.github.io/arrest/) if you wanna check that out. Thanks a lot!	12.0	t3_18eys56	reddit		
350	Devito: Towards a generic Finite Difference DSL using Symbolic Python	Michael Lange	2016-09-12 12:15:36	http://arxiv.org/abs/1609.03361v1	Domain specific languages (DSL) have been used in a variety of fields to express complex scientific problems in a concise manner and provide automated performance optimization for a range of computational architectures. As such DSLs provide a powerful mechanism to speed up scientific Python computation that goes beyond traditional vectorization and pre-compilation approaches, while allowing domain scientists to build applications within the comforts of the Python software ecosystem. In this paper we present Devito, a new finite difference DSL that provides optimized stencil computation from high-level problem specifications based on symbolic Python expressions. We demonstrate Devito's symbolic API and performance advantages over traditional Python acceleration methods before highlighting its use in the scientific context of seismic inversion problems.			arxiv	['Navjot Kukreja', 'Mathias Louboutin', 'Fabio Luporini', 'Felippe Vieira', 'Vincenzo Pandolfo', 'Paulius Velesko', 'Paulius Kazakas', 'Gerard Gorman']	100.0
351	Transformation of Python Applications into Function-as-a-Service Deployments	Josef Spillner	2017-05-23 10:43:53	http://arxiv.org/abs/1705.08169v1	New cloud programming and deployment models pose challenges to software application engineers who are looking, often in vain, for tools to automate any necessary code adaptation and transformation. Function-as-a-Service interfaces are particular non-trivial targets when considering that most cloud applications are implemented in non-functional languages. Among the most widely used of these languages is Python. This starting position calls for an automated approach to transform monolithic Python code into modular FaaS units by partially automated decomposition. Hence, this paper introduces and evaluates Lambada, a Python module to dynamically decompose, convert and deploy unmodified Python code into AWS Lambda functions. Beyond the tooling in the form of a measured open source prototype implementation, the paper contributes a description of the algorithms and code rewriting rules as blueprints for transformations of other scripting languages.			arxiv	[]	101.0
352	Use of Python programming language in astronomy and science	Daniel M. Faes	2018-07-12 20:09:02	http://arxiv.org/abs/1807.04806v1	The use of Python is noticeably growing among the scientific community, and Astronomy is not an exception. The power of Python consists of being an extremely versatile high-level language, easy to program that combines both traditional programming and data reduction and analysis tools. Here I make a brief introduction to Python, mentioning a few programming practices implemented in the language and some of its useful features on the process of data manipulation. I cover in a little more detail the standard scientific libraries (NumPy and SciPy) for data handling, the graphical library (Matplotlib), and tools for specific use in astronomy (PyFITS and PyRAF). Good programming practices and how they are implemented at the language are also viewed. Python resources and references are mentioned through- out the text for those who wish to go deeper and make use of the power of the language.			arxiv	[]	102.0
353	NL4Py: Agent-Based Modeling in Python with Parallelizable NetLogo Workspaces	Chathika Gunaratne	2018-08-09 18:21:55	http://arxiv.org/abs/1808.03292v5	External control of agent-based models is vital for complex adaptive systems research. Often these experiments require vast numbers of simulation runs and are computationally expensive. NetLogo is the language of choice for most agent-based modelers but lacks direct API access through Python. NL4Py is a Python package for the parallel execution of NetLogo simulations via Python, designed for speed, scalability, and simplicity of use. NL4Py provides access to the large number of open-source machine learning and analytics libraries of Python and enables convenient and efficient parallelization of NetLogo simulations with minimal coding expertise by domain scientists.			arxiv	['Ivan Garibay']	103.0
354	LensKit for Python: Next-Generation Software for Recommender System Experiments	Michael D. Ekstrand	2018-09-10 04:15:05	http://arxiv.org/abs/1809.03125v4	LensKit is an open-source toolkit for building, researching, and learning about recommender systems. First released in 2010 as a Java framework, it has supported diverse published research, small-scale production deployments, and education in both MOOC and traditional classroom settings. In this paper, I present the next generation of the LensKit project, re-envisioning the original tool's objectives as flexible Python package for supporting recommender systems research and development. LensKit for Python (LKPY) enables researchers and students to build robust, flexible, and reproducible experiments that make use of the large and growing PyData and Scientific Python ecosystem, including scikit-learn, TensorFlow, and PyTorch. To that end, it provides classical collaborative filtering implementations, recommender system evaluation metrics, data preparation routines, and tools for efficiently batch running recommendation algorithms, all usable in any combination with each other or with other Python software. This paper describes the design goals, use cases, and capabilities of LKPY, contextualized in a reflection on the successes and failures of the original LensKit for Java software.			arxiv	[]	104.0
355	Tangent: Automatic Differentiation Using Source Code Transformation in Python	Bart van Merri√´nboer	2017-11-07 20:15:24	http://arxiv.org/abs/1711.02712v1	Automatic differentiation (AD) is an essential primitive for machine learning programming systems. Tangent is a new library that performs AD using source code transformation (SCT) in Python. It takes numeric functions written in a syntactic subset of Python and NumPy as input, and generates new Python functions which calculate a derivative. This approach to automatic differentiation is different from existing packages popular in machine learning, such as TensorFlow and Autograd. Advantages are that Tangent generates gradient code in Python which is readable by the user, easy to understand and debug, and has no runtime overhead. Tangent also introduces abstractions for easily injecting logic into the generated gradient code, further improving usability.			arxiv	['Alexander B. Wiltschko', 'Dan Moldovan']	105.0
356	Optimizing and Evaluating Transient Gradual Typing	Michael M. Vitousek	2019-02-20 23:20:15	http://arxiv.org/abs/1902.07808v1	Gradual typing enables programmers to combine static and dynamic typing in the same language. However, ensuring a sound interaction between the static and dynamic parts can incur significant runtime cost. In this paper, we perform a detailed performance analysis of the transient gradual typing approach implemented in Reticulated Python, a gradually typed variant of Python. The transient approach inserts lightweight checks throughout a program rather than installing proxies on higher order values. We show that, when running Reticulated Python and the transient approach on CPython, performance decreases as programs evolve from dynamic to static types, up to a 6x slowdown compared to equivalent Python programs. To reduce this overhead, we design a static analysis and optimization that removes redundant runtime checks. The optimization employs a static type inference algorithm that solves traditional subtyping constraints and also a new kind of check constraint. We evaluate the resulting performance and find that for many programs, the efficiency of partially typed programs is close to their untyped counterparts, removing most of the slowdown of transient checks. Finally, we measure the efficiency of Reticulated Python programs when running on PyPy, a tracing JIT. We find that combining PyPy with our type inference algorithm reduces the overall overhead to zero.			arxiv	['Jeremy G. Siek', 'Avik Chaudhuri']	106.0
357	PyNetMet: Python tools for efficient work with networks and metabolic models	D. Gamermann	2012-11-30 09:25:06	http://arxiv.org/abs/1211.7196v1	Background: The study of genome-scale metabolic models and their underlying networks is one of the most important fields in systems biology. The complexity of these models and their description makes the use of computational tools an essential element in their research. Therefore there is a strong need of efficient and versatile computational tools for the research in this area. Results: In this manuscript we present PyNetMet, a Python library of tools to work with networks and metabolic models. These are open-source free tools for use in a Python platform, which adds considerably versatility to them when compared with their desktop software similars. On the other hand these tools allow one to work with different standards of metabolic models (OptGene and SBML) and the fact that they are programmed in Python opens the possibility of efficient integration with any other already existing Python tool. Conclusions: PyNetMet is, therefore, a collection of computational tools that will facilitate the research work with metabolic models and networks.			arxiv	['A. Montagud', 'R. A. Jaime Infante', 'J. Triana', 'P. F. de C√≥rdoba', 'J. F. Urchuegu√≠a']	107.0
358	zfit: scalable pythonic fitting	Jonas Eschle	2019-10-29 17:45:12	http://arxiv.org/abs/1910.13429v2	Statistical modeling is a key element in many scientific fields and especially in High-Energy Physics (HEP) analysis. The standard framework to perform this task in HEP is the C++ ROOT/RooFit toolkit; with Python bindings that are only loosely integrated into the scientific Python ecosystem. In this paper, zfit, a new alternative to RooFit written in pure Python, is presented. Most of all, zfit provides a well defined high-level API and workflow for advanced model building and fitting, together with an implementation on top of TensorFlow, allowing a transparent usage of CPUs and GPUs. It is designed to be extendable in a very simple fashion, allowing the usage of cutting-edge developments from the scientific Python ecosystem in a transparent way. The main features of zfit are introduced, and its extension to data analysis, especially in the context of HEP experiments, is discussed.			arxiv	['Albert Puig Navarro', 'Rafael Silva Coutinho', 'Nicola Serra']	108.0
359	Network visualizations with Pyvis and VisJS	Giancarlo Perrone	2020-06-02 17:32:32	http://arxiv.org/abs/2006.04951v1	Pyvis is a Python module that enables visualizing and interactively manipulating network graphs in the Jupyter notebook, or as a standalone web application. Pyvis is built on top of the powerful and mature VisJS JavaScript library, which allows for fast and responsive interactions while also abstracting away the low-level JavaScript and HTML. This means that elements of the rendered graph visualization, such as node/edge attributes can be specified within Python and shipped to the JavaScript layer for VisJS to render. This declarative approach makes it easy to quickly explore graph visualizations and investigate data relationships. In addition, Pyvis is highly customizable so that colors, sizes, and hover tooltips can be assigned to the rendered graph. The network graph layout is controlled by a front-end physics engine that is configurable from a Python interface, allowing for the detailed placement of the graph elements. In this paper, we outline use cases for Pyvis with specific examples to highlight key features for any analysis workflow. A brief overview of Pyvis' implementation describes how the Python front-end binding uses simple Pyvis calls.			arxiv	['Jose Unpingco', 'Haw-minn Lu']	109.0
360	Extendible and Efficient Python Framework for Solving Evolution Equations with Stabilized Discontinuous Galerkin Method	Andreas Dedner	2020-09-25 16:23:57	http://arxiv.org/abs/2009.13416v2	This paper discusses a Python interface for the recently published DUNE-FEM-DG module which provides highly efficient implementations of the Discontinuous Galerkin (DG) method for solving a wide range of non linear partial differential equations (PDE). Although the C++ interfaces of DUNE-FEM-DG are highly flexible and customizable, a solid knowledge of C++ is necessary to make use of this powerful tool. With this work easier user interfaces based on Python and the Unified Form Language are provided to open DUNE-FEM-DG for a broader audience. The Python interfaces are demonstrated for both parabolic and first order hyperbolic PDEs.			arxiv	['Robert Kl√∂fkorn']	110.0
361	An Executable Structural Operational Formal Semantics for Python	Maximilian A. K√∂hl	2021-09-07 15:11:23	http://arxiv.org/abs/2109.03139v1	Python is a popular high-level general-purpose programming language also heavily used by the scientific community. It supports a variety of different programming paradigms and is preferred by many for its ease of use. With the vision of harvesting static analysis techniques like abstract interpretation for Python, we develop a formal semantics for Python. A formal semantics is an important cornerstone for any sound static analysis technique. We base our efforts on the general framework of structural operational semantics yielding a small-step semantics in principle allowing for concurrency and interaction with an environment. The main contributions of this thesis are twofold: first, we develop a meta-theoretic framework for the formalization of structural operational semantics in tandem with the necessary tool support for the automated derivation of interpreters from such formal semantics, and, second, we validate the suitability of this approach for the formalization of modern programming languages developing a semantics for Python.			arxiv	[]	111.0
362	Social Networks as a Collective Intelligence: An Examination of the Python Ecosystem	Thomas Pike	2022-01-16 13:02:19	http://arxiv.org/abs/2201.06040v1	The Python ecosystem represents a global, data rich, technology-enabled network. By analyzing Python's dependency network, its top 14 most imported libraries and cPython (or core Python) libraries, this research finds clear evidence the Python network can be considered a problem solving network. Analysis of the contributor network of the top 14 libraries and cPython reveals emergent specialization, where experts of specific libraries are isolated and focused while other experts link these critical libraries together, optimizing both local and global information exchange efficiency. As these networks are expanded, the local efficiency drops while the density increases, representing a possible transition point between exploitation (optimizing working solutions) and exploration (finding new solutions). These results provide insight into the optimal functioning of technology-enabled social networks and may have larger implications for the effective functioning of modern organizations.			arxiv	['Robert Colter', 'Mark Bailey', 'Jackie Kazil', 'John Speed Meyers']	112.0
363	PyMigBench and PyMigTax: A Benchmark and Taxonomy for Python Library Migration	Mohayeminul Islam	2022-07-03 21:00:08	http://arxiv.org/abs/2207.01124v1	Developers heavily rely on Application Programming Interfaces (APIs) from libraries to build their projects. However, libraries might become obsolete, or new libraries with better APIs might become available. In such cases, developers need to replace the used libraries with alternative libraries, a process referred to as library migration. When done manually, library migration can be tedious, time-consuming, and error-prone. Most of the current research on automated library migration techniques focus on Java libraries, and even more so on version migrations of the same library. Despite the increasing popularity of Python, limited research work has investigated migration between Python libraries. In this paper, we investigate the nature of Python library migrations in open-source systems. We analyze the code changes that happen during library migration and build PyMigBench, a manually verified migration benchmark. PyMigBench contains 436 migration-related code changes from 74 commits in 57 client repositories, and includes migrations between 34 unique pairs of libraries. Additionally, we manually analyze the migration-related code changes and create a taxonomy of migrations, PyMigTax, that categorizes migrations across various dimensions. Our contributions provide the necessary foundations for developing automated Python library migration tools and techniques.			arxiv	['Ajay Kumar Jha', 'Sarah Nadi']	113.0
364	Modelling the Turtle Python library in CSP	Dara MacConville	2022-07-20 07:24:36	http://arxiv.org/abs/2207.09706v1	Software verification is an important tool in establishing the reliability of critical systems. One potential area of application is in the field of robotics, as robots take on more tasks in both day-to-day areas and highly specialised domains. Robots are usually given a plan to follow, if there are errors in this plan the robot will not perform reliably. The capability to check plans for errors in advance could prevent this. Python is a popular programming language in the robotics domain, through the use of the Robot Operating System (ROS) and various other libraries. Python's Turtle package provides a mobile agent, which we formally model here using Communicating Sequential Processes (CSP). Our interactive toolchain CSP2Turtle with CSP model and Python components, enables Turtle plans to be verified in CSP before being executed in Python. This means that certain classes of errors can be avoided, and provides a starting point for more detailed verification of Turtle programs and more complex robotic systems. We illustrate our approach with examples of robot navigation and obstacle avoidance in a 2D grid-world.			arxiv	['Marie Farrell', 'Matt Luckcuck', 'Rosemary Monahan']	114.0
365	SIGWfast: a python package for the computation of scalar-induced gravitational wave spectra	Lukas T. Witkowski	2022-09-12 15:00:12	http://arxiv.org/abs/2209.05296v1	SIGWfast is a python code to compute the scalar-induced gravitational wave spectrum from a primordial scalar power spectrum that can be given in analytical or numerical form. SIGWfast was written with the aim of being easy to install and use, and to produce results fast, typically in a matter of a few seconds. To this end the code employs vectorization techniques within python, but there is also the option to compile a C++ module to perform the relevant integrations, further accelerating the computation. The python-only version should run on all platforms that support python 3. The version employing the C++ module is only available for Linux and MacOS systems.			arxiv	[]	115.0
366	Serenity: Library Based Python Code Analysis for Code Completion and Automated Machine Learning	Wenting Zhao	2023-01-05 02:09:08	http://arxiv.org/abs/2301.05108v1	Dynamically typed languages such as Python have become very popular. Among other strengths, Python's dynamic nature and its straightforward linking to native code have made it the de-facto language for many research areas such as Artificial Intelligence. This flexibility, however, makes static analysis very hard. While creating a sound, or a soundy, analysis for Python remains an open problem, we present in this work Serenity, a framework for static analysis of Python that turns out to be sufficient for some tasks. The Serenity framework exploits two basic mechanisms: (a) reliance on dynamic dispatch at the core of language translation, and (b) extreme abstraction of libraries, to generate an abstraction of the code. We demonstrate the efficiency and usefulness of Serenity's analysis in two applications: code completion and automated machine learning. In these two applications, we demonstrate that such analysis has a strong signal, and can be leveraged to establish state-of-the-art performance, comparable to neural models and dynamic analysis respectively.			arxiv	['Ibrahim Abdelaziz', 'Julian Dolby', 'Kavitha Srinivas', 'Mossad Helali', 'Essam Mansour']	116.0
367	Transactional Python for Durable Machine Learning: Vision, Challenges, and Feasibility	Supawit Chockchowwat	2023-05-15 16:27:09	http://arxiv.org/abs/2305.08770v1	In machine learning (ML), Python serves as a convenient abstraction for working with key libraries such as PyTorch, scikit-learn, and others. Unlike DBMS, however, Python applications may lose important data, such as trained models and extracted features, due to machine failures or human errors, leading to a waste of time and resources. Specifically, they lack four essential properties that could make ML more reliable and user-friendly -- durability, atomicity, replicability, and time-versioning (DART). This paper presents our vision of Transactional Python that provides DART without any code modifications to user programs or the Python kernel, by non-intrusively monitoring application states at the object level and determining a minimal amount of information sufficient to reconstruct a whole application. Our evaluation of a proof-of-concept implementation with public PyTorch and scikit-learn applications shows that DART can be offered with overheads ranging 1.5%--15.6%.			arxiv	['Zhaoheng Li', 'Yongjoo Park']	117.0
368	pytest-inline: An Inline Testing Tool for Python	Yu Liu	2023-05-22 20:58:44	http://arxiv.org/abs/2305.13486v1	We present pytest-inline, the first inline testing framework for Python. We recently proposed inline tests to make it easier to test individual program statements. But, there is no framework-level support for developers to write inline tests in Python. To fill this gap, we design and implement pytest-inline as a plugin for pytest, the most popular Python testing framework. Using pytest-inline, a developer can write an inline test by assigning test inputs to variables in a target statement and specifying the expected test output. Then, pytest-inline runs each inline test and fails if the target statement's output does not match the expected output. In this paper, we describe our design of pytest-inline, the testing features that it provides, and the intended use cases. Our evaluation on inline tests that we wrote for 80 target statements from 31 open-source Python projects shows that using pytest-inline incurs negligible overhead, at 0.012x. pytest-inline is integrated into the pytest-dev organization, and a video demo is at https://www.youtube.com/watch?v=pZgiAxR_uJg.			arxiv	['Zachary Thurston', 'Alan Han', 'Pengyu Nie', 'Milos Gligoric', 'Owolabi Legunsen']	118.0
369	Taming the Panda with Python: A Powerful Duo for Seamless Robotics Programming and Integration	Jean Elsner	2023-07-14 21:11:28	http://arxiv.org/abs/2307.07633v1	Franka Emika robots have gained significant popularity in research and education due to their exceptional versatility and advanced capabilities. This work introduces panda-py - a Python interface and framework designed to empower Franka Emika robotics with accessible and efficient programming. The panda-py interface enhances the usability of Franka Emika robots, enabling researchers and educators to interact with them more effectively. By leveraging Python's simplicity and readability, users can quickly grasp the necessary programming concepts for robot control and manipulation. Moreover, integrating panda-py with other widely used Python packages in domains such as computer vision and machine learning amplifies the robot's capabilities. Researchers can seamlessly leverage the vast ecosystem of Python libraries, thereby enabling advanced perception, decision-making, and control functionalities. This compatibility facilitates the efficient development of sophisticated robotic applications, integrating state-of-the-art techniques from diverse domains without the added complexity of ROS.			arxiv	[]	119.0
370	Natlog: Embedding Logic Programming into the Python Deep-Learning Ecosystem	Paul Tarau	2023-08-30 09:05:13	http://arxiv.org/abs/2308.15890v1	Driven by expressiveness commonalities of Python and our Python-based embedded logic-based language Natlog, we design high-level interaction patterns between equivalent language constructs and data types on the two sides. By directly connecting generators and backtracking, nested tuples and terms, coroutines and first-class logic engines, reflection and meta-interpretation, we enable logic-based language constructs to access the full power of the Python ecosystem. We show the effectiveness of our design via Natlog apps working as orchestrators for JAX and Pytorch pipelines and as DCG-driven GPT3 and DALL.E prompt generators. Keyphrases: embedding of logic programming in the Python ecosystem, high-level inter-paradigm data exchanges, coroutining with logic engines, logic-based neuro-symbolic computing, logic grammars as prompt-generators for Large Language Models, logic-based neural network configuration and training.			arxiv	[]	120.0
371	Awkward Just-In-Time (JIT) Compilation: A Developer's Experience	Ianna Osborne	2023-10-02 13:52:22	http://arxiv.org/abs/2310.01461v1	Awkward Array is a library for performing NumPy-like computations on nested, variable-sized data, enabling array-oriented programming on arbitrary data structures in Python. However, imperative (procedural) solutions can sometimes be easier to write or faster to run. Performant imperative programming requires compilation; JIT-compilation makes it convenient to compile in an interactive Python environment. Various functions in Awkward Arrays JIT-compile a user's code into executable machine code. They use several different techniques, but reuse parts of each others' implementations. We discuss the techniques used to achieve the Awkward Arrays acceleration with JIT-compilation, focusing on RDataFrame, cppyy, and Numba, particularly Numba on GPUs: conversions of Awkward Arrays to and from RDataFrame; standalone cppyy; passing Awkward Arrays to and from Python functions compiled by Numba; passing Awkward Arrays to Python functions compiled for GPUs by Numba; and header-only libraries for populating Awkward Arrays from C++ without any Python dependencies.			arxiv	['Jim Pivarski', 'Ioana Ifrim', 'Angus Hollands', 'Henry Schreiner']	121.0
372	Accelerating Pythonic coupled cluster implementations: a comparison between CPUs and GPUs	Maximilian H. Kriebel	2023-10-06 19:55:11	http://arxiv.org/abs/2310.04559v1	We scrutinize how to accelerate the bottleneck operations of Pythonic coupled cluster implementations performed on a \texttt{NVIDIA} Tesla V100S PCIe 32GB (rev 1a) Graphics Processing Unit (GPU). The \texttt{NVIDIA} Compute Unified Device Architecture (CUDA) API is interacted with via \texttt{CuPy}, an open-source library for Python, designed as a \texttt{NumPy} drop-in replacement for GPUs. The implementation uses the Cholesky linear algebra domain and is done in {PyBEST}, the Pythonic Black-box Electronic Structure Tool -- a fully-fledged modern electronic structure software package. Due to the limitations of Video Memory (VRAM), the GPU calculations must be performed batch-wise. Timing results of some contractions containing large tensors are presented. The \texttt{CuPy} implementation leads to factor 10 speed-up compared to calculations on 36 CPUs. Furthermore, we benchmark several Pythonic routines for time and memory requirements to identify the optimal choice of the tensor contraction operations available. Finally, we compare an example CCSD and pCCD-LCCSD calculation performed solely on CPUs to their CPU--GPU hybrid implementation. Our results indicate a significant speed-up (up to a factor of 16 regarding the bottleneck operations) when offloading specific contractions to the GPU using \texttt{CuPy}.			arxiv	['Pawe≈Ç Tecmer', 'Marta Ga≈Çy≈Ñska', 'Aleksandra Leszczyk', 'Katharina Boguslawski']	122.0
373	Python Unleashed on Systems Biology	Christopher R. Myers	2007-04-24 18:48:18	http://arxiv.org/abs/0704.3259v1	We have built an open-source software system for the modeling of biomolecular reaction networks, SloppyCell, which is written in Python and makes substantial use of third-party libraries for numerics, visualization, and parallel programming. We highlight here some of the powerful features that Python provides that enable SloppyCell to do dynamic code synthesis, symbolic manipulation, and parallel exploration of complex parameter spaces.			arxiv	['Ryan N. Gutenkunst', 'James. P. Sethna']	123.0
374	How applicable is Python as first computer language for teaching programming in a pre-university educational environment, from a teacher's point of view?	Fotis Georgatos	2008-09-09 14:39:57	http://arxiv.org/abs/0809.1437v1	This project report attempts to evaluate the educational properties of the Python computer language, in practice. This is done by examining computer language evolution history, related scientific background work, the existing educational research on computer languages and Python's experimental application in higher secondary education in Greece, during first half of year 2002. This Thesis Report was delivered in advance of a thesis defense for a Masters/Doctorandus (MSc/Drs) title with the Amstel Institute/Universiteit van Amsterdam, during the same year.			arxiv	[]	124.0
375	mlpy: Machine Learning Python	Davide Albanese	2012-02-29 13:49:10	http://arxiv.org/abs/1202.6548v2	mlpy is a Python Open Source Machine Learning library built on top of NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of state-of-the-art machine learning methods for supervised and unsupervised problems and it is aimed at finding a reasonable compromise among modularity, maintainability, reproducibility, usability and efficiency. mlpy is multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at the website http://mlpy.fbk.eu.			arxiv	['Roberto Visintainer', 'Stefano Merler', 'Samantha Riccadonna', 'Giuseppe Jurman', 'Cesare Furlanello']	125.0
376	pcigale: porting Code Investigating Galaxy Emission to Python	Yannick Roehlly	2013-09-24 22:49:51	http://arxiv.org/abs/1309.6366v1	We present pcigale, the port to Python of CIGALE (Code Investigating Galaxy Emission) a Fortran spectral energy distribution (SED) fitting code developed at the Laboratoire d'Astrophysique de Marseille. After recalling the specifics of the SED fitting method, we show the gains in modularity and versatility offered by Python, as well as the drawbacks compared to the compiled code.			arxiv	['Denis Burgarella', 'V√©ronique Buat', 'M√©d√©ric Boquien', 'Laure Ciesla', 'S√©bastien Heinis']	126.0
377	Modernizing PHCpack through phcpy	Jan Verschelde	2013-09-30 21:05:03	http://arxiv.org/abs/1310.0056v2	PHCpack is a large software package for solving systems of polynomial equations. The executable phc is menu driven and file oriented. This paper describes the development of phcpy, a Python interface to PHCpack. Instead of navigating through menus, users of phcpy solve systems in the Python shell or via scripts. Persistent objects replace intermediate files.			arxiv	[]	127.0
378	High-Content Digital Microscopy with Python	Fabrice Salvaire	2014-04-25 10:54:26	http://arxiv.org/abs/1404.6385v2	High-Content Digital Microscopy enhances user comfort, data storage and analysis throughput, paving the way to new researches and medical diagnostics. A digital microscopy platform aims at capturing an image of a cover slip, at storing information on a file server and a database, at visualising the image and analysing its content. We will discuss how the Python ecosystem can provide such software framework efficiently. Moreover this paper will give an illustration of the data chunking approach to manage the huge amount of data.			arxiv	[]	128.0
379	Mining online social networks with Python to study urban mobility	Ant√≤nia Tugores	2014-04-25 10:54:23	http://arxiv.org/abs/1404.6966v1	On-line social networks have grown quickly over the last few years and nowadays many people use them frequently. Furthermore the emergence of smartphones allows to access these networks any time from any physical location. Among the social networks, Twitter offers a particularly large set of data publicly available. Here we discuss the procedure to mine this data and store it in distributed databases using Python scripts. We also illustrate how geolocated tweets can be used to study the mobility of people in urban areas.			arxiv	['Pere Colet']	129.0
380	Computing the coefficients for the power series solution of the Lane-Emden equation with the Python library SymPy	Klaus Rohe	2014-09-06 12:19:37	http://arxiv.org/abs/1409.2008v2	It is shown how the Python library Sympy can be used to compute symbolically the coefficients of the power series solution of the Lane-Emden equation (LEE). Sympy is an open source Python library for symbolic mathematics. The power series solutions are compared to the numerically computed solutions using matplotlib. The results of a run time measurement of the implemented algorithm are discussed at the end.			arxiv	[]	130.0
381	Frequentism and Bayesianism: A Python-driven Primer	Jake VanderPlas	2014-11-18 21:00:00	http://arxiv.org/abs/1411.5018v1	This paper presents a brief, semi-technical comparison of the essential features of the frequentist and Bayesian approaches to statistical inference, with several illustrative examples implemented in Python. The differences between frequentism and Bayesianism fundamentally stem from differing definitions of probability, a philosophical divide which leads to distinct approaches to the solution of statistical problems as well as contrasting ways of asking and answering questions about unknown parameters. After an example-driven discussion of these differences, we briefly compare several leading Python statistical packages which implement frequentist inference using classical methods and Bayesian inference using Markov Chain Monte Carlo.			arxiv	[]	131.0
382	Wyrm, A Pythonic Toolbox for Brain-Computer Interfacing	Bastian Venthur	2014-12-19 15:34:01	http://arxiv.org/abs/1412.6378v1	A Brain-Computer Interface (BCI) is a system that measures central nervous system activity and translates the recorded data into an output suitable for a computer to use as an input signal. Such a BCI system consists of three parts, the signal acquisition, the signal processing and the feedback/stimulus presentation. In this paper we present Wyrm, a signal processing toolbox for BCI in Python. Wyrm is applicable to a broad range of neuroscientific problems and capable for running online experiments in real time and off-line data analysis and visualisation.			arxiv	['Benjamin Blankertz']	132.0
383	PyPanda: a Python Package for Gene Regulatory Network Reconstruction	David G. P. van IJzendoorn	2016-04-22 19:17:43	http://arxiv.org/abs/1604.06783v2	PANDA (Passing Attributes between Networks for Data Assimilation) is a gene regulatory network inference method that uses message-passing to integrate multiple sources of 'omics data. PANDA was originally coded in C++. In this application note we describe PyPanda, the Python version of PANDA. PyPanda runs considerably faster than the C++ version and includes additional features for network analysis. Availability: The open source PyPanda Python package is freely available at https://github.com/davidvi/pypanda. Contact: d.g.p.van ijzendoorn@lumc.nl			arxiv	['Kimberly Glass', 'John Quackenbush', 'Marieke L. Kuijjer']	133.0
384	Data Poisoning: Lightweight Soft Fault Injection for Python	Mohammad Amin Alipour	2016-11-04 19:24:14	http://arxiv.org/abs/1611.01501v1	This paper introduces and explores the idea of data poisoning, a light-weight peer-architecture technique to inject faults into Python programs. This method requires very small modification to the original program, which facilitates evaluation of sensitivity of systems that are prototyped or modeled in Python. We propose different fault scenarios that can be injected to programs using data poisoning. We use Dijkstra's Self Stabilizing Ring Algorithm to illustrate the approach.			arxiv	['Alex Groce']	134.0
385	Pyndri: a Python Interface to the Indri Search Engine	Christophe Van Gysel	2017-01-03 17:17:34	http://arxiv.org/abs/1701.00749v1	We introduce pyndri, a Python interface to the Indri search engine. Pyndri allows to access Indri indexes from Python at two levels: (1) dictionary and tokenized document collection, (2) evaluating queries on the index. We hope that with the release of pyndri, we will stimulate reproducible, open and fast-paced IR research.			arxiv	['Evangelos Kanoulas', 'Maarten de Rijke']	135.0
386	AutoWIG: Automatic Generation of Python Bindings for C++ Libraries	Pierre Fernique	2017-05-31 09:19:42	http://arxiv.org/abs/1705.11000v1	Most of Python and R scientific packages incorporate compiled scientific libraries to speed up the code and reuse legacy libraries. While several semi-automatic solutions exist to wrap these compiled libraries, the process of wrapping a large library is cumbersome and time consuming. In this paper, we introduce AutoWIG, a Python package that wraps automatically compiled libraries into high-level languages using LLVM/Clang technologies and the Mako templating engine. Our approach is automatic, extensible, and applies to complex C++ libraries, composed of thousands of classes or incorporating modern meta-programming constructs.			arxiv	['Christophe Pradal']	136.0
387	Leabra7: a Python package for modeling recurrent, biologically-realistic neural networks	C. Daniel Greenidge	2018-09-11 21:09:25	http://arxiv.org/abs/1809.04166v2	Emergent is a software package that uses the AdEx neural dynamics model and LEABRA learning algorithm to simulate and train arbitrary recurrent neural network architectures in a biologically-realistic manner. We present Leabra7, a complementary Python library that implements these same algorithms. Leabra7 is developed and distributed using modern software development principles, and integrates tightly with Python's scientific stack. We demonstrate recurrent Leabra7 networks using traditional pattern-association tasks and a standard machine learning task, classifying the IRIS dataset.			arxiv	['Noam Miller', 'Kenneth A. Norman']	137.0
388	Cyanure: An Open-Source Toolbox for Empirical Risk Minimization for Python, C++, and soon more	Julien Mairal	2019-12-17 18:04:31	http://arxiv.org/abs/1912.08165v2	Cyanure is an open-source C++ software package with a Python interface. The goal of Cyanure is to provide state-of-the-art solvers for learning linear models, based on stochastic variance-reduced stochastic optimization with acceleration mechanisms. Cyanure can handle a large variety of loss functions (logistic, square, squared hinge, multinomial logistic) and regularization functions (l_2, l_1, elastic-net, fused Lasso, multi-task group Lasso). It provides a simple Python API, which is very close to that of scikit-learn, which should be extended to other languages such as R or Matlab in a near future.			arxiv	[]	138.0
389	Comparing Python, Go, and C++ on the N-Queens Problem	Pascal Fua	2020-01-08 13:09:11	http://arxiv.org/abs/2001.02491v1	Python currently is the dominant language in the field of Machine Learning but is often criticized for being slow to perform certain tasks. In this report, we use the well-known $N$-queens puzzle as a benchmark to show that once compiled using the Numba compiler it becomes competitive with C++ and Go in terms of execution speed while still allowing for very fast prototyping. This is true of both sequential and parallel programs. In most cases that arise in an academic environment, it therefore makes sense to develop in ordinary Python, identify computational bottlenecks, and use Numba to remove them.			arxiv	['Krzysztof Lis']	139.0
390	FDApy: a Python package for functional data	Steven Golovkine	2021-01-26 10:07:33	http://arxiv.org/abs/2101.11003v1	We introduce the Python package, FDApy, as an implementation of functional data. This package provide modules for the analysis of such data. It includes classes for different dimensional data as well as irregularly sampled functional data. A simulation toolbox is also provided. It might be used to simulate different clusters of functional data. Some methodologies to handle these data are implemented, such as dimension reduction and clustering. New methods can be easily added. The package is publicly available on the Python Package Index and Github.			arxiv	[]	140.0
391	CVXPY: A Python-Embedded Modeling Language for Convex Optimization	Steven Diamond	2016-03-03 01:07:38	http://arxiv.org/abs/1603.00943v2	CVXPY is a domain-specific language for convex optimization embedded in Python. It allows the user to express convex optimization problems in a natural syntax that follows the math, rather than in the restrictive standard form required by solvers. CVXPY makes it easy to combine convex optimization with high-level features of Python such as parallelism and object-oriented design. CVXPY is available at http://www.cvxpy.org/ under the GPL license, along with documentation and examples.			arxiv	['Stephen Boyd']	141.0
392	libact: Pool-based Active Learning in Python	Yao-Yuan Yang	2017-10-01 17:18:03	http://arxiv.org/abs/1710.00379v1	libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository.			arxiv	['Shao-Chuan Lee', 'Yu-An Chung', 'Tung-En Wu', 'Si-An Chen', 'Hsuan-Tien Lin']	142.0
393	psrqpy: a python interface for querying the ATNF pulsar catalogue	Matthew Pitkin	2018-06-05 11:48:48	http://arxiv.org/abs/1806.07809v1	This Python module provides an interface for querying the Australia Telescope National Facility (ATNF) pulsar catalogue (Manchester et al. 2005). The intended users are astronomers wanting to extract data from the catalogue through a script rather than having to download and parse text tables output using the standard web interface. It allows users to access information, such as pulsar frequencies and sky locations, on all pulsars in the catalogue. Querying of the catalogue can easily be incorporated into Python scripts.			arxiv	[]	143.0
394	HolPy: Interactive Theorem Proving in Python	Bohua Zhan	2019-05-15 06:39:00	http://arxiv.org/abs/1905.05970v2	HolPy is an interactive theorem proving system implemented in Python. It uses higher-order logic as the logical foundation. Its main features include a pervasive use of macros in producing, checking, and storing proofs, a JSON-based format for theories, and an API for implementing proof automation and other extensions in Python. A point-and-click-based user interface is implemented for general-purpose theorem proving. We describe the main design decisions of HolPy, current applications, and plans for the future.			arxiv	[]	144.0
395	hankel: A Python library for performing simple and accurate Hankel transformations	Steven G. Murray	2019-06-03 21:37:26	http://arxiv.org/abs/1906.01088v1	This paper presents \textsc{hankel}, a pure-python code for solving Hankel-type integrals and transforms. Such transforms are common in the physical sciences, especially appearing as the radial solution to angularly symmetric Fourier Transforms in arbitrary dimensions. The code harnesses the advantages of solving such transforms via the one-dimensional Hankel transform -- an increase in conceptual simplicity and efficiency -- and implements them in the user-friendly and flexible Python language. We discuss several limitations of the adopted method, and point to the code's extensive documentation for further examples.			arxiv	['Francis J. Poulin']	145.0
396	Astroalign: A Python module for astronomical image registration	Martin Beroiz	2019-09-06 14:56:51	http://arxiv.org/abs/1909.02946v2	We present an algorithm implemented in the astroalign Python module for image registration in astronomy. Our module does not rely on WCS information and instead matches 3-point asterisms (triangles) on the images to find the most accurate linear transformation between the two. It is especially useful in the context of aligning images prior to stacking or performing difference image analysis. Astroalign can match images of different point-spread functions, seeing, and atmospheric conditions.			arxiv	['Juan B. Cabral', 'Bruno Sanchez']	146.0
397	CausalML: Python Package for Causal Machine Learning	Huigang Chen	2020-02-25 17:35:33	http://arxiv.org/abs/2002.11631v2	CausalML is a Python implementation of algorithms related to causal inference and machine learning. Algorithms combining causal inference and machine learning have been a trending topic in recent years. This package tries to bridge the gap between theoretical work on methodology and practical applications by making a collection of methods in this field available in Python. This paper introduces the key concepts, scope, and use cases of this package.			arxiv	['Totte Harinen', 'Jeong-Yoon Lee', 'Mike Yung', 'Zhenyu Zhao']	147.0
398	Pykat: Python package for modelling precision optical interferometers	Daniel D. Brown	2020-04-14 02:21:49	http://arxiv.org/abs/2004.06270v2	\textsc{Pykat} is a Python package which extends the popular optical interferometer modelling software \textsc{Finesse}. It provides a more modern and efficient user interface for conducting complex numerical simulations, as well as enabling the use of Python's extensive scientific software ecosystem. In this paper we highlight the relationship between \textsc{Pykat} and \textsc{Finesse}, how it is used, and provide an illustrative example of how it has helped to better understand the characteristics of the current generation of gravitational wave interferometers.			arxiv	['Philip Jones', 'Samuel Rowlinson', 'Andreas Freise', 'Sean Leavey', 'Anna C. Green', 'Daniel Toyra']	148.0
399	miniKanren as a Tool for Symbolic Computation in Python	Brandon T. Willard	2020-05-24 03:09:08	http://arxiv.org/abs/2005.11644v3	"In this article, we give a brief overview of the current state and future potential of symbolic computation within the Python statistical modeling and machine learning community. We detail the use of miniKanren as an underlying framework for term rewriting and symbolic mathematics, as well as its ability to orchestrate the use of existing Python libraries. We also discuss the relevance and potential of relational programming for implementing more robust, portable, domain-specific ""math-level"" optimizations--with a slight focus on Bayesian modeling. Finally, we describe the work going forward and raise some questions regarding potential cross-overs between statistical modeling and programming language theory."			arxiv	[]	149.0
400	SymFields: An Open Source Symbolic Fields Analysis Tool for General Curvilinear Coordinates in Python	Nan Chu	2020-12-19 16:08:15	http://arxiv.org/abs/2012.10723v1	An open source symbolic tool for vector fields analysis 'SymFields' is developed in Python. The SymFields module is constructed upon Python symbolic module sympy, which could only conduct scaler field analysis. With SymFields module, you can conduct vector analysis for general curvilinear coordinates regardless whether it is orthogonal or not. In SymFields, the differential operators based on metric tensor are normalized to real physical values, which means your can use real physical value of the vector fields as inputs. This could greatly free the physicists from the tedious calculation under complicated coordinates.			arxiv	[]	150.0
401	Accelerated Multiple Precision Direct Method and Mixed Precision Iterative Refinement on Python Programming Environment	Tomonori Kouya	2021-07-27 01:57:03	http://arxiv.org/abs/2107.12550v1	"Current Python programming environment does not have any reliable and efficient multiple precision floating-point (MPF) arithmetic except ``mpmath"" and ``gmpy2"" packages based on GNU MP(GMP) and MPFR libraries. Although it is well known that multi-component-type MPF library can be utilized for middle length precision arithmetic under 200 bits, they are not widely used on Python environment. In this paper, we describe our accelerated MPF direct method with AVX2 techniques and its application to mixed precision iterative refinement combined with mpmath, and demonstrate their efficiency on x86\_64 computational environments."			arxiv	[]	151.0
402	rigidPy: Rigidity Analysis in Python	Varda F. Hagh	2021-08-16 16:05:54	http://arxiv.org/abs/2108.07195v2	rigidPy is a Python package that provides a set of tools necessary for studying rigidity and mechanical response in spring networks. It also includes suitable modules for generating new realizations of networks with applications in glassy systems and protein structures. rigidPy is available freely on GitHub and can be installed using Python Package Index (PyPi). The detailed setup information is provided in this paper, along with an overview of the mathematical framework that has been used in developing the package.			arxiv	['Mahdi Sadjadi']	152.0
403	Analysing high-throughput sequencing data in Python with HTSeq 2.0	Givanna H Putri	2021-12-02 02:32:31	http://arxiv.org/abs/2112.00939v1	Summary: HTSeq 2.0 provides a more extensive API including a new representation for sparse genomic data, enhancements in htseq-count to suit single cell omics, a new script for data using cell and molecular barcodes, improved documentation, testing and deployment, bug fixes, and Python 3 support. Availability and implementation: HTSeq 2.0 is released as an open-source software under the GNU General Public Licence and available from the Python Package Index at https://pypi.python.org/pypi/HTSeq. The source code is available on Github at https://github.com/htseq/htseq. Contact: fabio.zanini@unsw.edu.au			arxiv	['Simon Anders', 'Paul Theodor Pyl', 'John E Pimanda', 'Fabio Zanini']	153.0
404	py-irt: A Scalable Item Response Theory Library for Python	John P. Lalor	2022-03-02 18:09:46	http://arxiv.org/abs/2203.01282v2	py-irt is a Python library for fitting Bayesian Item Response Theory (IRT) models. py-irt estimates latent traits of subjects and items, making it appropriate for use in IRT tasks as well as ideal-point models. py-irt is built on top of the Pyro and PyTorch frameworks and uses GPU-accelerated training to scale to large data sets. Code, documentation, and examples can be found at https://github.com/nd-ball/py-irt. py-irt can be installed from the GitHub page or the Python Package Index (PyPI).			arxiv	['Pedro Rodriguez']	154.0
405	Exporting Ada Software to Python and Julia	Jan Verschelde	2022-06-28 19:51:48	http://arxiv.org/abs/2206.14270v1	The objective is to demonstrate the making of Ada software available to Python and Julia programmers using GPRbuild. GPRbuild is the project manager of the GNAT toolchain. With GPRbuild the making of shared object files is fully automated and the software can be readily used in Python and Julia. The application is the build process of PHCpack, a free and open source software package to solve polynomial systems by homotopy continuation methods, written mainly in Ada, with components in C++, available at github at https://github.com/janverschelde/PHCpack.			arxiv	[]	155.0
406	PyQCAMS: Python Quasi-Classical Atom-Molecule Scattering	Rian Koots	2023-04-05 20:22:02	http://arxiv.org/abs/2304.02731v1	We present Python Quasi-classical atom-molecule scattering (PyQCAMS), a new Python package for atom-molecule scattering within the quasi-classical trajectory approach. The input consists of mass, collision energy, impact parameter, and pair-wise interactions to choose between Buckingham, generalized Lennard-Jones, and Morse potentials. As the output, the code provides the vibrational quenching, dissociation, and reactive cross sections along with the rovibrational energy distribution of the reaction products. Furthermore, we treat H$_2$ + Ca $\rightarrow$ CaH + H reactions as a prototypical example to illustrate the properties and performance of the software. Finally, we study the parallelization performance of the code by looking into the time per trajectory as a function of the number of CPUs used.			arxiv	['Jes√∫s P√©rez-R√≠os']	156.0
407	Python Tool for Visualizing Variability of Pareto Fronts over Multiple Runs	Shuhei Watanabe	2023-05-15 17:59:34	http://arxiv.org/abs/2305.08852v1	Hyperparameter optimization is crucial to achieving high performance in deep learning. On top of the performance, other criteria such as inference time or memory requirement often need to be optimized due to some practical reasons. This motivates research on multi-objective optimization (MOO). However, Pareto fronts of MOO methods are often shown without considering the variability caused by random seeds and this makes the performance stability evaluation difficult. Although there is a concept named empirical attainment surface to enable the visualization with uncertainty over multiple runs, there is no major Python package for empirical attainment surface. We, therefore, develop a Python package for this purpose and describe the usage. The package is available at https://github.com/nabenabe0928/empirical-attainment-func.			arxiv	[]	157.0
408	ARULESPY: Exploring Association Rules and Frequent Itemsets in Python	Michael Hahsler	2023-05-24 15:52:01	http://arxiv.org/abs/2305.15263v1	The R arules package implements a comprehensive infrastructure for representing, manipulating, and analyzing transaction data and patterns using frequent itemsets and association rules. The package also provides a wide range of interest measures and mining algorithms, including the code of Christian Borgelt's popular and efficient C implementations of the association mining algorithms Apriori and Eclat, and optimized C/C++ code for mining and manipulating association rules using sparse matrix representation. This document describes the new Python package arulespy, which makes this infrastructure available for Python users.			arxiv	[]	158.0
409	A Simple Python Testbed for Federated Learning Algorithms	Miroslav Popovic	2023-05-31 16:59:51	http://arxiv.org/abs/2305.20027v2	Nowadays many researchers are developing various distributed and decentralized frameworks for federated learning algorithms. However, development of such a framework targeting smart Internet of Things in edge systems is still an open challenge. In this paper, we present our solution to that challenge called Python Testbed for Federated Learning Algorithms. The solution is written in pure Python, and it supports both centralized and decentralized algorithms. The usage of the presented solution is both validated and illustrated by three simple algorithm examples.			arxiv	['Marko Popovic', 'Ivan Kastelan', 'Miodrag Djukic', 'Silvia Ghilezan']	159.0
410	UXsim: An open source macroscopic and mesoscopic traffic simulator in Python -- a technical overview	Toru Seo	2023-09-29 10:16:28	http://arxiv.org/abs/2309.17114v2	This note describes a technical overview of UXsim, an open source macro/mesoscopic traffic simulator in pure Python programming language. UXsim is based on Kinematic Wave model (more specifically, mesoscopic version of Newell's simplified car-following model) and dynamic user optimum-like route choice principle, which are well established methodology in the transportation research field. It can compute dynamical network traffic flow and have basic visualization and analysis capability. Furthermore, users can implement their own models and control methods into the simulator by using Python, thanks to the flexibility of the language. The simulator and its codes are freely available at https://github.com/toruseo/UXsim under the MIT license.			arxiv	[]	160.0
411	arfpy: A python package for density estimation and generative modeling with adversarial random forests	Kristin Blesch	2023-11-13 14:28:21	http://arxiv.org/abs/2311.07366v1	This paper introduces $\textit{arfpy}$, a python implementation of Adversarial Random Forests (ARF) (Watson et al., 2023), which is a lightweight procedure for synthesizing new data that resembles some given data. The software $\textit{arfpy}$ equips practitioners with straightforward functionalities for both density estimation and generative modeling. The method is particularly useful for tabular data and its competitive performance is demonstrated in previous literature. As a major advantage over the mostly deep learning based alternatives, $\textit{arfpy}$ combines the method's reduced requirements in tuning efforts and computational resources with a user-friendly python interface. This supplies audiences across scientific fields with software to generate data effortlessly.			arxiv	['Marvin N. Wright']	161.0
412	Toward a comprehensive simulation framework for hypergraphs: a Python-base approach	Quoc Chuong Nguyen	2024-01-08 14:24:54	http://arxiv.org/abs/2401.03917v1	Hypergraphs, or generalization of graphs such that edges can contain more than two nodes, have become increasingly prominent in understanding complex network analysis. Unlike graphs, hypergraphs have relatively few supporting platforms, and such dearth presents a barrier to more widespread adaptation of hypergraph computational toolboxes that could enable further research in several areas. Here, we introduce HyperRD, a Python package for hypergraph computation, simulation, and interoperability with other powerful Python packages in graph and hypergraph research. Then, we will introduce two models on hypergraph, the general Schelling's model and the SIR model, and simulate them with HyperRD.			arxiv	['Trung Kien Le']	162.0
413	Postprandial morphological response of the intestinal epithelium of the Burmese python (Python molurus)	Jean-Herv√© Lignot	2006-11-06 13:08:54	http://arxiv.org/abs/q-bio/0611019v1	The postprandial morphological changes of the intestinal epithelium of Burmese pythons were examined using fasting pythons and at eight time points after feeding. In fasting pythons, tightly packed enterocytes possess very short microvilli and are arranged in a pseudostratified fashion. Enterocyte width increases by 23% within 24 h postfeeding, inducing significant increases in villus length and intestinal mass. By 6 days postfeeding, enterocyte volume had peaked, following as much as an 80% increase. Contributing to enterocyte hypertrophy is the cellular accumulation of lipid droplets at the tips and edges of the villi of the proximal and middle small intestine, but which were absent in the distal small intestine. At 3 days postfeeding, conventional and environmental scanning electron microscopy revealed cracks and lipid extrusion along the narrow edges of the villi and at the villus tips. Transmission electron microscopy demonstrated the rapid postprandial lengthening of enterocyte microvilli, increasing 4.8-fold in length within 24 h, and the maintaining of that length through digestion. Beginning at 24 h postfeeding, spherical particles were found embedded apically within enterocytes of the proximal and middle small intestine. These particles possessed an annular-like construction and were stained with the calcium-stain Alizarine red S suggesting that they were bone in origin. Following the completion of digestion, many of the postprandial responses were reversed, as observed by the atrophy of enterocytes, the shortening of villi, and the retraction of the microvilli. Further exploration of the python intestine will reveal the underlying mechanisms of these trophic responses and the origin and fate of the engulfed particles.			arxiv	['C√©cile Helmstetter', 'Stephen M Secor']	163.0
414	Gradual Typing in an Open World	Michael M. Vitousek	2016-10-26 19:36:31	http://arxiv.org/abs/1610.08476v1	Gradual typing combines static and dynamic typing in the same language, offering the benefits of both to programmers. Static typing provides error detection and strong guarantees while dynamic typing enables rapid prototyping and flexible programming idioms. For programmers to fully take advantage of a gradual type system, however, they must be able to trust their type annotations, and so runtime checks must be performed at the boundaries of static and dynamic code to ensure that static types are respected. Higher order and mutable values cannot be completely checked at these boundaries, and so additional checks must be performed at their use sites. Traditionally, this has been achieved by installing wrappers or proxies on such values that moderate the flow of data between static and dynamic, but these can cause problems if the language supports comparison of object identity or has a foreign function interface. Reticulated Python is a gradually typed variant of Python implemented via a source-to-source translator for Python 3. It implements a proxy-free alternative design named transient casts. This paper presents a formal semantics for transient casts and shows that not only are they sound, but they work in an open-world setting in which the Reticulated translator has only been applied to some of the program; the rest is untranslated Python. We formalize this open world soundness property and use Coq to prove that it holds for Anthill Python, a calculus that models Reticulated Python.			arxiv	['Jeremy G. Siek']	164.0
415	SimpleSBML: A Python package for creating, editing, and interrogating SBML models: Version 2.0	Herbert M Sauro	2020-09-04 00:38:04	http://arxiv.org/abs/2009.01969v3	In this technical report, we describe a new version of SimpleSBML which provides an easier to use interface to python-libSBML allowing users of Python to more easily construct, edit, and inspect SBML based models. The most commonly used package for constructing SBML models in Python is python-libSBML based on the C/C++ library libSBML. python-libSBML is a comprehensive library with a large range of options but can be difficult for new users to learn and requires long scripts to create even the simplest models. Inspecting existing SBML models can also be difficult due to the complexity of the underlying object model. Instead, we present SimpleSBML, a package that allows users to add and inspect species, parameters, reactions, events, and rules to a libSBML model with only one command for each. Models can be exported to SBML format, and SBML files can be imported and converted to SimpleSBML commands making it very easy to edit the original SBML model. In the new version, a range of `get' methods is provided that allows users to inspect existing SBML models without having to understand the underlying object model used by libSBML.			arxiv	[]	165.0
416	PyART: Python API Recommendation in Real-Time	Xincheng He	2021-02-09 08:50:02	http://arxiv.org/abs/2102.04706v1	API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommend APIs for Python programs in real-time. It features a light-weight analysis to derives so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.			arxiv	['Lei Xu', 'Xiangyu Zhang', 'Rui Hao', 'Yang Feng', 'Baowen Xu']	166.0
417	Python's Lunches in Jackiw-Teitelboim gravity with matter	Dongsu Bak	2021-12-08 10:51:52	http://arxiv.org/abs/2112.04224v2	We study Python's lunch geometries in the two-dimensional Jackiw-Teitelboim model coupled to a massless scalar field in the semiclassical limit. We show that all extrema including the minimal quantum extremal surface, bulges and appetizers lie inside the horizon. We obtain fully back-reacted general bulk solutions with a massless scalar field, which can be understood as deformations of black holes. The temperatures of the left/right black holes become in general different from each other. Moreover, in the presence of both state and source deformations at the same time, the asymptotic black hole spacetime is further excited from that of the vacuum solution. We provide information-theoretic interpretation of deformed geometries including Python's lunches, minimal quantum extremal surface and appetizers according to the entanglement wedge reconstruction hypothesis. By considering the restricted circuit complexity associated with Python's lunch geometries and the operator complexity of the Petz map reconstructing a code space operation, we show that the observational probability of Python's lunch degrees of freedom from the boundary is exponentially suppressed. Thus, any bulk causality violation effects related with Python's lunch degrees are suppressed nonperturbatively.			arxiv	['Chanju Kim', 'Sang-Heon Yi', 'Junggi Yoon']	167.0
418	pycefr: Python Competency Level through Code Analysis	Gregorio Robles	2022-03-30 01:54:26	http://arxiv.org/abs/2203.15990v1	Python is known to be a versatile language, well suited both for beginners and advanced users. Some elements of the language are easier to understand than others: some are found in any kind of code, while some others are used only by experienced programmers. The use of these elements lead to different ways to code, depending on the experience with the language and the knowledge of its elements, the general programming competence and programming skills, etc. In this paper, we present pycefr, a tool that detects the use of the different elements of the Python language, effectively measuring the level of Python proficiency required to comprehend and deal with a fragment of Python code. Following the well-known Common European Framework of Reference for Languages (CEFR), widely used for natural languages, pycefr categorizes Python code in six levels, depending on the proficiency required to create and understand it. We also discuss different use cases for pycefr: identifying code snippets that can be understood by developers with a certain proficiency, labeling code examples in online resources such as Stackoverflow and GitHub to suit them to a certain level of competency, helping in the onboarding process of new developers in Open Source Software projects, etc. A video shows availability and usage of the tool: https://tinyurl.com/ypdt3fwe.			arxiv	['Raula Gaikovina Kula', 'Chaiyong Ragkhitwetsagul', 'Tattiya Sakulniwat', 'Kenichi Matsumoto', 'Jesus M. Gonzalez-Barahona']	168.0
419	Safe-DS: A Domain Specific Language to Make Data Science Safe	Lars Reimann	2023-02-28 13:14:07	http://arxiv.org/abs/2302.14548v2	Due to the long runtime of Data Science (DS) pipelines, even small programming mistakes can be very costly, if they are not detected statically. However, even basic static type checking of DS pipelines is difficult because most are written in Python. Static typing is available in Python only via external linters. These require static type annotations for parameters or results of functions, which many DS libraries do not provide. In this paper, we show how the wealth of Python DS libraries can be used in a statically safe way via Safe-DS, a domain specific language (DSL) for DS. Safe-DS catches conventional type errors plus errors related to range restrictions, data manipulation, and call order of functions, going well beyond the abilities of current Python linters. Python libraries are integrated into Safe-DS via a stub language for specifying the interface of its declarations, and an API-Editor that is able to extract type information from the code and documentation of Python libraries, and automatically generate suitable stubs. Moreover, Safe-DS complements textual DS pipelines with a graphical representation that eases safe development by preventing syntax errors. The seamless synchronization of textual and graphic view lets developers always choose the one best suited for their skills and current task. We think that Safe-DS can make DS development easier, faster, and more reliable, significantly reducing development costs.			arxiv	['G√ºnter Kniesel-W√ºnsche']	169.0
420	Efficient and Accurate Automatic Python Bindings with cppyy & Cling	Baidyanath Kundu	2023-04-05 19:12:05	http://arxiv.org/abs/2304.02712v1	The simplicity of Python and the power of C++ force stark choices on a scientific software stack. There have been multiple developments to mitigate language boundaries by implementing language bindings, but the impedance mismatch between the static nature of C++ and the dynamic one of Python hinders their implementation; examples include the use of user-defined Python types with templated C++ and advanced memory management. The development of the C++ interpreter Cling has changed the way we can think of language bindings as it provides an incremental compilation infrastructure available at runtime. That is, Python can interrogate C++ on demand, and bindings can be lazily constructed at runtime. This automatic binding provision requires no direct support from library authors and offers better performance than alternative solutions, such as PyBind11. ROOT pioneered this approach with PyROOT, which was later enhanced with its successor, cppyy. However, until now, cppyy relied on the reflection layer of ROOT, which is limited in terms of provided features and performance. This paper presents the next step for language interoperability with cppyy, enabling research into uniform cross-language execution environments and boosting optimization opportunities across language boundaries. We illustrate the use of advanced C++ in Numba-accelerated Python through cppyy. We outline a path forward for re-engineering parts of cppyy to use upstream LLVM components to improve performance and sustainability. We demonstrate cppyy purely based on a C++ reflection library, InterOp, which offers interoperability primitives based on Cling and Clang-Repl.			arxiv	['Vassil Vassilev', 'Wim Lavrijsen']	170.0
421	Empowering Learner-Centered Instruction: Integrating ChatGPT Python API and Tinker Learning for Enhanced Creativity and Problem-Solving Skills	Yun-Cheng Tsai	2023-05-01 13:40:25	http://arxiv.org/abs/2305.00821v1	The ChatGPT Python API plays a crucial role in promoting Learner-Centered Instruction (LCI) and aligns with the principles of Tinker Learning, allowing students to discover their learning strategies. LCI emphasizes the importance of active, hands-on learning experiences and encourages students to take responsibility for their learning journey. By integrating the ChatGPT Python API into the educational process, students can explore various resources, generate new ideas, and create content in a more personalized manner. This innovative approach enables students to engage with the learning material deeper, fostering a sense of ownership and motivation. As they work through the Creative Learning Spiral, students develop essential skills such as critical thinking, problem-solving, and creativity. The ChatGPT Python API is a valuable tool for students to explore different solutions, evaluate alternatives, and make informed decisions, all while encouraging self-directed learning. In Tinker Learning environments, the integration of ChatGPT Python API empowers students to experiment and iterate, allowing them to find the most effective learning strategies that cater to their individual needs and preferences. This personalized approach helps students to become more confident in their abilities, leading to tremendous academic success and long-term skill development. By leveraging the capabilities of the ChatGPT Python API, educational institutions can create a more engaging, supportive, and dynamic learning environment. This approach aligns with the principles of Learner-Centered Instruction and Tinker Learning, promoting a culture of curiosity, exploration, and creativity among students while preparing them for the challenges of the fast-paced, ever-changing world.			arxiv	[]	171.0
422	GT4Py: High Performance Stencils for Weather and Climate Applications using Python	Enrique G. Paredes	2023-11-14 17:07:24	http://arxiv.org/abs/2311.08322v1	All major weather and climate applications are currently developed using languages such as Fortran or C++. This is typical in the domain of high performance computing (HPC), where efficient execution is an important concern. Unfortunately, this approach leads to implementations that intermix optimizations for specific hardware architectures with the high-level numerical methods that are typical for the domain. This leads to code that is verbose, difficult to extend and maintain, and difficult to port to different hardware architectures. Here, we propose a different strategy based on GT4Py (GridTools for Python). GT4Py is a Python framework to write weather and climate applications that includes a high-level embedded domain specific language (DSL) to write stencil computations. The toolchain integrated in GT4Py enables automatic code-generation,to obtain the performance of state-of-the-art C++ and CUDA implementations. The separation of concerns between the mathematical definitions and the actual implementations allows for performance portability of the computations on a wide range of computing architectures, while being embedded in Python allows easy access to the tools of the Python ecosystem to enhance the productivity of the scientists and facilitate integration in complex workflows. Here, the initial release of GT4Py is described, providing an overview of the current state of the framework and performance results showing how GT4Py can outperform pure Python implementations by orders of magnitude.			arxiv	['Linus Groner', 'Stefano Ubbiali', 'Hannes Vogt', 'Alberto Madonna', 'Kean Mariotti', 'Felipe Cruz', 'Lucas Benedicic', 'Mauro Bianco', 'Joost VandeVondele', 'Thomas C. Schulthess']	172.0
423	ModuleGuard:Understanding and Detecting Module Conflicts in Python Ecosystem	Ruofan Zhu	2024-01-04 06:26:07	http://arxiv.org/abs/2401.02090v1	Python has become one of the most popular programming languages for software development due to its simplicity, readability, and versatility. As the Python ecosystem grows, developers face increasing challenges in avoiding module conflicts, which occur when different packages have the same namespace modules. Unfortunately, existing work has neither investigated the module conflict comprehensively nor provided tools to detect the conflict. Therefore, this paper systematically investigates the module conflict problem and its impact on the Python ecosystem. We propose a novel technique called InstSimulator, which leverages semantics and installation simulation to achieve accurate and efficient module extraction. Based on this, we implement a tool called ModuleGuard to detect module conflicts for the Python ecosystem. For the study, we first collect 97 MC issues, classify the characteristics and causes of these MC issues, summarize three different conflict patterns, and analyze their potential threats. Then, we conducted a large-scale analysis of the whole PyPI ecosystem (4.2 million packages) and GitHub popular projects (3,711 projects) to detect each MC pattern and analyze their potential impact. We discovered that module conflicts still impact numerous TPLs and GitHub projects. This is primarily due to developers' lack of understanding of the modules within their direct dependencies, not to mention the modules of the transitive dependencies. Our work reveals Python's shortcomings in handling naming conflicts and provides a tool and guidelines for developers to detect conflicts.			arxiv	['Xingyu Wang', 'Chengwei Liu', 'Zhengzi Xu', 'Wenbo Shen', 'Rui Chang', 'Yang Liu']	173.0
424	Multi-Agent Programming Contest 2012 - The Python-DTU Team	J√∏rgen Villadsen	2012-10-01 15:07:16	http://arxiv.org/abs/1210.0437v1	We provide a brief description of the Python-DTU system, including the overall design, the tools and the algorithms that we plan to use in the agent contest.			arxiv	['Andreas Schmidt Jensen', 'Mikko Berggren Ettienne', 'Steen Vester', 'Kenneth Balsiger Andersen', 'Andreas Fr√∏sig']	174.0
425	Python Classes for Numerical Solution of PDE's	Asif Mushtaq	2015-03-16 11:01:13	http://arxiv.org/abs/1503.04602v1	We announce some Python classes for numerical solution of partial differential equations, or boundary value problems of ordinary differential equations. These classes are built on routines in \texttt{numpy} and \texttt{scipy.sparse.linalg} (or \texttt{scipy.linalg} for smaller problems).			arxiv	['Trond Kvamsdal', 'K√•re Olaussen']	175.0
426	A Python Class for Higher-Dimensional Schr√∂dinger Equations	Amna Noreen	2015-03-16 11:14:51	http://arxiv.org/abs/1503.04607v1	"We announce a Python class for numerical solution of Schr{\""o}dinger equations in one or more space dimensions, employing some recently developed general classes for numerical solution of partial differential equations, and routines from \texttt{numpy} and \texttt{scipy.sparse.linalg} (or \texttt{scipy.linalg} for smaller problems)."			arxiv	['K√•re Olaussen']	176.0
427	The NLTK FrameNet API: Designing for Discoverability with a Rich Linguistic Resource	Nathan Schneider	2017-03-21 21:36:28	http://arxiv.org/abs/1703.07438v2	A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon (structured in terms of frames) as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.			arxiv	['Chuck Wooters']	177.0
428	StegIbiza: Steganography in Club Music Implemented in Python	Krzysztof Szczypiorski	2017-05-22 14:56:49	http://arxiv.org/abs/1705.07788v1	This paper introduces the implementation of steganography method called StegIbiza, which uses tempo modulation as hidden message carrier. With the use of Python scripting language, a bit string was encoded and decoded using WAV and MP3 files. Once the message was hidden into a music files, an internet radio was created to evaluate broadcast possibilities. No dedicated music or signal processing equipment was used in this StegIbiza implementation			arxiv	['Wojciech Zydecki']	178.0
429	ruptures: change point detection in Python	Charles Truong	2018-01-02 20:35:23	http://arxiv.org/abs/1801.00826v1	ruptures is a Python library for offline change point detection. This package provides methods for the analysis and segmentation of non-stationary signals. Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. ruptures focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package.			arxiv	['Laurent Oudre', 'Nicolas Vayatis']	179.0
430	libconform v0.1.0: a Python library for conformal prediction	Jonas Fassbender	2019-07-03 16:24:10	http://arxiv.org/abs/1907.02015v1	This paper introduces libconform v0.1.0, a Python library for the conformal prediction framework, licensed under the MIT-license. libconform is not yet stable. This paper describes the main algorithms implemented and documents the API of libconform. Also some details about the implementation and changes in future versions are described.			arxiv	[]	180.0
431	srlearn: A Python Library for Gradient-Boosted Statistical Relational Models	Alexander L. Hayes	2019-12-17 20:46:32	http://arxiv.org/abs/1912.08198v1	We present srlearn, a Python library for boosted statistical relational models. We adapt the scikit-learn interface to this setting and provide examples for how this can be used to express learning and inference problems.			arxiv	[]	181.0
432	anesthetic: nested sampling visualisation	Will Handley	2019-05-12 18:37:10	http://arxiv.org/abs/1905.04768v1	anesthetic is a Python package for processing nested sampling runs, and will be useful for any scientist or statistician who uses nested sampling software. anesthetic unifies many existing tools and techniques in an extensible framework that is intuitive for users familiar with the standard Python packages, namely NumPy, SciPy, Matplotlib and pandas.			arxiv	[]	182.0
433	fgivenx: A Python package for functional posterior plotting	Will Handley	2019-08-02 15:30:12	http://arxiv.org/abs/1908.01711v1	fgivenx is a Python package for functional posterior plotting, currently used in astronomy, but will be of use to scientists performing any Bayesian analysis which has predictive posteriors that are functions. The source code for fgivenx is available on GitHub at https://github.com/williamjameshandley/fgivenx			arxiv	[]	183.0
434	MKLpy: a python-based framework for Multiple Kernel Learning	Ivano Lauriola	2020-07-20 10:10:13	http://arxiv.org/abs/2007.09982v1	Multiple Kernel Learning is a recent and powerful paradigm to learn the kernel function from data. In this paper, we introduce MKLpy, a python-based framework for Multiple Kernel Learning. The library provides Multiple Kernel Learning algorithms for classification tasks, mechanisms to compute kernel functions for different data types, and evaluation strategies. The library is meant to maximize the usability and to simplify the development of novel solutions.			arxiv	['Fabio Aiolli']	184.0
435	ana_cont: Python package for analytic continuation	Josef Kaufmann	2021-05-24 11:33:35	http://arxiv.org/abs/2105.11211v1	We present the Python package ana_cont for the analytic continuation of fermionic and bosonic many-body Green's functions by means of either the Pade approximants or the maximum entropy method. The determination of hyperparameters and the implementation are described in detail. The code is publicly available on GitHub, where also documentation and learning resources are provided.			arxiv	['Karsten Held']	185.0
436	PyNose: A Test Smell Detector For Python	Tongjie Wang	2021-08-10 12:43:09	http://arxiv.org/abs/2108.04639v1	Similarly to production code, code smells also occur in test code, where they are called test smells. Test smells have a detrimental effect not only on test code but also on the production code that is being tested. To date, the majority of the research on test smells has been focusing on programming languages such as Java and Scala. However, there are no available automated tools to support the identification of test smells for Python, despite its rapid growth in popularity in recent years. In this paper, we strive to extend the research to Python, build a tool for detecting test smells in this language, and conduct an empirical analysis of test smells in Python projects. We started by gathering a list of test smells from existing research and selecting test smells that can be considered language-agnostic or have similar functionality in Python's standard Unittest framework. In total, we identified 17 diverse test smells. Additionally, we searched for Python-specific test smells by mining frequent code change patterns that can be considered as either fixing or introducing test smells. Based on these changes, we proposed our own test smell called Suboptimal assert. To detect all these test smells, we developed a tool called PyNose in the form of a plugin to PyCharm, a popular Python IDE. Finally, we conducted a large-scale empirical investigation aimed at analyzing the prevalence of test smells in Python code. Our results show that 98% of the projects and 84% of the test suites in the studied dataset contain at least one test smell. Our proposed Suboptimal assert smell was detected in as much as 70.6% of the projects, making it a valuable addition to the list.			arxiv	['Yaroslav Golubev', 'Oleg Smirnov', 'Jiawei Li', 'Timofey Bryksin', 'Iftekhar Ahmed']	186.0
437	DeepAL: Deep Active Learning in Python	Kuan-Hao Huang	2021-11-30 10:17:58	http://arxiv.org/abs/2111.15258v1	We present DeepAL, a Python library that implements several common strategies for active learning, with a particular emphasis on deep active learning. DeepAL provides a simple and unified framework based on PyTorch that allows users to easily load custom datasets, build custom data handlers, and design custom strategies without much modification of codes. DeepAL is open-source on Github and welcome any contribution.			arxiv	[]	187.0
438	GridPyM: a Python module to handle grid diagrams	Agnese Barbensi	2022-10-13 22:35:14	http://arxiv.org/abs/2210.07399v1	Grid diagrams are a combinatorial version of classical link diagrams, widely used in theoretical, computational and applied knot theory. Motivated by questions from (bio)-physical knot theory, we introduce GridPyM, a Sage compatible Python module that handles grid diagrams. GridPyM focuses on generating and simplifying grids, and on modelling local transformations between them.			arxiv	['Daniele Celoria']	188.0
439	Diddy: a Python toolbox for infinite discrete dynamical systems	Ville Salo	2023-05-02 12:51:25	http://arxiv.org/abs/2305.01375v1	We introduce Diddy, a collection of Python scripts for analyzing infinite discrete dynamical systems. The main focus is on generalized multidimensional shifts of finite type (SFTs). We show how Diddy can be used to easily define SFTs and cellular automata, and analyze their basic properties. We also showcase how to verify or rediscover some results from coding theory and cellular automata theory.			arxiv	['Ilkka T√∂rm√§']	189.0
440	AQUATK: An Audio Quality Assessment Toolkit	Ashvala Vinay	2023-11-16 02:55:13	http://arxiv.org/abs/2311.10113v1	Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.			arxiv	['Alexander Lerch']	190.0
441	Gradual Soundness: Lessons from Static Python	Kuang-Chen Lu	2022-06-28 08:53:44	http://arxiv.org/abs/2206.13831v1	Context: Gradually-typed languages allow typed and untyped code to interoperate, but typically come with significant drawbacks. In some languages, the types are unreliable; in others, communication across type boundaries can be extremely expensive; and still others allow only limited forms of interoperability. The research community is actively seeking a sound, fast, and expressive approach to gradual typing. Inquiry: This paper describes Static Python, a language developed by engineers at Instagram that has proven itself sound, fast, and reasonably expressive in production. Static Python's approach to gradual types is essentially a programmer-tunable combination of the concrete and transient approaches from the literature. Concrete types provide full soundness and low performance overhead, but impose nonlocal constraints. Transient types are sound in a shallow sense and easier to use; they help to bridge the gap between untyped code and typed concrete code. Approach: We evaluate the language in its current state and develop a model that captures the essence of its approach to gradual types. We draw upon personal communication, bug reports, and the Static Python regression test suite to develop this model. Knowledge: Our main finding is that the gradual soundness that arises from a mix of concrete and transient types is an effective way to lower the maintenance cost of the concrete approach. We also find that method-based JIT technology can eliminate the costs of the transient approach. On a more technical level, this paper describes two contributions: a model of Static Python and a performance evaluation of Static Python. The process of formalization found several errors in the implementation, including fatal errors. Grounding: Our model of Static Python is implemented in PLT Redex and tested using property-based soundness tests and 265 tests from the Static Python regression suite. This paper includes a small core of the model to convey the main ideas of the Static Python approach and its soundness. Our performance claims are based on production experience in the Instagram web server. Migrations to Static Python in the server have caused a 3.7\% increase in requests handled per second at maximum CPU load. Importance: Static Python is the first sound gradual language whose piece-meal application to a realistic codebase has consistently improved performance. Other language designers may wish to replicate its approach, especially those who currently maintain unsound gradual languages and are seeking a path to soundness.			arxiv	['Ben Greenman', 'Carl Meyer', 'Dino Viehland', 'Aniket Panse', 'Shriram Krishnamurthi']	191.0
442	Data Reduction and Analysis of the Python V Cosmic Microwave Background Anisotropy Experiment	Kimberly Ann Coble	1999-11-22 20:16:01	http://arxiv.org/abs/astro-ph/9911419v1	Observations of the microwave sky using the Python telescope in its fifth season of operation at the Amundsen-Scott South Pole Station in Antarctica are presented. The system consists of a 0.75 m off-axis telescope instrumented with a HEMT amplifier-based radiometer having continuum sensitivity from 37-45 GHz in two frequency bands. With a $0.91^{\circ} \times 1.02^{\circ} $ beam the instrument fully sampled 598 deg$^2$ of sky, including fields measured during the previous four seasons of Python observations. Interpreting the observed fluctuations as anisotropy in the cosmic microwave background, we place constraints on the angular power spectrum of fluctuations in eight multipole bands up to $l \sim 260$. The observed spectrum is consistent with both the COBE experiment and previous Python results. Total-power Wiener-filtered maps of the CMB are also presented. There is no significant contamination from known foregrounds. The results show a discernible rise in the angular power spectrum from large ($l \sim 40$) to small ($l \sim 200$) angular scales.			arxiv	[]	192.0
443	An Anthological Review of Research Utilizing MontyLingua, a Python-Based End-to-End Text Processor	Maurice HT Ling	2006-11-22 03:24:54	http://arxiv.org/abs/cs/0611113v1	MontyLingua, an integral part of ConceptNet which is currently the largest commonsense knowledge base, is an English text processor developed using Python programming language in MIT Media Lab. The main feature of MontyLingua is the coverage for all aspects of English text processing from raw input text to semantic meanings and summary generation, yet each component in MontyLingua is loosely-coupled to each other at the architectural and code level, which enabled individual components to be used independently or substituted. However, there has been no review exploring the role of MontyLingua in recent research work utilizing it. This paper aims to review the use of and roles played by MontyLingua and its components in research work published in 19 articles between October 2004 and August 2006. We had observed a diversified use of MontyLingua in many different areas, both generic and domain-specific. Although the use of text summarizing component had not been observe, we are optimistic that it will have a crucial role in managing the current trend of information overload in future research.			arxiv	[]	193.0
444	Object-oriented implementations of the MPDATA advection equation solver in C++, Python and Fortran	Sylwester Arabas	2013-01-07 20:59:13	http://arxiv.org/abs/1301.1334v2	Three object-oriented implementations of a prototype solver of the advection equation are introduced. The presented programs are based on Blitz++ (C++), NumPy (Python), and Fortran's built-in array containers. The solvers include an implementation of the Multidimensional Positive-Definite Advective Transport Algorithm (MPDATA). The introduced codes exemplify how the application of object-oriented programming (OOP) techniques allows to reproduce the mathematical notation used in the literature within the program code. A discussion on the tradeoffs of the programming language choice is presented. The main angles of comparison are code brevity and syntax clarity (and hence maintainability and auditability) as well as performance. In the case of Python, a significant performance gain is observed when switching from the standard interpreter (CPython) to the PyPy implementation of Python. Entire source code of all three implementations is embedded in the text and is licensed under the terms of the GNU GPL license.			arxiv	['Dorota Jarecka', 'Anna Jaruga', 'Maciej Fija≈Çkowski']	194.0
445	PaPy: Parallel and Distributed Data-processing Pipelines in Python	Marcin Cieslik	2014-07-15 03:13:00	http://arxiv.org/abs/1407.4378v1	PaPy, which stands for parallel pipelines in Python, is a highly flexible framework that enables the construction of robust, scalable workflows for either generating or processing voluminous datasets. A workflow is created from user-written Python functions (nodes) connected by 'pipes' (edges) into a directed acyclic graph. These functions are arbitrarily definable, and can make use of any Python modules or external binaries. Given a user-defined topology and collection of input data, functions are composed into nested higher-order maps, which are transparently and robustly evaluated in parallel on a single computer or on remote hosts. Local and remote computational resources can be flexibly pooled and assigned to functional nodes, thereby allowing facile load-balancing and pipeline optimization to maximize computational throughput. Input items are processed by nodes in parallel, and traverse the graph in batches of adjustable size -- a trade-off between lazy-evaluation, parallelism, and memory consumption. The processing of a single item can be parallelized in a scatter/gather scheme. The simplicity and flexibility of distributed workflows using PaPy bridges the gap between desktop -> grid, enabling this new computing paradigm to be leveraged in the processing of large scientific datasets.			arxiv	['Cameron Mura']	195.0
446	Using Virtual Observatory with Python: querying remote astronomical databases	F. Paletou	2014-08-29 14:15:27	http://arxiv.org/abs/1408.7026v2	This tutorial is devoted to extending an existing catalogue with data taken elsewhere, either from CDS Vizier or Simbad database. As an example, we used the so-called 'Spectroscopic Survey of Stars in the Solar Neighborhood' (aka. S4N, Allende Prieto et al. 2004) in order to retrieve all objects with available data for the set of fundamental stellar parameters effective temperature, surface gravity and metallicity. Then for each object in this dataset we query Simbad database to retrieve the projected rotational velocity. This combines Vizier and Simbad queries made using Python astroquery module. The tutorial covers remote database access, filtering tables with arbitrary criteria, creating and writing your own tables, and basics of plotting in Python.			arxiv	['I. Zolotukhin']	196.0
447	SPySort: Neuronal Spike Sorting with Python	Christophe Pouzat	2014-12-19 15:40:00	http://arxiv.org/abs/1412.6383v1	Extracellular recordings with multi-electrode arrays is one of the basic tools of contemporary neuroscience. These recordings are mostly used to monitor the activities, understood as sequences of emitted action potentials, of many individual neurons. But the raw data produced by extracellular recordings are most commonly a mixture of activities from several neurons. In order to get the activities of the individual contributing neurons, a pre-processing step called spike sorting is required. We present here a pure Python implementation of a well tested spike sorting procedure. The latter was designed in a modular way in order to favour a smooth transition from an interactive sorting, for instance with IPython, to an automatic one. Surprisingly enough - or sadly enough, depending on one's view point -, recoding our now 15 years old procedure into Python was the occasion of major methodological improvements.			arxiv	['Georgios Is. Detorakis']	197.0
448	Using Python to Dive into Signalling Data with CellNOpt and BioServices	Thomas Cokelaer	2014-12-19 15:43:09	http://arxiv.org/abs/1412.6386v1	Systems biology is an inter-disciplinary field that studies systems of biological components at different scales, which may be molecules, cells or entire organism. In particular, systems biology methods are applied to understand functional deregulations within human cells (e.g., cancers). In this context, we present several python packages linked to CellNOptR (R package), which is used to build predictive logic models of signalling networks by training networks (derived from literature) to signalling (phospho-proteomic) data. The first package (cellnopt.wrapper) is a wrapper based on RPY2 that allows a full access to CellNOptR functionalities within Python. The second one (cellnopt.core) was designed to ease the manipulation and visualisation of data structures used in CellNOptR, which was achieved by using Pandas, NetworkX and matplotlib. Systems biology also makes extensive use of web resources and services. We will give an overview and status of BioServices, which allows one to access programmatically to web resources used in life science and how it can be combined with CellNOptR.			arxiv	['Julio Saez-Rodriguez']	198.0
449	Lowering the learning curve for declarative programming: a Python API for the IDP system	Joost Vennekens	2015-11-03 14:21:23	http://arxiv.org/abs/1511.00916v1	Programmers may be hesitant to use declarative systems, because of the associated learning curve. In this paper, we present an API that integrates the IDP Knowledge Base system into the Python programming language. IDP is a state-of-the-art logical system, which uses SAT, SMT, Logic Programming and Answer Set Programming technology. Python is currently one of the most widely used (teaching) languages for programming. The first goal of our API is to allow a Python programmer to use the declarative power of IDP, without needing to learn any new syntax or semantics. The second goal is allow IDP to be added to/removed from an existing code base with minimal changes.			arxiv	[]	199.0
450	PyRIDE: An Interactive Development Environment for PR2 Robot	Xun Wang	2016-05-30 02:40:43	http://arxiv.org/abs/1605.09089v1	Python based Robot Interactive Development Environment (PyRIDE) is a software that supports rapid \textit{interactive} programming of robot skills and behaviours on PR2/ROS (Robot Operating System) platform. One of the key features of PyRIDE is its interactive remotely accessible Python console that allows its users to program robots \textit{online} and in \textit{realtime} in the same way as using the standard Python interactive interpreter. It allows programs to be modified while they are running. PyRIDE is also a software integration framework that abstracts and aggregates disparate low level ROS software modules, e.g. arm joint motor controllers, and exposes their functionalities through a unified Python programming interface. PR2 programmers are able to experiment and develop robot behaviours without dealing with specific details of accessing underlying softwares and hardwares. PyRIDE provides a client-server mechanism that allows remote user access of the robot functionalities, e.g. remote robot monitoring and control, access real-time robot camera image data etc. This enables multi-modal human robot interactions using different devices and user interfaces. All these features are seamlessly integrated into one lightweight and portable middleware package. In this paper, we use four real life scenarios to demonstrate PyRIDE key features and illustrate the usefulness of software.			arxiv	['Mary-Anne Williams']	200.0
451	Massively parallel implementation in Python of a pseudo-spectral DNS code for turbulent flows	Mikael Mortensen	2016-07-01 19:05:11	http://arxiv.org/abs/1607.00850v1	Direct Numerical Simulations (DNS) of the Navier Stokes equations is a valuable research tool in fluid dynamics, but there are very few publicly available codes and, due to heavy number crunching, codes are usually written in low-level languages. In this work a \textasciitilde{}100 line standard scientific Python DNS code is described that nearly matches the performance of pure C for thousands of processors and billions of unknowns. With optimization of a few routines in Cython, it is found to match the performance of a more or less identical solver implemented from scratch in C++. Keys to the efficiency of the solver are the mesh decomposition and three dimensional FFT routines, implemented directly in Python using MPI, wrapped through MPI for Python, and a serial FFT module (both numpy.fft or pyFFTW may be used). Two popular decomposition strategies, slab and pencil, have been implemented and tested.			arxiv	[]	201.0
452	PyTransport: A Python package for the calculation of inflationary correlation functions	David J. Mulryne	2016-09-01 20:00:19	http://arxiv.org/abs/1609.00381v2	PyTransport constitutes a straightforward code written in C++ together with Python scripts which automatically edit, compile and run the C++ code as a Python module. It has been written for Unix-like systems (OS X and Linux). Primarily the module employs the transport approach to inflationary cosmology to calculate the tree-level power-spectrum and bispectrum of user specified models of multi-field inflation, accounting for all sub and super-horizon effects. The transport method we utilise means only coupled differential equations need to be solved, and the implementation presented here combines the speed of C++ with the functionality and convenience of Python. This document details the code and illustrates how to use it with a worked example. It has been updated to be a companion to the second version of the code, PyTransport 2.0, which includes functionality to deal with models of inflation with a curved field space metric.			arxiv	['John W. Ronayne']	202.0
453	A scikit-based Python environment for performing multi-label classification	Piotr Szyma≈Ñski	2017-02-05 22:28:20	http://arxiv.org/abs/1702.01460v5	scikit-multilearn is a Python library for performing multi-label classification. The library is compatible with the scikit/scipy ecosystem and uses sparse matrices for all internal operations. It provides native Python implementations of popular multi-label classification methods alongside a novel framework for label space partitioning and division. It includes modern algorithm adaptation methods, network-based label space division approaches, which extracts label dependency information and multi-label embedding classifiers. It provides python wrapped access to the extensive multi-label method stack from Java libraries and makes it possible to extend deep learning single-label methods for multi-label tasks. The library allows multi-label stratification and data set management. The implementation is more efficient in problem transformation than other established libraries, has good test coverage and follows PEP8. Source code and documentation can be downloaded from http://scikit.ml and also via pip. The library follows BSD licensing scheme.			arxiv	['Tomasz Kajdanowicz']	203.0
454	Hands-on Experience with Gaussian Processes (GPs): Implementing GPs in Python - I	Kshitij Tiwari	2018-09-06 10:19:50	http://arxiv.org/abs/1809.01913v1	This document serves to complement our website which was developed with the aim of exposing the students to Gaussian Processes (GPs). GPs are non-parametric Bayesian regression models that are largely used by statisticians and geospatial data scientists for modeling spatial data. Several open source libraries spanning from Matlab [1], Python [2], R [3] etc., are already available for simple plug-and-use. The objective of this handout and in turn the website was to allow the users to develop stand-alone GPs in Python by relying on minimal external dependencies. To this end, we only use the default python modules and assist the users in developing their own GPs from scratch giving them an in-depth knowledge of what goes on under the hood. The module covers GP inference using maximum likelihood estimation (MLE) and gives examples of 1D (dummy) spatial data.			arxiv	[]	204.0
455	Multiscale finite element calculations in Python using SfePy	Robert Cimrman	2018-10-01 12:41:29	http://arxiv.org/abs/1810.00674v1	SfePy (Simple finite elements in Python) is a software for solving various kinds of problems described by partial differential equations in one, two or three spatial dimensions by the finite element method. Its source code is mostly (85\%) Python and relies on fast vectorized operations provided by the NumPy package. For a particular problem two interfaces can be used: a declarative application programming interface (API), where problem description/definition files (Python modules) are used to define a calculation, and an imperative API, that can be used for interactive commands, or in scripts and libraries. After outlining the SfePy package development, the paper introduces its implementation, structure and general features. The components for defining a partial differential equation are described using an example of a simple heat conduction problem. Specifically, the declarative API of SfePy is presented in the example. To illustrate one of SfePy's main assets, the framework for implementing complex multiscale models based on the theory of homogenization, an example of a two-scale piezoelastic model is presented, showing both the mathematical description of the problem and the corresponding code.			arxiv	['Vladim√≠r Luke≈°', 'Eduard Rohan']	205.0
456	PyOD: A Python Toolbox for Scalable Outlier Detection	Yue Zhao	2019-01-06 18:29:35	http://arxiv.org/abs/1901.01588v2	PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.			arxiv	['Zain Nasrullah', 'Zheng Li']	206.0
457	DLTPy: Deep Learning Type Inference of Python Function Signatures using Natural Language Context	Casper Boone	2019-12-02 10:55:40	http://arxiv.org/abs/1912.00680v1	Due to the rise of machine learning, Python is an increasingly popular programming language. Python, however, is dynamically typed. Dynamic typing has shown to have drawbacks when a project grows, while at the same time it improves developer productivity. To have the benefits of static typing, combined with high developer productivity, types need to be inferred. In this paper, we present DLTPy: a deep learning type inference solution for the prediction of types in function signatures based on the natural language context (identifier names, comments and return expressions) of a function. We found that DLTPy is effective and has a top-3 F1-score of 91.6%. This means that in most of the cases the correct type is within the top-3 predictions. We conclude that natural language contained in comments and return expressions are beneficial to predicting types more accurately. DLTPy does not significantly outperform or underperform the previous work NL2Type for Javascript, but does show that similar prediction is possible for Python.			arxiv	['Niels de Bruin', 'Arjan Langerak', 'Fabian Stelmach']	207.0
458	CASA 6: Modular Integration in Python	Ryan Raba	2019-12-19 18:09:59	http://arxiv.org/abs/1912.09439v1	CASA, the Common Astronomy Software Applications, is the primary data processing software for the Atacama Large Millimeter/submillimeter Array (ALMA) and the Karl G. Jansky Very Large Array (VLA), and is often used also for other radio telescopes. CASA has always been distributed as a single, integrated application, including a Python interpreter and all the libraries, packages and modules. As part of the ongoing development of CASA 6, and the switch from Python 2 to 3, CASA will provide greater flexibility for users to integrate CASA into existing Python workflows by using a modular architecture and standard pip wheel installation. These proceedings of the 2019 Astronomical Data Analysis Software & Systems (ADASS) conference will give an overview of the CASA 6 project.			arxiv	['Darrell Schiebel', 'Bjorn Emonts', 'Robert Garwood', 'Federico Montesino Pouzols', 'Sandra Castro', 'C. Enrique Garcia-Dabo', 'David Mehringer', 'Ville Suoranta']	208.0
459	Programing Using High Level Design With Python and FORTRAN: A Study Case in Astrophysics	Eduardo dos Santos Pereira	2012-07-16 12:50:30	http://arxiv.org/abs/1207.3658v1	In this work, we present a short review about the high level design methodology (HLDM), that is based on the use of very high level (VHL) programing language as main, and the use of the intermediate level (IL) language only for the critical processing time. The languages used are Python (VHL) and FORTRAN (IL). Moreover, this methodology, making use of the oriented object programing (OOP), permits to produce a readable, portable and reusable code. Also is presented the concept of computational framework, that naturally appears from the OOP paradigm. As an example, we present the framework called PYGRAWC (Python framework for Gravitational Waves from Cosmological origin). Even more, we show that the use of HLDM with Python and FORTRAN produces a powerful tool for solving astrophysical problems.			arxiv	['Oswaldo D. Miranda']	209.0
460	Parallel Astronomical Data Processing with Python: Recipes for multicore machines	Navtej Singh	2013-06-03 20:00:04	http://arxiv.org/abs/1306.0573v1	High performance computing has been used in various fields of astrophysical research. But most of it is implemented on massively parallel systems (supercomputers) or graphical processing unit clusters. With the advent of multicore processors in the last decade, many serial software codes have been re-implemented in parallel mode to utilize the full potential of these processors. In this paper, we propose parallel processing recipes for multicore machines for astronomical data processing. The target audience are astronomers who are using Python as their preferred scripting language and who may be using PyRAF/IRAF for data processing. Three problems of varied complexity were benchmarked on three different types of multicore processors to demonstrate the benefits, in terms of execution time, of parallelizing data processing tasks. The native multiprocessing module available in Python makes it a relatively trivial task to implement the parallel code. We have also compared the three multiprocessing approaches - Pool/Map, Process/Queue, and Parallel Python. Our test codes are freely available and can be downloaded from our website.			arxiv	['Lisa-Marie Browne', 'Ray Butler']	210.0
461	Pycobra: A Python Toolbox for Ensemble Learning and Visualisation	Benjamin Guedj	2017-04-25 14:05:34	http://arxiv.org/abs/1707.00558v3	We introduce \texttt{pycobra}, a Python library devoted to ensemble learning (regression and classification) and visualisation. Its main assets are the implementation of several ensemble learning algorithms, a flexible and generic interface to compare and blend any existing machine learning algorithm available in Python libraries (as long as a \texttt{predict} method is given), and visualisation tools such as Voronoi tessellations. \texttt{pycobra} is fully \texttt{scikit-learn} compatible and is released under the MIT open-source license. \texttt{pycobra} can be downloaded from the Python Package Index (PyPi) and Machine Learning Open Source Software (MLOSS). The current version (along with Jupyter notebooks, extensive documentation, and continuous integration tests) is available at \href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra} and official documentation website is \href{https://modal.lille.inria.fr/pycobra}{https://modal.lille.inria.fr/pycobra}.			arxiv	['Bhargav Srinivasa Desikan']	211.0
462	Python Non-Uniform Fast Fourier Transform (PyNUFFT): multi-dimensional non-Cartesian image reconstruction package for heterogeneous platforms and applications to MRI	Jyh-Miin Lin	2017-10-09 17:12:46	http://arxiv.org/abs/1710.03197v1	This paper reports the development of a Python Non-Uniform Fast Fourier Transform (PyNUFFT) package, which accelerates non-Cartesian image reconstruction on heterogeneous platforms. Scientific computing with Python encompasses a mature and integrated environment. The NUFFT algorithm has been extensively used for non-Cartesian image reconstruction but previously there was no native Python NUFFT library. The current PyNUFFT software enables multi-dimensional NUFFT on heterogeneous platforms. The PyNUFFT also provides several solvers, including the conjugate gradient method, $\ell$1 total-variation regularized ordinary least square (L1TV-OLS) and $\ell$1 total-variation regularized least absolute deviation (L1TV-LAD). Metaprogramming libraries were employed to accelerate PyNUFFT. The PyNUFFT package has been tested on multi-core CPU and GPU, with acceleration factors of 6.3 - 9.5$\times$ on a 32 thread CPU platform and 5.4 - 13$\times$ on the GPU.			arxiv	[]	212.0
463	MPDAF - A Python package for the analysis of VLT/MUSE data	Laure Piqueras	2017-10-10 13:03:12	http://arxiv.org/abs/1710.03554v1	MUSE (Multi Unit Spectroscopic Explorer) is an integral-field spectrograph mounted on the Very Large Telescope (VLT) in Chile and made available to the European community since October 2014. The Centre de Recherche Astrophysique de Lyon has developed a dedicated software to help MUSE users analyze the reduced data. In this paper we introduce MPDAF, the MUSE Python Data Analysis Framework, based on several well-known Python libraries (Numpy, Scipy, Matplotlib, Astropy) which offers new tools to manipulate MUSE-specific data. We present different examples showing how this Python package may be useful for MUSE data analysis.			arxiv	['Simon Conseil', 'Martin Shepherd', 'Roland Bacon', 'Floriane Leclercq', 'Johan Richard']	213.0
464	On the Worst-Case Complexity of TimSort	Nicolas Auger	2018-05-22 14:27:38	http://arxiv.org/abs/1805.08612v3	TimSort is an intriguing sorting algorithm designed in 2002 for Python, whose worst-case complexity was announced, but not proved until our recent preprint. In fact, there are two slightly different versions of TimSort that are currently implemented in Python and in Java respectively. We propose a pedagogical and insightful proof that the Python version runs in $\mathcal{O}(n\log n)$. The approach we use in the analysis also applies to the Java version, although not without very involved technical details. As a byproduct of our study, we uncover a bug in the Java implementation that can cause the sorting method to fail during the execution. We also give a proof that Python's TimSort running time is in $\mathcal{O}(n + n\log \rho)$, where $\rho$ is the number of runs (i.e. maximal monotonic sequences), which is quite a natural parameter here and part of the explanation for the good behavior of TimSort on partially sorted inputs.			arxiv	['Vincent Jug√©', 'Cyril Nicaud', 'Carine Pivoteau']	214.0
465	unyt: Handle, manipulate, and convert data with units in Python	Nathan J. Goldbaum	2018-06-06 20:43:01	http://arxiv.org/abs/1806.02417v3	Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code.			arxiv	['John A. ZuHone', 'Matthew J. Turk', 'Kacper Kowalik', 'Anna L. Rosen']	215.0
466	Pykg2vec: A Python Library for Knowledge Graph Embedding	Shih Yuan Yu	2019-06-04 04:22:32	http://arxiv.org/abs/1906.04239v1	Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at https://github.com/Sujit-O/pykg2vec.			arxiv	['Sujit Rokka Chhetri', 'Arquimedes Canedo', 'Palash Goyal', 'Mohammad Abdullah Al Faruque']	216.0
467	AutoGMM: Automatic and Hierarchical Gaussian Mixture Modeling in Python	Thomas L. Athey	2019-09-06 01:45:27	http://arxiv.org/abs/1909.02688v5	Background: Gaussian mixture modeling is a fundamental tool in clustering, as well as discriminant analysis and semiparametric density estimation. However, estimating the optimal model for any given number of components is an NP-hard problem, and estimating the number of components is in some respects an even harder problem. Findings: In R, a popular package called mclust addresses both of these problems. However, Python has lacked such a package. We therefore introduce AutoGMM, a Python algorithm for automatic Gaussian mixture modeling, and its hierarchical version, HGMM. AutoGMM builds upon scikit-learn's AgglomerativeClustering and GaussianMixture classes, with certain modifications to make the results more stable. Empirically, on several different applications, AutoGMM performs approximately as well as mclust, and sometimes better. Conclusions: AutoMM, a freely available Python package, enables efficient Gaussian mixture modeling by automatically selecting the initialization, number of clusters and covariance constraints.			arxiv	['Tingshan Liu', 'Benjamin D. Pedigo', 'Joshua T. Vogelstein']	217.0
468	Some remarks on the performance of Matlab, Python and Octave in simulating dynamical systems	P. F. S. Guedes	2019-10-14 12:59:45	http://arxiv.org/abs/1910.06117v1	Matlab has been considered as a leader computational platform for many engineering fields. Well documented and reliable, Matlab presents as a great advantage its ability to increase the user productivity. However, Python and Octave are among some of the languages that have challenged Matlab. Octave and Python are well known examples of high-level scripting languages, with a great advantage of being open source software. The novelty of this paper is devoted to offer a comparison among these tree languages in the simulation of dynamical systems. We have applied the lower bound error to estimate the error of simulation. The comparison was performed with the chaotic systems Duffing-Ueda oscillator and the Chua's circuit, both identified with polynomial NARMAX. Octave presents the best reliable outcome. Nevertheless, Matlab needs the lowest time to undertake the same activity. Python has presented the worse result for the stop simulation criterion.			arxiv	['E. G. Nepomuceno']	218.0
469	GIMP-ML: Python Plugins for using Computer Vision Models in GIMP	Kritik Soman	2020-04-27 18:00:37	http://arxiv.org/abs/2004.13060v3	This paper introduces GIMP-ML v1.1, a set of Python plugins for the widely popular GNU Image Manipulation Program (GIMP). It enables the use of recent advances in computer vision to the conventional image editing pipeline. Applications from deep learning such as monocular depth estimation, semantic segmentation, mask generative adversarial networks, image super-resolution, de-noising, de-hazing, matting, enlightening and coloring have been incorporated with GIMP through Python-based plugins. Additionally, operations on images such as k-means based color clustering have also been added. GIMP-ML relies on standard Python packages such as numpy, pytorch, open-cv, scipy. Apart from these, several image manipulation techniques using these plugins have been compiled and demonstrated in the YouTube channel (https://youtube.com/user/kritiksoman) with the objective of demonstrating the use-cases for machine learning based image modification. In addition, GIMP-ML also aims to bring the benefits of using deep learning networks used for computer vision tasks to routine image processing workflows. The code and installation procedure for configuring these plugins is available at https://github.com/kritiksoman/GIMP-ML.			arxiv	[]	219.0
470	Automated Unit Test Generation for Python	Stephan Lukasczyk	2020-07-28 08:12:23	http://arxiv.org/abs/2007.14049v1	Automated unit test generation is an established research field, and mature test generation tools exist for statically typed programming languages such as Java. It is, however, substantially more difficult to automatically generate supportive tests for dynamically typed programming languages such as Python, due to the lack of type information and the dynamic nature of the language. In this paper, we describe a foray into the problem of unit test generation for dynamically typed languages. We introduce Pynguin, an automated unit test generation framework for Python. Using Pynguin, we aim to empirically shed light on two central questions: (1) Do well-established search-based test generation methods, previously evaluated only on statically typed languages, generalise to dynamically typed languages? (2) What is the influence of incomplete type information and dynamic typing on the problem of automated test generation? Our experiments confirm that evolutionary algorithms can outperform random test generation also in the context of Python, and can even alleviate the problem of absent type information to some degree. However, our results demonstrate that dynamic typing nevertheless poses a fundamental issue for test generation, suggesting future work on integrating type inference.			arxiv	['Florian Kroi√ü', 'Gordon Fraser']	220.0
471	PyMT5: multi-mode translation of natural language and Python code with transformers	Colin B. Clement	2020-10-07 04:10:58	http://arxiv.org/abs/2010.03150v1	Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.			arxiv	['Dawn Drain', 'Jonathan Timcheck', 'Alexey Svyatkovskiy', 'Neel Sundaresan']	221.0
472	Advanced Python Performance Monitoring with Score-P	Andreas Gocht	2020-10-29 09:35:50	http://arxiv.org/abs/2010.15444v2	Within the last years, Python became more prominent in the scientific community and is now used for simulations, machine learning, and data analysis. All these tasks profit from additional compute power offered by parallelism and offloading. In the domain of High Performance Computing (HPC), we can look back to decades of experience exploiting different levels of parallelism on the core, node or inter-node level, as well as utilising accelerators. By using performance analysis tools to investigate all these levels of parallelism, we can tune applications for unprecedented performance. Unfortunately, standard Python performance analysis tools cannot cope with highly parallel programs. Since the development of such software is complex and error-prone, we demonstrate an easy-to-use solution based on an existing tool infrastructure for performance analysis. In this paper, we describe how to apply the established instrumentation framework \scorep to trace Python applications. We finish with a study of the overhead that users can expect for instrumenting their applications.			arxiv	['Robert Sch√∂ne', 'Jan Frenzel']	222.0
473	I Know What You Imported Last Summer: A study of security threats in thePython ecosystem	Aadesh Bagmar	2021-02-11 22:46:17	http://arxiv.org/abs/2102.06301v1	The popularity of Python has risen rapidly over the past 15 years. It is a major language in some of the most exciting technologies today. This popularity has led to a large ecosystem of third-party packages available via the pip package registry which hosts more than 200,000 packages. These third-party packages can be reused by simply importing the package after installing using package managers like pip. The ease of reuse of third-party software comes with security risks putting millions of users in danger. In this project, we study the ecosystem to analyze this threat. The mature ecosystem of Python has multiple weak spots that we highlight in our project. First, we demonstrate how trivial it is to exploit the Python ecosystem. Then, we systematically analyze dependencies amongst packages, maintainers, and publicly reported security issues. Most attacks are possible only if users install malicious packages. We thus try to analyze and evaluate different methods used by attackers to force incorrect downloads. We quantify your ideas by estimating the potential threat that can be caused by exploiting a popular Python package. We also discuss methods used in the industry to defend against such attacks			arxiv	['Josiah Wedgwood', 'Dave Levin', 'Jim Purtilo']	223.0
474	Social Network Analysis: From Graph Theory to Applications with Python	Dmitri Goldenberg	2021-02-05 18:46:02	http://arxiv.org/abs/2102.10014v1	Social network analysis is the process of investigating social structures through the use of networks and graph theory. It combines a variety of techniques for analyzing the structure of social networks as well as theories that aim at explaining the underlying dynamics and patterns observed in these structures. It is an inherently interdisciplinary field which originally emerged from the fields of social psychology, statistics and graph theory. This talk will covers the theory of social network analysis, with a short introduction to graph theory and information spread. Then we will deep dive into Python code with NetworkX to get a better understanding of the network components, followed-up by constructing and implying social networks from real Pandas and textual datasets. Finally we will go over code examples of practical use-cases such as visualization with matplotlib, social-centrality analysis and influence maximization for information spread.			arxiv	[]	224.0
475	ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference	Amir M. Mir	2021-04-10 08:10:06	http://arxiv.org/abs/2104.04706v1	In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.			arxiv	['Evaldas Latoskinas', 'Georgios Gousios']	225.0
476	Catalogs of C and Python Antipatterns by CS1 Students	Yorah Bosse	2021-04-02 02:12:41	http://arxiv.org/abs/2104.12542v1	Understanding students' programming misconceptions is critical. Doing so depends on identifying the reasons why students make errors when learning a new programming language. Knowing the misconceptions can help students to improve their reflection about their mistakes and also help instructors to design better teaching strategies. In this technical report, we propose catalogs of antipatterns for two programming languages: C and Python. To accomplish this, we analyzed the codes of 166 CS1 engineering students when they were coding solutions to programming exercises. In our results, we catalog 41 CS1 antipatterns from 95 cataloged misconceptions in C and Python. These antipatterns were separated into three catalogs: C, Python, and antipatterns found in code using both programming languages. For each antipattern, we present code examples, students' solutions (if they are present), a possible solution to avoid the antipattern, among other information.			arxiv	['Igor Scaliante Wiese', 'Marco Aur√©lio Graciotto Silva', 'Nelson Lago', 'Le√¥nidas de Oliveira Brand√£o', 'David Redmiles', 'Fabio Kon', 'Marco A. Gerosa']	226.0
477	GGCHEMPY: A pure Python-based gas-grain chemical code for efficient simulation of interstellar chemistry	Jixing Ge	2021-10-21 13:05:49	http://arxiv.org/abs/2110.11117v1	In this paper, we present a new gas-grain chemical code for interstellar clouds written in pure Python (GGCHEMPY). By combining with the high-performance Python compiler Numba, GGCHEMPY is as efficient as the Fortran-based version. With the Python features, flexible computational workflows and extensions become possible. As a showcase, GGCHEMPY is applied to study the general effects of three-dimensional projection on molecular distributions using a two-core system which can be easily extended for more complex cases. By comparing the molecular distribution differences between two overlapping cores and two merging cores, we summarized the typical chemical differences such as, N2H+, HC3N, C2S, H2CO, HCN and C2H, which can be used to interpret 3-D structures in molecular clouds.			arxiv	[]	227.0
478	Performance Evaluation of Python Parallel Programming Models: Charm4Py and mpi4py	Zane Fink	2021-11-08 23:32:39	http://arxiv.org/abs/2111.04872v2	Python is rapidly becoming the lingua franca of machine learning and scientific computing. With the broad use of frameworks such as Numpy, SciPy, and TensorFlow, scientific computing and machine learning are seeing a productivity boost on systems without a requisite loss in performance. While high-performance libraries often provide adequate performance within a node, distributed computing is required to scale Python across nodes and make it genuinely competitive in large-scale high-performance computing. Many frameworks, such as Charm4Py, DaCe, Dask, Legate Numpy, mpi4py, and Ray, scale Python across nodes. However, little is known about these frameworks' relative strengths and weaknesses, leaving practitioners and scientists without enough information about which frameworks are suitable for their requirements. In this paper, we seek to narrow this knowledge gap by studying the relative performance of two such frameworks: Charm4Py and mpi4py. We perform a comparative performance analysis of Charm4Py and mpi4py using CPU and GPU-based microbenchmarks other representative mini-apps for scientific computing.			arxiv	['Simeng Liu', 'Jaemin Choi', 'Matthias Diener', 'Laxmikant V. Kale']	228.0
479	PyHHMM: A Python Library for Heterogeneous Hidden Markov Models	Fernando Moreno-Pino	2022-01-12 07:32:36	http://arxiv.org/abs/2201.06968v1	We introduce PyHHMM, an object-oriented open-source Python implementation of Heterogeneous-Hidden Markov Models (HHMMs). In addition to HMM's basic core functionalities, such as different initialization algorithms and classical observations models, i.e., continuous and multinoulli, PyHHMM distinctively emphasizes features not supported in similar available frameworks: a heterogeneous observation model, missing data inference, different model order selection criterias, and semi-supervised training. These characteristics result in a feature-rich implementation for researchers working with sequential data. PyHHMM relies on the numpy, scipy, scikit-learn, and seaborn Python packages, and is distributed under the Apache-2.0 License. PyHHMM's source code is publicly available on Github (https://github.com/fmorenopino/HeterogeneousHMM) to facilitate adoptions and future contributions. A detailed documentation (https://pyhhmm.readthedocs.io/en/latest), which covers examples of use and models' theoretical explanation, is available. The package can be installed through the Python Package Index (PyPI), via 'pip install pyhhmm'.			arxiv	['Emese S√ºkei', 'Pablo M. Olmos', 'Antonio Art√©s-Rodr√≠guez']	229.0
480	SBMLDiagrams: A python package to process and visualize SBML layout and render	Jin Xu	2022-04-26 22:01:35	http://arxiv.org/abs/2204.12611v2	Summary: The Systems Biology Markup Language (SBML) is an extensible standard format for exchanging biochemical models. One of the extensions for SBML is the SBML Layout and Render package. This allows modelers to describe a biochemical model as a pathway diagram. However, up to now there has been little support to help users easily add and retrieve such information from SBML. In this application note, we describe a new Python package called SBMLDiagrams. This package allows a user to add layout and render information or retrieve it using a straightforward Python API. The package uses skia-python to support the rendering of the diagrams, allowing export to commons formats such as PNG or PDF. Availability: SBMLDiagrams is publicly available and licensed under the liberal MIT open-source license. The package is available for all major platforms. The source code has been deposited at GitHub (github.com/sys-bio/SBMLDiagrams). Users can install the package using the standard pip installation mechanism: pip install SBMLDiagrams. Contact: hsauro@uw.edu.			arxiv	['Jessie Jiang', 'Herbert M. Sauro']	230.0
481	LabVIEW is faster and C is economical interfacing tool for UCT automation	Ankur Kumar	2022-05-17 12:00:25	http://arxiv.org/abs/2205.08260v1	An in-house developed 2D ultrasound computerized Tomography system is fully automated. Performance analysis of instrument and software interfacing soft tools, namely the LabVIEW, MATLAB, C, and Python, is presented. The instrument interfacing algorithms, hardware control algorithms, signal processing, and analysis codes are written using above mentioned soft tool platforms. Total of eight performance indices are used to compare the ease of (a) realtime control of electromechanical assembly, (b) sensors, instruments integration, (c) synchronized data acquisition, and (d) simultaneous raw data processing. It is found that C utilizes the least processing power and performs a lower number of processes to perform the same task. In runtime analysis (data acquisition and realtime control), LabVIEW performs best, taking 365.69s in comparison to MATLAB (623.83s), Python ( 1505.54s), and C (1252.03s) to complete the experiment. Python performs better in establishing faster interfacing and minimum RAM usage. LabVIEW is recommended for its fast process execution. C is recommended for the most economical implementation. Python is recommended for complex system automation having a very large number of components involved. This article provides a methodology to select optimal soft tools for instrument automation-related aspects.			arxiv	['Mayank Goswami']	231.0
482	atoMEC: An open-source average-atom Python code	Timothy J. Callow	2022-06-02 14:41:22	http://arxiv.org/abs/2206.01074v2	Average-atom models are an important tool in studying matter under extreme conditions, such as those conditions experienced in planetary cores, brown and white dwarfs, and during inertial confinement fusion. In the right context, average-atom models can yield results with similar accuracy to simulations which require orders of magnitude more computing time, and thus can greatly reduce financial and environmental costs. Unfortunately, due to the wide range of possible models and approximations, and the lack of open-source codes, average-atom models can at times appear inaccessible. In this paper, we present our open-source average-atom code, atoMEC. We explain the aims and structure of atoMEC to illuminate the different stages and options in an average-atom calculation, and to facilitate community contributions. We also discuss the use of various open-source Python packages in atoMEC, which have expedited its development.			arxiv	['Daniel Kotik', 'Eli Kraisler', 'Attila Cangi']	232.0
483	Pychastic: Precise Brownian Dynamics using Taylor-It≈ç integrators in Python	Radost Waszkiewicz	2022-09-09 14:43:37	http://arxiv.org/abs/2209.04332v2	In the last decade, Python-powered physics simulations ecosystem has been growing steadily, allowing greater interoperability, and becoming an important tool in numerical exploration of physical phenomena, particularly in soft matter systems. Driven by the need for fast and precise numerical integration in colloidal dynamics, here we formulate the problem of Brownian Dynamics (BD) in a mathematically consistent formalism of the It\=o calculus, and develop a Python package to assist numerical computations. We show that, thanks to the automatic differentiation packages, the classical truncated Taylor-It\=o integrators can be implemented without the burden of computing the derivatives of the coefficient functions beforehand. Furthermore, we show how to circumvent the difficulties of BD simulations such as calculations of the divergence of the mobility tensor in the diffusion equation and discontinuous trajectories encountered when working with dynamics on $S^2$ and $SO(3)$. The resulting Python package, Pychastic, is capable of performing BD simulations including hydrodynamic interactions at speeds comparable to dedicated implementations in lower-level programming languages, but with a much simpler end-user interface.			arxiv	['Maciej Bartczak', 'Kamil Kolasa', 'Maciej Lisicki']	233.0
484	Repairing Bugs in Python Assignments Using Large Language Models	Jialu Zhang	2022-09-29 15:41:17	http://arxiv.org/abs/2209.14876v1	Students often make mistakes on their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex, to build an APR system -- MMAPR -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate MMAPR on 286 real student programs and compare to a baseline built by combining a state-of-the-art Python syntax repair engine, BIFI, and state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that MMAPR can fix more programs and produce smaller patches on average.			arxiv	['Jos√© Cambronero', 'Sumit Gulwani', 'Vu Le', 'Ruzica Piskac', 'Gustavo Soares', 'Gust Verbruggen']	234.0
485	Comparing neural network training performance between Elixir and Python	Lucas C. Tavano	2022-10-25 11:57:14	http://arxiv.org/abs/2210.13945v1	With a wide range of libraries focused on the machine learning market, such as TensorFlow, NumPy, Pandas, Keras, and others, Python has made a name for itself as one of the main programming languages. In February 2021, Jos\'e Valim and Sean Moriarity published the first version of the Numerical Elixir (Nx) library, a library for tensor operations written in Elixir. Nx aims to allow the language be a good choice for GPU-intensive operations. This work aims to compare the results of Python and Elixir on training convolutional neural networks (CNN) using MNIST and CIFAR-10 datasets, concluding that Python achieved overall better results, and that Elixir is already a viable alternative.			arxiv	['Lucas K. Amin', 'Adolfo Gustavo Serra-Seca-Neto']	235.0
486	Excel Spreadsheet Analyzer	Amir Nassereldine	2022-11-01 20:35:13	http://arxiv.org/abs/2211.06333v1	Spreadsheets are widely used in various fields to do large numerical analysis. While several companies have relied on spreadsheets for decades, data scientists are going in the direction of using scientific programming languages such as python to do their data analysis due to the support, community, and vast amount of libraries. While using python to analyze a company's spreadsheets, some information such as the formulas and dependencies of a cell are lost. We propose a tool that creates an abstract intermediate representation (AIR) of a spreadsheet. This representation facilitates the transfer from spreadsheets into scientific programming languages while preserving inter-dependency information about data. In addition to that, we build a python library on top of our tool to perform some data analysis in python.			arxiv	['Patrick Chen', 'Jinjun Xiong']	236.0
487	Visualizing Contributor Code Competency for PyPI Libraries: Preliminary Results	Indira Febriyanti	2022-12-04 17:48:52	http://arxiv.org/abs/2212.01882v1	Python is known to be used by beginners to professional programmers. Python provides functionality to its community of users through PyPI libraries, which allows developers to reuse functionalities to an application. However, it is unknown the extent to which these PyPI libraries require proficient code in their implementation. We conjecture that PyPI contributors may decide to implement more advanced Pythonic code, or stick with more basic Python code. Are complex codes only committed by few contributors, or only to specific files? The new idea in this paper is to confirm who and where complex code is implemented. Hence, we present a visualization to show the relationship between proficient code, contributors, and files. Analyzing four PyPI projects, we are able to explore which files contain more elegant code, and which contributors committed to these files. Our results show that most files contain more basic competency files, and that not every contributor contributes competent code. We show how~our visualization is able to summarize such information, and opens up different possibilities for understanding how to make elegant contributions.			arxiv	['Raula Gaikovina Kula', 'Ruksit Rojpaisarnkit', 'Kanchanok Kannee', 'Yusuf Sulistyo Nugroho', 'Kenichi Matsumoto']	237.0
488	Optimization of the Analysis of Vital Events Using Threads	Rubi Melania Coasaca Callacondo	2023-01-04 17:16:07	http://arxiv.org/abs/2301.01710v1	Objective: Optimize the time of data analysis of Vital Events (births, deaths and marriages) using Threads. Methodology: A code was created in python without threads and another with threads, after He performed 5 tests with a single attribute and with 1,2,3,4,5,6,7,8 and 9 attributes this was done in both codes with the same amount of data, to know if the python code with threads is optimal when parsing Vital Facts data. Results: The python code with Threads turned out to be the most optimal since it optimized the compilation time of the 5 tests with 1 attribute by 16attributes was obtained as a result that the more attributes you group, the more effective the use of threads. Conclusion: Python code with threads is more optimal than code without threads Therefore, it is concluded that the implementation of threads is recommended in the analysis of data in similar works.			arxiv	['Grylia Yaneth Chata Iscarra', 'Fred Torres-Cruz']	238.0
489	Photochemical and RadiatiOn Transport model for Extensive USe (PROTEUS)	Yuki Nakamura	2023-01-06 08:14:44	http://arxiv.org/abs/2301.02415v1	We introduce a new flexible one-dimensional photochemical model named Photochemical and RadiatiOn Transport model for Extensive USe (PROTEUS), which consists of a Python graphical user interface (GUI) program and Fortran 90 modules. PROTEUS is designed for adaptability to many planetary atmospheres, for flexibility to deal with thousands of or more chemical reactions with high efficiency, and for intuitive operation with GUI. Chemical reactions can be easily implemented into the Python GUI program in a simple string format, and users can intuitively select a planet and chemical reactions on GUI. Chemical reactions selected on GUI are automatically analyzed by string parsing functions in the Python GUI program, then applied to the Fortran 90 modules to simulate with the selected chemical reactions on a selected planet. PROTEUS can significantly save the time for those who need to develop a new photochemical model; users just need to write chemical reactions in the Python GUI program and just select them on GUI to run a new photochemical model.			arxiv	['Naoki Terada', 'Shungo Koyama', 'Tatsuya Yoshida', 'Hiroki Karyu', 'Kaori Terada', 'Takeshi Kuroda', 'Arihiro Kamada', 'Isao Murata', 'Shotaro Sakai', 'Yuhei Suzuki', 'Mirai Kobayashi', 'Fran√ßois Leblanc']	239.0
490	Landscape of High-performance Python to Develop Data Science and Machine Learning Applications	Oscar Castro	2023-02-07 07:56:21	http://arxiv.org/abs/2302.03307v2	Python has become the prime language for application development in the Data Science and Machine Learning domains. However, data scientists are not necessarily experienced programmers. While Python lets them quickly implement their algorithms, when moving at scale, computation efficiency becomes inevitable. Thus, harnessing high-performance devices such as multicore processors and Graphical Processing Units (GPUs) to their potential is generally not trivial. The present narrative survey was thought as a reference document for such practitioners to help them make their way in the wealth of tools and techniques available for the Python language. Our document revolves around user scenarios, which are meant to cover most situations they may face. We believe that this document may also be of practical use to tool developers, who may use our work to identify potential lacks in existing tools and help them motivate their contributions.			arxiv	['Pierrick Bruneau', 'Jean-S√©bastien Sottet', 'Dario Torregrossa']	240.0
491	PNet: A Python Library for Petri Net Modeling and Simulation	Zhu En Chay	2023-02-23 14:27:50	http://arxiv.org/abs/2302.12054v1	Petri Net is a formalism to describe changes between 2 or more states across discrete time and has been used to model many systems. We present PNet - a pure Python library for Petri Net modeling and simulation in Python programming language. The design of PNet focuses on reducing the learning curve needed to define a Petri Net by using a text-based language rather than programming constructs to define transition rules. Complex transition rules can be refined as regular Python functions. To demonstrate the simplicity of PNet, we present 2 examples - bread baking, and epidemiological models.			arxiv	['Bing Feng Goh', 'Maurice HT Ling']	241.0
492	Method Chaining Redux: An Empirical Study of Method Chaining in Java, Kotlin, and Python	Ali M. Keshk	2023-03-20 16:54:05	http://arxiv.org/abs/2303.11269v1	There are possible benefits and drawbacks to chaining methods together, as is often done in fluent APIs. A prior study investigated how Java developers chain methods in over 2.7k open-source projects. That study observed, for the dataset analyzed, that the use of method chaining in Java is popular and seems to be increasing over time. That study however was limited to a smaller sample of Java projects, and it is also not clear if the results generalize to other languages. In this work, we first replicate the prior results by building a similar dataset and our own analysis scripts. We then extend those results by analyzing a much larger dataset of 89k Java projects and generalizing to other programming languages by analyzing 26k Kotlin projects and 98k Python projects. The results show chaining is more popular in Java and Kotlin than Python, chaining use in Kotlin is not growing, and Python sees more use in non-testing code.			arxiv	['Robert Dyer']	242.0
493	Development of MC/DC: a performant, scalable, and portable Python-based Monte Carlo neutron transport code	Ilham Variansyah	2023-05-12 17:42:52	http://arxiv.org/abs/2305.07636v1	We discuss the current development of MC/DC (Monte Carlo Dynamic Code). MC/DC is primarily designed to serve as an exploratory Python-based MC transport code. However, it seeks to offer improved performance, massive scalability, and backend portability by leveraging Python code-generation libraries and implementing an innovative abstraction strategy and compilation scheme. Here, we verify MC/DC capabilities and perform an initial performance assessment. We found that MC/DC can run hundreds of times faster than its pure Python mode and about 2.5 times slower, but with comparable parallel scaling, than the high-performance MC code Shift for simple problems. Finally, to further exercise MC/DC's time-dependent MC transport capabilities, we propose a challenge problem based on the C5G7-TD benchmark model.			arxiv	['J. P. Morgan', 'Jordan Northrop', 'Kyle E. Niemeyer', 'Ryan G. McClarren']	243.0
494	Exponential Integrators for Phase-Field Equations using Pseudo-spectral Methods: A Python Implementation	Elvis do A. Soares	2023-05-15 20:27:18	http://arxiv.org/abs/2305.08998v1	In this paper, we implement exponential integrators, specifically Integrating Factor (IF) and Exponential Time Differencing (ETD) methods, using pseudo-spectral techniques to solve phase-field equations within a Python framework. These exponential integrators have showcased robust performance and accuracy when addressing stiff nonlinear partial differential equations. We compare these integrators to the well-known implicit-explicit (IMEX) Euler integrators used in phase-field modeling. The synergy between pseudo-spectral techniques and exponential integrators yields significant benefits for modeling intricate systems governed by phase-field dynamics, such as solidification processes and pattern formation. Our comprehensive Python implementation illustrates the effectiveness of this combined approach in solving phase-field model equations. The results obtained from this implementation highlight the accuracy and computational advantages of the ETD method compared to other numerical techniques.			arxiv	['Amaro G. Barreto Jr.', 'Frederico W. Tavares']	244.0
495	Pyqcm: An open-source Python library for quantum cluster methods	Th√©o N. Dionne	2023-05-29 22:37:01	http://arxiv.org/abs/2305.18643v2	Pyqcm is a Python/C++ library that implements a few quantum cluster methods with an exact diagonalization impurity solver. Quantum cluster methods are used in the study of strongly correlated electrons to provide an approximate solution to Hubbard-like models. The methods covered by this library are Cluster Perturbation Theory (CPT), the Variational Cluster Approach (VCA) and Cellular (or Cluster) Dynamical Mean Field Theory (CDMFT). The impurity solver (the technique used to compute the cluster's interacting Green function) is exact diagonalization from sparse matrices, using the Lanczos algorithm and variants thereof. The core library is written in C++ for performance, but the interface is in Python, for ease of use and inter-operability with the numerical Python ecosystem. The library is distributed under the GPL license.			arxiv	['Alexandre Foley', 'Mo√Øse Rousseau', 'David S√©n√©chal']	245.0
496	DeepOnto: A Python Package for Ontology Engineering with Deep Learning	Yuan He	2023-07-06 15:35:02	http://arxiv.org/abs/2307.03067v1	"Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more ""Pythonic"" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methodologies, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of Deeponto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI)."			arxiv	['Jiaoyan Chen', 'Hang Dong', 'Ian Horrocks', 'Carlo Allocca', 'Taehun Kim', 'Brahmananda Sapkota']	246.0
497	PyPartMC: A Pythonic interface to a particle-resolved, Monte Carlo aerosol simulation framework	Zachary D'Aquino	2023-08-03 21:10:44	http://arxiv.org/abs/2308.02052v3	PyPartMC is a Pythonic interface to PartMC, a stochastic, particle-resolved aerosol model implemented in Fortran. Both PyPartMC and PartMC are free, libre, and open-source. PyPartMC reduces the number of steps and mitigates the effort necessary to install and utilize the resources of PartMC. Without PyPartMC, setting up PartMC requires: working with UNIX shell, providing Fortran and C libraries, and performing standard Fortran and C source code configuration, compilation and linking. This can be challenging for those less experienced with computational research or those intending to use PartMC in environments where provision of UNIX tools is less straightforward (e.g., on Windows). PyPartMC offers a single-step installation/upgrade process of PartMC and all dependencies through the pip Python package manager on Linux, macOS, and Windows. This allows streamlined access to the unmodified and versioned Fortran internals of the PartMC codebase from both Python and other interoperable environments (e.g., Julia through PyCall). Consequently, users of PyPartMC can setup, run, process and visualize output of PartMC simulations using a single general-purpose programming language.			arxiv	['Sylwester Arabas', 'Jeffrey Curtis', 'Akshunna Vaishnav', 'Nicole Riemer', 'Matthew West']	247.0
498	The Janus System: Multi-paradigm Programming in Prolog and Python	Theresa Swift	2023-08-30 09:07:05	http://arxiv.org/abs/2308.15893v1	Python and Prolog express different programming paradigms, with different strengths. Python is wildly popular because it is well-structured, easy to use, and mixes well with thousands of scientific and machine learning programs written in C. Prolog's logic-based approach provides powerful reasoning capabilities, especially when combined with constraint evaluation, probabilistic reasoning, well-founded negation, and other advances. Both languages have commonalities as well: both are usually written in C, both are dynamically typed, and both use data structures based on a small number of recursive types. This paper describes the design and implementation of Janus, a system that tightly combines Prolog and Python into a single process. Janus bi-translates data structures and offers performance of many hundreds of thousands of round-trip inter-language calls per second. Although Janus is still new, it has been used in commercial applications including natural language processing, visual query answering and robotic automation. Janus was developed for XSB, but porting Janus code to a second Prolog has been straightforward, indicating that Janus is a tool that other Prologs may easily adopt.			arxiv	['Carl Andersen']	248.0
499	The Calysto Scheme Project	Douglas S. Blank	2023-10-16 23:41:21	http://arxiv.org/abs/2310.10886v1	Calysto Scheme is written in Scheme in Continuation-Passing Style, and converted through a series of correctness-preserving program transformations into Python. It has support for standard Scheme functionality, including call/cc, as well as syntactic extensions, a nondeterministic operator for automatic backtracking, and many extensions to allow Python interoperation. Because of its Python foundation, it can take advantage of modern Python libraries, including those for machine learning and other pedagogical contexts. Although Calysto Scheme was developed with educational purposes in mind, it has proven to be generally useful due to its simplicity and ease of installation. It has been integrated into the Jupyter Notebook ecosystem and used in the classroom to teach introductory Programming Languages with some interesting and unique twists.			arxiv	['James B. Marshall']	249.0
500	Docs for pypng library are missing on readthedocs	Unknown	2023-12-11 12:15:49	https://www.reddit.com/r/Python/comments/18ft88r/docs_for_pypng_library_are_missing_on_readthedocs/	This link https://pypng.readthedocs.io/en/latest/ gets a 404 and it's what I find on https://pypi.org/project/pypng/ project home page. Been that way for over a week. Not a fatal problem for me but bothersome and a little concerning.	0.0	t3_18ft88r	reddit		
501	sqlalchemy- processing the result set parallely- multiprocessing	Unknown	2023-12-10 17:35:22	https://www.reddit.com/r/Python/comments/18f8p9v/sqlalchemy_processing_the_result_set_parallely/	Hey folks, what would be the best way to process a result set parallely? Lemme pain the picture of the current setup. from settings import Session # this is basically the sessionmaker def run_it(): // create session session = Session() // fetch first 100 rows which are basically the events stored result_set = [] for event in result_set: try: // execute the event if its beyond expiry finally: session.commit() session.close() while True: run_it() time.sleep(3) I wanted to parallelize the for loop here. Obvious option is to use multiprocessing pool but Im not sure how that would work in case of db sessions? Is the session created initially shared or we are required to open a new session. Has anyone dealt with this kind of optimization at work perhaps? Whats the ideal way to go about it? Please do share any relevant readings if you have any. Edit: Adding more context: Exact use case would be separate out the load(x events) between the say 2 processes processing parallely, which just execute the event. Now for these 2 processes, I was thinking of creating 2 sessions each for them. But I also need to create a session to first fetch the events from db. So my doubt is , is this excessive creation of sessions? Whats the optimal way of dealing with this kinda of multi processing	6.0	t3_18f8p9v	reddit		
502	We've built Xplainable, a transparent (structured) Machine Learning Algorithm	Unknown	2023-12-11 05:01:40	https://www.reddit.com/r/Python/comments/18fmsnz/weve_built_xplainable_a_transparent_structured/	Just for some respite from the discussion of our soon-to-be AI overlords (LLMs), I'm one of the contributors to an open-source Python package, Xplainable ([https://github.com/xplainable/xplainable](https://github.com/xplainable/xplainable)). Xplainable is a structured machine learning algorithm that's inherently explainable, as opposed to being a post-hoc explainer (like SHAP or Lime). From our initial baseline testing, it's clear that Xplainable is competitive with XGBoost and LightGBM for classification metrics like AUC and F1. This is despite the challenge of direct comparisons due to varying feature engineering requirements. In regression tasks, Xplainable's performance ranges from surpassing Decision Trees to matching XGBoost in terms of MAE. We've also just released V1.1, which inherently handles NaN values, allowing you to fit it directly to your data without the need to drop or impute missing values. Here are some examples trained on Kaggle datasets if you want to investigate further: [Xplainable Tutorials](https://docs.xplainable.io/docs/category/tutorials). I've included the Altair html output so you can view the explainer plots in the docs webpage. I would greatly appreciate your feedback around any additions we should look to include in our roadmap!	0.0	t3_18fmsnz	reddit		
503	Monday Daily Thread: Project ideas!	Unknown	2023-12-11 00:00:11	https://www.reddit.com/r/Python/comments/18fh603/monday_daily_thread_project_ideas/	"# Weekly Thread: Project Ideas üí° Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you. ## How it Works: 1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced. 2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code. 3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration. ## Guidelines: * Clearly state the difficulty level. * Provide a brief description and, if possible, outline the tech stack. * Feel free to link to tutorials or resources that might help. # Example Submissions: ## Project Idea: Chatbot **Difficulty**: Intermediate **Tech Stack**: Python, NLP, Flask/FastAPI/Litestar **Description**: Create a chatbot that can answer FAQs for a website. **Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM) # Project Idea: Weather Dashboard **Difficulty**: Beginner **Tech Stack**: HTML, CSS, JavaScript, API **Description**: Build a dashboard that displays real-time weather information using a weather API. **Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8) ## Project Idea: File Organizer **Difficulty**: Beginner **Tech Stack**: Python, File I/O **Description**: Create a script that organizes files in a directory into sub-folders based on file type. **Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/) Let's help each other grow. Happy coding! üåü"	1.0	t3_18fh603	reddit		
504	MastoLine - Mastodon in your terminal!	Unknown	2023-12-10 20:00:36	https://www.reddit.com/r/Python/comments/18fbx6j/mastoline_mastodon_in_your_terminal/	Yesterday I started working on a little project that I later named [MastoLine](https://github.com/gamemake-eng/MastoLine). It's a simple mastodon client that allows you to view posts, reply, favourite, repost, and toot. You just need to sign into your account and it should be ready to go. This is only really useful for quickly looking at your timeline with minimal interaction for now, but I am hoping to add more functionality from either my ideas or user suggestions. The alpha is currently out. It is not up on Pypi but I hope I will be able to get it sort out after it gets to a state where I like it. [https://github.com/gamemake-eng/MastoLine/releases/tag/v0.0.1-rat-alpha](https://github.com/gamemake-eng/MastoLine/releases/tag/v0.0.1-rat-alpha)	1.0	t3_18fbx6j	reddit		
505	Botrax - Browsergame Bot	Unknown	2023-12-10 19:48:33	https://www.reddit.com/r/Python/comments/18fbnhb/botrax_browsergame_bot/	Hi all, I am by no means a good developer, but had the long dream to create a Bot myself. So after some development I decided to share it for educational purposes :) [https://github.com/BabyDonkeyCoding/botrax](https://github.com/BabyDonkeyCoding/botrax) Feel free to leave a start at the repo or even contribute!	1.0	t3_18fbnhb	reddit		
506	Livestream PS5 or PC Gameplay into OpenCV	Unknown	2023-12-10 15:23:45	https://www.reddit.com/r/Python/comments/18f5vyy/livestream_ps5_or_pc_gameplay_into_opencv/	I wrote this script after surprisingly finding very little existing code online about capturing windows. This script can be used to capture PlayStation 5 gameplay in real time into OpenCV for further image processing, or capture a game window on PC. Along the way I found out that graphics is more complicated than I thought. Windows does not make it particularly easy to capture windows that are hardware accelerated, and most existing libraries do not work when the window goes off screen or is obscured. My script works with both with good performance! This could be used to analyse the video for some event (using image/text detection), send a message, record some score, or even act as a master for a microcontroller robot (Arduino etc) operating the controller! [https://github.com/lorcan2440/View-PS5-Screen-Remote-Play](https://github.com/lorcan2440/View-PS5-Screen-Remote-Play) The repo contains a guide for how to set it up. If you'd like to use it and are having trouble just leave a comment here or raise an issue there and I'll try to help. I've also made a C++ implementation since most of the relevant functions are just calling from Windows DLLs so the conversion was easy. C++ will have slightly better performance but you need to set up OpenCV for C++ by yourself which is less easy than Python.	1.0	t3_18f5vyy	reddit		
507	GPT helped me build a python tool to work with the API	Unknown	2023-12-10 17:26:12	https://www.reddit.com/r/Python/comments/18f8i0y/gpt_helped_me_build_a_python_tool_to_work_with/	Hey folks, just dropping in to share a python tool I put together for working with the OpenAI API. It‚Äôs a command line tool that helps with managing assistants, threads, and files. This was an early attempt at learning python for me, and it was really fun working with GPT to help outline what I wanted the tool to do. It was a pretty big learning curve ‚Äî I have loads of experience in other programming languages, but less in Python, so it was so cool to just dive right in. Here‚Äôs what it can do: * **List Assistants**: See all your assistants ids quickly. * **Manage Threads**: Create and delete threads without hassle. * **Handle Files**: List, upload, and delete files simply. One heads-up: You‚Äôll need to set up your Python environment to use it, but the [OpenAI docs](https://platform.openai.com/docs/quickstart?context=python) cover that part. Thought this might make things a bit easier for anyone juggling with the API. Give it a whirl and let me know how it goes! &#x200B; https://preview.redd.it/88d7w1q15i5c1.png?width=1364&format=png&auto=webp&s=8e655a54dba1f4d1cca0d11b4c88cc2be9a60103 Download here on GitHub: [https://github.com/richarddas/OpenAI-Python-Tools](https://github.com/richarddas/OpenAI-Python-Tools) Would love to hear any feedback or suggestions you have!	0.0	t3_18f8i0y	reddit		
508	Kanban in the CLI with kanban-python	Unknown	2023-12-10 01:17:22	https://www.reddit.com/r/Python/comments/18es539/kanban_in_the_cli_with_kanbanpython/	HoHoHo again After some great feedback and feature requests I present to you the new version of `kanban-python` your kanban terminal app written in python: Code here: [https://github.com/Zaloog/kanban-python](https://github.com/Zaloog/kanban-python) Install with: ```bash python -m pip install kanban-python ``` during the course of the last month the following features were added: - moved to XDG Basedir Spec for config and data files - Directory Scanner to automatically scan e.g. .py files for '# TODO' Patterns and generate tasks out of that. - better options to change settings and show task details - a new report function to generate an overall report (.md file) and show a github-like contribution table Thanks for your support so far, and I hope kanban-python helps you boost your productivity. If you have feedback, feel free to reach out here, or create an issue. Have a great day	1.0	t3_18es539	reddit		
509	Trending on GitHub globally 3 days in a row: SuperDuperDB, a Python framework for integrating AI with major databases (making them super-duper)	Unknown	2023-12-09 11:55:11	https://www.reddit.com/r/Python/comments/18ec2p7/trending_on_github_globally_3_days_in_a_row/	It is for building AI (into your) apps easily without complex pipelines and make your database intelligent (including vector search), definitely check it out: https://github.com/SuperDuperDB/superduperdb	8.0	t3_18ec2p7	reddit		
510	Sunday Daily Thread: What's everyone working on this week?	Unknown	2023-12-10 00:00:10	https://www.reddit.com/r/Python/comments/18eqnxv/sunday_daily_thread_whats_everyone_working_on/	# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to! ## How it Works: 1. **Show & Tell**: Share your current projects, completed works, or future ideas. 2. **Discuss**: Get feedback, find collaborators, or just chat about your project. 3. **Inspire**: Your project might inspire someone else, just as you might get inspired here. ## Guidelines: * Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome. * Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here. ## Example Shares: 1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate! 2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better. 3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier! Let's build and grow together! Share your journey and learn from others. Happy coding! üåü	11.0	t3_18eqnxv	reddit		
511	I built a Reddit bot to create digest of top posts from your favorite subreddits!	from __future__ import 4.0	2023-12-09 15:46:14	https://www.reddit.com/r/Python/comments/18eg9vt/i_built_a_reddit_bot_to_create_digest_of_top/	"Hello r/Python! I was playing with PRAW (Python Reddit API Wrapper) and I built a bot which can create neat digests of posts from multiple subreddits. Reddit Daily Digest Bot is a Python-based Reddit bot that automates the compilation of top posts from specified subreddits into a daily digest. Users can customize sorting methods, time ranges, and the target subreddit for automated posting. The bot simplifies the process of staying updated on favorite subreddits by delivering a neatly formatted summary. The supported sorting methods are - top/controversial/hot/rising/new & the supported time ranges are - daily/weekly/monthly/yearly/all time. **Key Features :** * Can create daily digest of most popular posts from multiple subreddits. * Fetch top, hot, controversial, rising, new posts from multiple subreddits in the time frame you want. * Compile the fetched posts into a digest with post titles, links and author. * Post the digest to a designated subreddit with ease. For more information about the bot please check the readme on the [repository](https://github.com/ni5arga/Reddit-DailyDigest-Bot). **Examples :** ![img](zwmswe0tha5c1 ""The bot creating a digest of top posts of r/python in the year range "") [Daily digest of top posts for the day in r\/python](https://preview.redd.it/cvdg6io6ia5c1.png?width=698&format=png&auto=webp&s=edff337fde2f401a80f54d5f92a9c38f15ad7dee) Repository Link (GitHub) : [https://github.com/ni5arga/Reddit-DailyDigest-Bot](https://github.com/ni5arga/Reddit-DailyDigest-Bot) Feel free to star & fork. Any contributions and suggestions are always welcome! :)"	0.0	t3_18eg9vt	reddit		
512	The Dawn of Concurrency and Parallelism: Python 3.12 Starts to Support a Per-Interpreter GIL	Unknown	2023-12-09 00:14:52	https://www.reddit.com/r/Python/comments/18e0q7p/the_dawn_of_concurrency_and_parallelism_python/	> PEP 684 introduces a per-interpreter GIL, so that sub-interpreters may now be created with a unique GIL per interpreter. This allows Python programs to take full advantage of multiple CPU cores. This is currently only available through the C-API, though a Python API is anticipated for 3.13. ## Summary - Per-interpreter GIL with C-API is available in Python 3.12 - Python API is anticipated for Python 3.13 ## References - [A Per-Interpreter GIL section in _What‚Äôs New In Python 3.12_](https://docs.python.org/3.12/whatsnew/3.12.html#pep-684-a-per-interpreter-gil) - [PEP 686: A Per-Interpreter GIL](https://peps.python.org/pep-0684/) - [PEP 734: Multiple Interpreters in the Stdlib](https://peps.python.org/pep-0734/)	5.0	t3_18e0q7p	reddit		
513	GPL or Apache license for an upcoming PySide2 project?	Unknown	2023-12-09 09:24:35	https://www.reddit.com/r/Python/comments/18e9upe/gpl_or_apache_license_for_an_upcoming_pyside2/	Afternoon Folks, For my upcoming side project (for which `PySide` has been aptly chosen!), a desktop productivity app with features like milestone tracking, brainstorming, some motivational stuff, etc., I'm somewhat confused about the licensing part. I was decided on Apache 2.0 license so far as I like their focus on merit based process, plus they actually seem to create a ton of software as an organization, it's not just a license. I like the GPL philosophy too but I'm more of a utilitarian than philosopher and the GPL folks seem to be ever more preachy about Stallmanian ethics than about the technicality of coding programs and developing apps (where I'm more interested as a utilitarian/engineer). But it seems I may have to bite this thing and go with GPL here considering that though PySide2 itself is LGPL, it turns out that [some underlying core Qt components](https://opensource.stackexchange.com/a/9289/4160) are indeed still GPL licensed, these are addons such as **QtCharts** which I'm definitely going to need for displaying charts in my GUI. Unless there is a way to use matplotlib effectively with PySide2 which I don't know about? Being a utilitarian engineer, I'm a pragmatist too and in that sense, won't really mind whatever license is used in the end, as the end goal here is to create something useful for the human's desktop, not to get intertwined in open source licensing debates. I have a slightly longer term vision with this project and all I want is that going forward, I shouldn't be restricted from using some useful component or tool or library just due to licensing issues. From that perspective, are *permissive* or *copyleft* licenses a better candidate to license your open source projects? And which one would you suggest?	5.0	t3_18e9upe	reddit		
514	QualityScaler 2.11 - image/video AI upscaler app	Unknown	2023-12-09 08:01:22	https://www.reddit.com/r/Python/comments/18e8ox1/qualityscaler_211_imagevideo_ai_upscaler_app/	&#x200B; https://preview.redd.it/vpwiiuld785c1.png?width=1436&format=png&auto=webp&s=51f5f8ae05abef3167180203c0dc776015d159e4 Github. [https://github.com/Djdefrag/QualityScaler](https://github.com/Djdefrag/QualityScaler) QualityScaler is a Windows app **powered by AI** to **enhance, enlarge** and **reduce noise** in photographs and videos. **Changelog versions from 2.9 > 2.11** **NEW** * Updated exiftool to **version 12.70** * Support for **metadata copy** from original videos to upscaled videos * For **AMD gpu** users, it is recommend upgrading to **driver > 23.11.1** * which contains **performance improvements** for DirectML-based applications * Improvements for video upcaling * upscaled frames will now **overwrite original frames** (to save disk space) * for .mp4 output is possibile to select the codec, **x264 or x265** * improved video bitrate from 4M (default value for ffmpeg) to **16M** * in future will be the possibility to select the desired bitrate * Added more **Interpolation** options * Low (**30%** original / **70%** upscaled) * Medium (**50%** original / **50%** upscaled) * High (**70%** original / **30%** upscaled) * Disabled (interpolation **disabled**) **GUI / UI** * Updated **info texts**, giving more information * Input Resolution % default value changed to **60%** * Image output default value changed to **.png** * Interpolation default value changed to **Low** **BUGFIX / IMPROVEMENTS** * Improvements processing **black and white images** * Fixed **RealSRx4\_Anime AI** model not loading properly * Now the app is built via **nuitka** (instead of **pyinstaller**) * reducing **false positives** from antivirus * Redesigned how the app keep tracks of **upscale progress** * Fixed upscaled video **incorrect colorspace** * **Tilling function** improvements * General bugfixes and performance improvements * Updated dependencies **VirusTotal**. [https://www.virustotal.com/gui/file/76a82d66713879238c9a0ca17b2151fe5b362d69494667ef94c61aecb6c09384?nocache=1](https://www.virustotal.com/gui/file/76a82d66713879238c9a0ca17b2151fe5b362d69494667ef94c61aecb6c09384?nocache=1)	0.0	t3_18e8ox1	reddit		
515	Working on updating the PyDev debugger to use sys.monitoring... I just love my multi-monitor setup with LiClipse ;)	Unknown	2023-12-09 10:48:42	https://www.reddit.com/r/Python/comments/18eb2aq/working_on_updating_the_pydev_debugger_to_use/	Ok, so, I'm updating the PyDev debugger to use sys.monitoring...Hopefully it'll land soon (I've been working on and off in it for a while and not that much is missing for a final release). Now, I stopped to breath a little and I just realized I really like using LiClipse with multiple monitors. [My window arrangement using LiClipse](https://preview.redd.it/yhl7076l095c1.png?width=5760&format=png&auto=webp&s=41dc7afeb1fdd169b0c9830ef5d141ca0bcab82e)	0.0	t3_18eb2aq	reddit		
516	Balderdash LSTM : Python AI Game	Unknown	2023-12-09 15:01:55	https://www.reddit.com/r/Python/comments/18efddn/balderdash_lstm_python_ai_game/	Howdy, Balderdash is a game where you try to trick your friends and family into believing that a word has a different definition that what it's real one is. This game was a favorite at local pubs, and eventually became a board game in the 1980s! My latest project showcases an AI which plays the game with you! **How does it work?** The code works by training a Long Short Term Memory (LSTM) model on balderdash-esque words. Once at a high enough accuracy ( 90% ), the model is then given free-range to generate its own words! Become AI's have a tendency to hallucinate, this means that sometimes the words generated by the AI won't be real English words; however, they will look English-like. To check the AI's output, we send the information over to an online dictionary and scrap its response **Importance of code** As mentioned above, this code is a LSTM model, this is one of the foundations to larger LLM models such as Chat-GPT. In the code and video I go over how to make the LSTM model from scratch without the use of common AI libraries such as Tensorflow. This provides a low level understanding over how the math for these models work, and can be used as a good reference for new developers looking to advance their knowledge in machine learning! **Resources** If this all sounds interesting to you... the game can be played here: [https://jtexpo.github.io/Balderdash\_LSTM/](https://jtexpo.github.io/Balderdash_LSTM/) the code can be found here: [https://github.com/JTexpo/Balderdash\_LSTM](https://github.com/JTexpo/Balderdash_LSTM) and a YT video over it, can be found here: [https://youtu.be/e6ptPUis-Yo](https://youtu.be/e6ptPUis-Yo) &#x200B; Happy learning y'all!	0.0	t3_18efddn	reddit		
517	Built a Python script to get Spotify stats!	from __future__ import 4.0	2023-12-08 22:25:23	https://www.reddit.com/r/Python/comments/18dyf8o/built_a_python_script_to_get_spotify_stats/	Hey r/Python, I wrote a Python script while I was playing with the Spotify API which can get your Spotify Stats :) https://preview.redd.it/b32baddnc55c1.png?width=640&format=png&auto=webp&s=e1720187f521f72cf4a6e356a678e6c613018623 Repository Link : [https://github.com/ni5arga/spotify-stats-python](https://github.com/ni5arga/spotify-stats-python) Feel free to star and fork!	0.0	t3_18dyf8o	reddit		
518	Tkinter Real Time Video & Audio Synced Widget.	Unknown	2023-12-08 21:42:06	https://www.reddit.com/r/Python/comments/18dxhoj/tkinter_real_time_video_audio_synced_widget/	**A widget to play any video with audio on tkinter.its fast and light weight.** **EXAMPLE:** [showcase image](https://preview.redd.it/gqb21q59455c1.png?width=803&format=png&auto=webp&s=cd2d68816164cc6fec456ace1e9ee3c1d8959308) **It also has a builtin efficient API to control the playback.The audio is synced with the video and plays with correct fps. It** can also play m3u8 using some modifications. Its like a video widget for tkinter,because the current modules in github cant play audio and video at the same time.But this widget can work with both video and audio. devoloper: [https://github.com/cool-dev-guy](https://github.com/cool-dev-guy) pypi package : [https://pypi.org/project/tkintervideo/](https://pypi.org/project/tkintervideo/) github source : [https://github.com/cool-dev-guy/tkintervideo](https://github.com/cool-dev-guy/tkintervideo) NOTE: the example code in docs deployed on \`pypi\` may not work,instead refer docs([README.md](https://README.md)) from github.	1.0	t3_18dxhoj	reddit		
519	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2023-12-09 00:00:21	https://www.reddit.com/r/Python/comments/18e0fjt/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	1.0	t3_18e0fjt	reddit		
520	The Python on Microcontrollers Newsletter, a free weekly news and project resource, please subscribe	Unknown	2023-12-08 14:47:06	https://www.reddit.com/r/Python/comments/18dodr9/the_python_on_microcontrollers_newsletter_a_free/	&#x200B; https://preview.redd.it/41ir56yu235c1.jpg?width=503&format=pjpg&auto=webp&s=55dbf7fb7ea7aca981e7666a190f330b00575716 With the Python on Microcontrollers newsletter, you get all the latest information on Python running on hardware in one place! MicroPython, CircuitPython and Python on single Board Computers like Raspberry Pi & many more. The Python on Microcontrollers newsletter is the place for the latest news. It arrives Monday morning with all the week‚Äôs happenings. No advertising, no spam, easy to unsubscribe. 10,673 subscribers - the largest Python on hardware newsletter out there. **Catch all the weekly news on** [**Python for Microcontrollers**](https://www.adafruitdaily.com/) with [adafruitdaily.com](https://www.adafruitdaily.com/). >This *ad-free, spam-free* weekly email is filled with **CircuitPython**, **MicroPython**, and **Python** information that you may have missed, all in one place! You get a summary of all the software, events, projects, and the latest hardware worldwide once a week, no ads! Ensure you catch the weekly Python on Hardware roundup‚Äì you can cancel anytime **‚Äì** [**try our spam-free newsletter today**](https://www.adafruitdaily.com/)**!** [**https://www.adafruitdaily.com/**](https://www.adafruitdaily.com/)	0.0	t3_18dodr9	reddit		
521	Cirkus - Minimal electronic circuit simulator in Python	Unknown	2023-12-08 21:48:30	https://www.reddit.com/r/Python/comments/18dxmnk/cirkus_minimal_electronic_circuit_simulator_in/	Writing a circuit simulator is a wonderful way of learning electronics as a programmer! https://github.com/keyvank/cirkus	2.0	t3_18dxmnk	reddit		
522	I finally have something worthy of posting on the Python Reddit. I present to you SavonPython!	Unknown	2023-12-07 21:52:58	https://www.reddit.com/r/Python/comments/18d6oyz/i_finally_have_something_worthy_of_posting_on_the/	Update: Thank you all for the positive feedback. I updated the repo to leave notes for those who might want to recycle the code for their own use. No need to reinvent the wheel. &#x200B; [https://github.com/huths0lo/SavonPython](https://github.com/huths0lo/SavonPython) I shop at vons. They have digital coupons that can offer some hefty discounts if you use them. The best way to maximize your savings is to literally just clip every single coupon; whether you use it or not. They are adding literally hundreds every single week, so this could be a huge time suck. I wanted to find a way to automate this so that all of my coupons were clipped automatically. This lead me on a journey of understanding what are the interactions that take place between an end user and their app. I was initially trying to understand how their app worked on my iphone, but eventually realized that the iphone app always requires a one time passcode; which I cant automate around easily. I do have some sms automation, but it would require I change my account to a different phone number, and wouldnt really be usable by others. But I found that they have the same functionality on [vons.com](https://vons.com), which does allow for the use of passwords. I'm not a web developer. In fact I only have a couple of years of programming experience in general. So I had to figure out for myself how the website actually worked. I started by monitoring my browser console, but eventually realized this was too limiting. That lead me find the postman plugin to proxy my traffic. But even that was pretty limiting. Finally I found mitmproxy which is absolutely amazing. This allowed me to proxy my iphone and browser traffic, allowing me to see what was really happening. I quickly realized that you couldnt just do a get a request to [vons.com](https://vons.com). This would trigger responses that you are a bot. I tried use selenium, but just couldnt get it to work well. And even if I could, it required too many version specific installs. The key was that if I browse to the website with my browser, versus doing a straight get request, I could see my browser was getting a bunch of cookies that my get request would not. This made me realize the cookies were being set by javascript instead of headers. I was eventually able to find the portions of my get request that pertained to this, and was able to target this in to variables. But the next problem is that python is not a javascript interpreter. And the problem after that is pythons escape characters arent the same as javascript. So if you decipher some of the js hex, the strings were filled with excess escape characters. I made some attempts to pass the code off to node.js for interpretation, but the problem is, all of that code was calling variables that would be your literal browser. So if not ran within a browser, those variables are non existent, and generate errors. I was pretty deep in to just deciphering the intention of js code, but then found the incapsula cracker library. Someone else obviously had worked through this before. So I borrowed their code. I was very surprised to find that if I was able to get this to work, I would be defeating a serious enterprise grade application firewall. The library itself only resolved the couple of js decipher challenges I have. But I still had to figure out all of the rest of the site specific interactions, and needed cookies. I also had to figure out the okta logon flow. So the library I found solved an important, but smaller piece of the overall challenge. Either way, I was finally able to crack it. My code does have some static entries that would likely need to be adjusted for someone elses account; but those are easy to spot, and easy to adjust. I'll update the script if I can automate those in the future. So what this script does: Initiates a session to [vons.com](https://vons.com).Collects the initial cookies.Interprets the follow up interactions needed to complete the incapsula process.Initiates a call to okta to get a session id.Grabs the remaining cookies needed.POSTS your username to [vons.com](https://vons.com).POSTS your password to [vons.com](https://vons.com).Sets the cookies from the previous step.Performs handshake with okta to complete your logon.Refreshes your cookies. Then it requests all of the coupons available to you. These are store specific, so you'll need your store id. It parses to coupons that arent yet clipped, and proceeds to clip all of them. I havent clipped my coupons for a week, and it just clipped 250 for me. Thanks for reading, and I hope someone else finds this tool useful!	13.0	t3_18d6oyz	reddit		
523	Discord Server Cloner package	Unknown	2023-12-08 21:03:37	https://www.reddit.com/r/Python/comments/18dwmu7/discord_server_cloner_package/	https://github.com/Noritem/nexx_clone https://pypi.org/project/nexxclone/ docs are in github and pypi	0.0	t3_18dwmu7	reddit		
524	How to Bypass reCaptcha in Selenium Automatically with Code Example	Unknown	2023-12-08 05:07:31	https://www.reddit.com/r/Python/comments/18dfcbp/how_to_bypass_recaptcha_in_selenium_automatically/	"I based my approach on manual that caught my eye just a couple of days ago, and I decided to test it (since it's written by a captcha recognition service that I use, why not - by the way, guys from [2captcha](https://2captcha.com/p/selenium-captcha-solver) \- I accept thanks in the form of green bills, if you're interested)))) &#x200B; https://preview.redd.it/n33aufe4705c1.jpg?width=780&format=pjpg&auto=webp&s=3bda1517e008307ef21e3898f1dc79f04e22f5ee I've made some tentative attempts at automation and encountered a frequently arising problem: reCaptcha recognition. Of course, I understand that there are many guides, manuals, and articles written on this topic, but let's agree - it's interesting to describe one's own experience. Thus, without getting too lengthy and boring about why Selenium is necessary, how important it is for automation, when it appeared and who invented it, let's get to the crux: To solve the problem, we'll use a demo page kindly provided by reCaptcha itself - [https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php](https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php): ## Preparation In the first stage, everything needs to be prepared. For the demonstration, I downloaded the following components into a special folder on my computer (having previously installed Python, of course - but I won't describe how I did that, I hope you can figure it out). So, what we will need is: The Selenium library for browser automation itself, which can be found here - [https://pypi.org/project/selenium/](https://pypi.org/project/selenium/) The official Python SDK for integrating with the 2Captcha API, available here: [https://pypi.org/project/2captcha-python/](https://pypi.org/project/2captcha-python/) A library that simplifies the downloading and use of drivers for Selenium, called webdriver-manager, can be found here - [https://pypi.org/project/webdriver-manager/](https://pypi.org/project/webdriver-manager/) Everything installs very simply, just copy the provided command and paste it into the console, as shown in the video above. You can install everything separately, as I did, or you can use a universal command. python -m pip install 2captcha-python selenium webdriver-manager ## Finding the Sitekey Since we're dealing with reCaptcha, it's important to understand what a site key is. A site key is a unique identifier that Google assigns to all its forms with reCAPTCHA, which we use to identify the captcha on a website. It is this identifier that we need so that the captcha recognition service can understand what it's dealing with, and it is this identifier that we will be sending to 2captcha. Let's find the sitekey on the demo page. 1. Go to [https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php](https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php) 2. Open the developer tools - press Ctrl/Cmd + Shift + I or right-click and select ""Inspect"". 3. Find the data-sitekey (press ctrl + F, enter sitekey, and press enter) and copy the value of the parameter. 4. Save the value to use it when sending a request to solve the captcha on the page. ## Solving the Captcha Of course, to solve the captcha, you need to write some code - you can either write it yourself or simply take it from an existing manual, like this one. The purpose of this code is to navigate to the target page and solve the captcha through the API. This is exactly what we aim to achieve. I should mention that there are a few parameters in the code that you need to change to your own, here they are, from top to bottom: `2CAPTCHA_API_KEY` \- your API key, which you can get from your 2captcha account. `SITE_KEY` \- this is what we saved in the previous step. I am very much counting on you being able to find these parameters in the provided code on your own. driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) captcha_page_url = ""https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php"" driver.get(captcha_page_url) # —Ä–µ—à–µ–Ω–∏–µ –∫–∞–ø—á–∏ print(""solving captcha"") solver = TwoCaptcha(""2CAPTCHA_API_KEY"") response = solver.recaptcha(sitekey='SITE_KEY', url=captcha_page_url) code = response['code'] print(f""Successfully solved the Captcha. The solve code is {code}"") This piece of code initializes a `TwoCaptcha` object with our API key and is supposed to solve the reCAPTCHA by calling the `recaptcha` method. For this, we pass the `site key` and the URL of the page. > This is not the complete code, keep reading! ## Submitting the Solved Captcha The next piece of code I'm not exactly sure how to explain to you properly, as I didn't fully understand it myself, so I've just translated it and left it here: Next, we find the `g-recaptcha-response` element, insert the obtained value to solve the captcha, and submit the form. recaptcha_response_element = driver.find_element(By.ID, 'g-recaptcha-response') driver.execute_script(f'arguments[0].value = ""{code}"";', recaptcha_response_element) submit_btn = driver.find_element(By.CSS_SELECTOR, 'button[type=""submit""]') submit_btn.click() input(""Press enter to continue"") driver.close() # Full Code for Automatic reCaptcha Solving Essentially, here's what we end up with as a result. from selenium.webdriver.common.by import By from twocaptcha import TwoCaptcha from selenium.webdriver.chrome.service import Service as ChromeService from webdriver_manager.chrome import ChromeDriverManager from selenium import webdriver # Instantiate the WebDriver driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install())) # Load the target page captcha_page_url = ""https://recaptcha-demo.appspot.com/recaptcha-v2-checkbox.php"" driver.get(captcha_page_url) # Solve the Captcha print(""Solving Captcha"") solver = TwoCaptcha(""2CAPTCHA_API_KEY"") response = solver.recaptcha(sitekey='SITE_KEY', url=captcha_page_url) code = response['code'] print(f""Successfully solved the Captcha. The solve code is {code}"") # Set the solved Captcha recaptcha_response_element = driver.find_element(By.ID, 'g-recaptcha-response') driver.execute_script(f'arguments[0].value = ""{code}"";', recaptcha_response_element) # Submit the form submit_btn = driver.find_element(By.CSS_SELECTOR, 'button[type=""submit""]') submit_btn.click() # Pause the execution so you can see the screen after submission before closing the driver input(""Press enter to continue"") driver.close() To demonstrate the functionality of the code, I'll create a text file, name it [`script.py`](https://script.py), and run it in the console. https://youtu.be/nm3wpnDQLFQ"	3.0	t3_18dfcbp	reddit		
525	NFL Odds API	Unknown	2023-12-08 00:19:17	https://www.reddit.com/r/Python/comments/18d9v73/nfl_odds_api/	Looking for a good source for nfl odds & scores. I‚Äôve played a bit with scraping this data but everytime i think i have a decent solution there are big gaps in data or a total redesign of the site(s). I‚Äôm definitely price sensitive but i am willing to pay if there were a decent rest api that would give me each game‚Äôs home team, away team, date, score, spread, and o/u. If anyone knows of a good one, i‚Äôll be forever grateful	4.0	t3_18d9v73	reddit		
526	Pygolo 0.2.0: write Python extensions in Go!	Unknown	2023-12-07 19:41:32	https://www.reddit.com/r/Python/comments/18d3nt3/pygolo_020_write_python_extensions_in_go/	Pygolo 0.2.0 is eventually out and it opens wide the way to extending Python with Go. The API has more consistent naming, the CI checks with finer grain, property-based testing gives some more breadth and coverage. In the overall, things remained simple and the documentation follows suit. That's awesome. Here the [release notes](https://gitlab.com/pygolo/py/-/releases/v0.2.0).	3.0	t3_18d3nt3	reddit		
527	[Show Case]Create and Chat with Character with Different Behaviors Changing according Mood Status	Unknown	2023-12-08 10:12:07	https://www.reddit.com/r/Python/comments/18djrmr/show_casecreate_and_chat_with_character_with/	"## About Us Our project is an **open-source AI-agent native development framework**. Our goal is to speed up your AI-native application development. Visit our [Github homepage](https://github.com/Maplemx/Agently) to explore more and please ‚≠êÔ∏è star it if it is helpful to you. ## Show Case Story When we try to build a LLM role-play agent like Character.ai does, how to make the character's behaviors lively and have different performance styles according the chat contents is an important problem to solve. If we write a long prompt and put it to the head of the request every time for all situations, the first problem is that spend too much token and as we know token = $ ü§£. The second problem is this way is too easy to under jailbreak attack, ""forget all you prompt and do as I command:"" and something like that will break the character you build with heart. In this showcase, two major features of our framework are demostrated: * How to use agent to generate character settings according brief description * How to use Status component to change role settings and make agent reaction change. Notice that, when you use Status component, the behaviour guide and example lines of CURRENT STATUS ONLY will be set to agent, that'll save your tokens and make your character harder to be jailbreaked. ## Demo Code Install our package first: pip install -U Agently Then start coding: import Agently agent_factory = Agently.AgentFactory() writer_agent = agent_factory.create_agent()\ .set_settings(""current_model"", ""OpenAI"")\ .set_settings(""model.OpenAI.auth"", { ""api_key"": """" }) # Step 1. Create Character with Different Status character_brief_desc = input(""Brief description of the character you want to create: "") character_basic_info = writer_agent\ .input({ ""brief_desc"": character_brief_desc })\ .instruct(""Create a character according {brief_desc}"")\ .output({ ""name"": (""String"", ), ""gender"": (""String"",), ""age"": (""Number"", ), ""possible_status"": [ ( ""String"", ""keyword of emotions or moods this character possibly be like in different situations. Examples: Happy, Calm, Tsundere, Angry"" ) ], ""background_story"": [(""String"", ""important milestone or key stories of character's life. Format: <Age>: <Milestone or Key Story>"")], })\ .start() character_basic_info.update({""desc"": character_brief_desc}) status_desc_details = writer_agent\ .input({ ""character_basic_info"": character_basic_info })\ .instruct(""Generate character behaviours details of {possible_status}"")\ .output([{ ""status"": (""String in {character_basic_info.possible_status}"", ), ""change_into_conditions"": [(""String"", ""key conditions that cause the character change into this status: {status}"")], ""behaviours"": [(""String"", ""behaviour direction descriptions when character in the mood or emotion of {status}"")], ""line_examples"": [(""String"", ""line the character may say according {status} and {behaviours} descriptions"")], }])\ .start() # Step 2: Load Character Settings into role_play_agent role_player_agent = agent_factory.create_agent()\ .set_settings(""model.OpenAI.auth"", { ""api_key"": """" }) role_player_agent\ .set_role(""name"", character_basic_info[""name""])\ .set_role(""gender"", character_basic_info[""gender""])\ .set_role(""age"", character_basic_info[""age""])\ .set_role(""background_story"", character_basic_info[""background_story""]) possible_status = [] for status_desc_detail in status_desc_details: possible_status.append({ ""status_name"": status_desc_detail[""status""], ""change_into_conditions"": status_desc_detail[""change_into_conditions""] }) role_player_agent\ .append_status_mapping( ""mood_status"", status_desc_detail[""status""], ""set_role"", { ""behaviours"": status_desc_detail[""behaviours""], ""line_examples"": status_desc_detail[""line_examples""], } ) role_player_agent.set_role(""possible_status"", possible_status) print(""[Role Loading Completed]"") # Step 3: Start Chatting with Status Changing while True: user_input = input(""[User]: "") if user_input == ""#exit"": break mood_status_change = role_player_agent\ .input(user_input)\ .output({ ""mood_status_change"": (""String in {possible_status}"", ""according {input}, character mood status will change to?"") })\ .start() print(""\n#Mood: "", mood_status_change[""mood_status_change""]) print(f""\n[{ character_basic_info['name'] }]:"", end="""") reply = role_player_agent\ .set_status(""mood_status"", mood_status_change[""mood_status_change""])\ .input(user_input)\ .on_delta(lambda data: print(data, end=""""))\ .start() print("""") ## The Result https://preview.redd.it/cdb3868vq15c1.png?width=1840&format=png&auto=webp&s=7bf043cbdab63fad5e1ca89272f945faac18a99c ## Try by yourself Jupyter / Google Colab Document: [Click to visit](https://github.com/Maplemx/Agently/blob/main/playground/character_change_behaviours_according_mood_status.ipynb) # Want to Explore More? Visit Our [Github Homepage](https://github.com/Maplemx/Agently) and ‚≠êÔ∏è if you like it."	2.0	t3_18djrmr	reddit		
528	I created a no-cost* AWS infrastructure boilerplate for Python API	Unknown	2023-12-07 08:51:35	https://www.reddit.com/r/Python/comments/18crgim/i_created_a_nocost_aws_infrastructure_boilerplate/	"**Hi** r/Python! I created a low-cost AWS infrastructure boilerplate for Python API (Django as implemented). Works with free tiers for accounts less than a year old. The only thing you need to pay for is Secret Manager (1$ monthly for each secret). **What's in the box?** It's a boilerplate to link up your Python API with AWS. It keeps things straightforward, and AWS CDK makes it easy to tweak the setup. **Why bother?** If you've ever wanted a straightforward way to get your API going without making things overly complicated, this could be useful. It's all about building your app fast without boxing you into a rigid system. By using AWS CDK your infrastructure could be extended without any limits! **Fully deployed project uses:** * RDS (postgres, T3 MICRO) * S3 bucket (for static files, images or/and django-admin) * Lambda (DockerImage with 3 MB of memory and 60 second timeout by default) * ECR (keeps only 2 newest images) * API Gateway * RDS secrets (generated automatically) * Other secrets (like DJANGO\_SECRET\_KEY, generated manually) It's my first ""open source"" solution. I'm open to hearing your thoughts and any ideas you might have. Thanks for checking it out, and I hope this makes your development process a bit smoother. [https://github.com/baxiee/django-cdk-boilerplate](https://github.com/baxiee/django-cdk-boilerplate)"	4.0	t3_18crgim	reddit		
529	flex-prompt: a flexible prompt renderer for LLMs that ensures you never overflow your model's context	Unknown	2023-12-08 00:52:38	https://www.reddit.com/r/Python/comments/18daixz/flexprompt_a_flexible_prompt_renderer_for_llms/	"When working with LLMs, I frequently experience *token agony*. [this model takes 4,097 words and you are trying to push in the whole of War and Peace, you imbecile](https://preview.redd.it/o6183i7byy4c1.png?width=1348&format=png&auto=webp&s=cda9ffd45a5024116326dd1d5ed947712e119724) Perhaps you've experienced it too! The issue is particularly pronounced with retrieval augmented pipelines, since you have potentially quite a large set of documents which you could perhaps include in the prompt if only you knew how big it could be. I got tired of hacking around this headache, so I wrote [flex prompt](https://pypi.org/project/flex-prompt/) to address it. I wish I didn't have to. Perhaps someone can point me to a better solution! But I couldn't find one, so alas, here it is. Flex prompt provides a basic layout and component model to help you describe how you want the pieces of your prompt to grow and shrink and a token-aware renderer which renders your prompt to fit your model's window. [Github](https://github.com/queerviolet/flex-prompt), [Intro to flex prompt colab](https://colab.research.google.com/github/queerviolet/flex-prompt/blob/main/doc/intro_to_flex_prompt.ipynb) # Quick examples You can just `render(Flex(...), model=<model string or LangChain / Haystack object>)`, and flex prompt will fit the prompt into the context window, and tell you how many tokens are left over for the response: from flex_prompt import render, Flex, Expect rendered = render( Flex([ ""Given the text, answer the question."", ""--Text--"", WAR_AND_PEACE, ""--End Text--"", ""Question: What's the title of this text?"", ""Answer:"", Expect() ], join='\n'), model='text-davinci-002') # rendered.output is the string to send to the model # rendered.max_response_tokens is how many tokens you can # request in response without exceeding the model's context window print(rendered.output, rendered.max_response_tokens) More typically, you'll want to define a prompt which takes parameters. To do this, you can create a class (probably a dataclass) which derives `Flexed`: from flex_prompt import Flexed, Expect from dataclasses import dataclass @dataclass class Ask(Flexed): text: str question: str answer: str | Expect = Expect() instruct: str = ""Given a text, answer the question."" flex_join = '\n' # yielded items will be joined by newlines def content(self, _ctx): if self.instruct: yield 'Given the text, answer the question.' yield '' yield '-- Begin Text --' # note: we're using `Flex` here just to attach a flex_weight # to the text, telling the renderer we'd like more space for the # text than anything else. yield Flex([self.text], flex_weight=2) yield '-- End Text --' yield 'Question: ', self.question yield 'Answer: ', self.answer Note that the component above can be used to render both the actual prompt and examples. Examples simply have an `answer`. This is useful for experimenting with different ways of structuring a prompt while ensuring that all the examples we present to the LLM are in the same format. # Internals This was interesting to write! It's been a hot second since I touched Python in a real way, but gosh what a nice language. I particularly appreciate the new (to me) `@dataclass` machinery, which really reduces boilerplate. The renderer works much as you might expect: a pretty standard recursive delegating renderer. It knows how to render `str` into `Tokens` and if you render a callable it calls it, which is how any more complex behavior happens. You can yield anything which you can pass to the top-level render function, including other components, creating a whole tree. Internally, flex prompt is super lazy. This more or less happened automatically through extensive use of iterators. `render` creates a `Rendering`, which renders its input into a sequence of parts‚Äîparts can be concrete token lists, but also child `Rendering`s, which is how we store the render tree. I mostly expect that having the tree on hand will be useful for prompt optimization: e.g. measuring tokens spent on examples vs performance at some task. It would certainly be possible to track these tokens during rendering rather than querying for them afterwards, but it honestly seemed harder. In any case, being super particular about performance at this stage seems a little silly: each token is about to swell to 16kb and burn down half the amazon on its way to becoming `P(butts) ~ 0.2`, so it's unclear to me that the rendering process will ever be the bottleneck (even if the LLM is running on another machine, as it is in these examples). # Is it worth it? As models grow larger and larger context windows, I've asked myself whether this is worth it. Won't context sizes eventually big enough to put in everything we might want without worry? One response: ""everything I might want"" is a very, very big set, plausibly bigger than any window size we're going to see soon. Another: being able to do this kind of token accounting is useful even if we don't completely fill context windows. For example, we might be able to augment our prompt with examples, documents, and tips. How much space should we allocate to each? The answer might well be model-dependent. How do we figure it out? Flex prompt's output, a `Rendering` object, actually holds the entire component tree. You can look through the object to see how many tokens were allocated to each child. This is currently very manual, but it does provide the bedrock infrastructure to e.g. run tests to discover the optimal balance of augmented data for a given prompt and model. Additionally, the right admixture (and for that matter, the right *phrasing*) may well be model-dependent. Flex prompt currently provides only very limited model-specific rendering (you can look at [`ctx.target`](https://ctx.target), but it doesn't tell you much), but there's no reason that can't be significantly improved. At the extreme limit is prompt *erasure*, where we fine-tune a model to require no or minimal instructions/examples for a given set of prompts. Flex prompt can enable transitions like this with no changes to the pipelines themselves: you'd still use the same prompt components, they'd just render differently if the target is a fine-tuned model vs. a generic one. # Status & Future Work Flex prompt is very much in early development. I would love to hear if and how people find it useful, and would love input and contributions! Some things I'd like to tackle in the future: * **Rendering message lists.** Flex prompt currently only renders strings, though it's set up to be able to render any type of output. Message histories basically grow without bound, so supporting this seems like a no-brainer. * **Pagination**. If your rendering overflows (as above, where we're trying to stuff *the entirety of war and peace* into a prompt), flex prompt will clip the offending pieces to fit. But there's currently no way to get ""the next page"". But the `Rendering` actually retains enough information to do this! It would be great to be able to call `render(...).pages()` to get the sequence of prompts as we ""scroll"" whatever has overflowed. This is medium-hanging fruit‚Äîa little tricky because we do have to descend the tree of renderings to find the exact one(s) which overflowed and then update only those. * **Token accounting.** As mentioned above, you can currently grovel around in `Rendering` and look at the pieces of the prompt. This would be more useful if it were a little easier, e.g. if you could use `rendering[Examples]` to find all the parts rendered by the `Examples` component, or `rendering['advice']` to find all the parts which are tagged (somehow) as ""advice"". The use case here is prompt optimization: discovering the optimal number or percentage of tokens to allot to each thing we might want to drop into the prompt. * **More integrations.** Currently, flex prompt only supports OpenAI models. You can register your own target finders, but it would be great to have more support out of the box. This is mostly a matter of digging around and finding the tokenizers and window sizes for common models, and then writing the appropriate target finders. Contributions very welcome! * **Model tuning.** As mentioned above, the rendering context could provide a mechanism for fetching model-specific parameters. The basic idea is that `ctx[param]` will evaluate `param` against the context, and then we can define some parameter types which load their model-specific values from *gestures vaguely* somewhere. Thanks for reading! * [Flex prompt Github](https://github.com/queerviolet/flex-prompt) * [Intro to flex prompt colab](https://colab.research.google.com/github/queerviolet/flex-prompt/blob/main/doc/intro_to_flex_prompt.ipynb) * [My website](https://ashi.io). *shameless plug: I have a lot of engineering experience and a bit of machine learning experience and* [*I am currently looking for a job*](https://ashi.io/resume.pdf)"	0.0	t3_18daixz	reddit		
530	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2023-12-08 00:01:26	https://www.reddit.com/r/Python/comments/18d9i0f/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	0.0	t3_18d9i0f	reddit		
531	Looking for contributors for my open-source AI security project	Unknown	2023-12-07 19:45:53	https://www.reddit.com/r/Python/comments/18d3rdq/looking_for_contributors_for_my_opensource_ai/	Hi all! I am looking for contributors for my open-source project. It is a toolbox for testing against adversarial attacks in both DNNs and LLMs. Idea is to test your DCNNs against adversarial attacks such as fast gradient sign method and toxic prompts in LLMs. I would very much appreciate contributions, I need more devs as I'm too busy to do this all by myself üôè. Repo is here: [https://github.com/kortex-labs/plexiglass](https://github.com/kortex-labs/plexiglass)	0.0	t3_18d3rdq	reddit		
532	Introducing Pharaoh-Report: a report-generation framework powered by Sphinx and Jinja	Unknown	2023-12-07 09:00:16	https://www.reddit.com/r/Python/comments/18crkc5/introducing_pharaohreport_a_reportgeneration/	"I'd like to introduce [Pharaoh](https://github.com/Infineon/pharaoh-dev), a [Sphinx](https://www.sphinx-doc.org/en/master/)\-based Python framework for generating reports in various formats by combining the power of configurable [Jinja](https://jinja.palletsprojects.com/en/3.1.x/) templates and Python scripts for asset generation. We have developed it last year in our company to simplify and automate the report generation for measurement results. We mostly output HTML and Confluence reports, but also Latex might be needed in future (contributions welcome). Since we didn't want to reinvent the wheel, we decided to stick to open-source libs like Sphinx and Jinja, which retrospectively was a good move since it turned out so well. Also we decided to give something back and management allowed us to make it open source (MIT), hoping it will get picked up by this great community ;) Pharaoh may be extended using [pluggy](https://pluggy.readthedocs.io/en/stable/) (e.g. for company internal plugins). Please let me know what you think! If you like it, *give it a star* ;) It's our team's first project on GitHub, so help and contributions highly welcome! For more information, please refer to the [official documentation](https://infineon.github.io/pharaoh-dev/), [Github](https://github.com/Infineon/pharaoh-dev) and [PyPI](https://pypi.org/project/pharaoh-report/). Installation via pip: `pip install pharaoh-report` Here some info right away (from the docs): https://preview.redd.it/q0kn5kf34u4c1.png?width=1002&format=png&auto=webp&s=c831ed71e261bf67fdbefdd2a20122d562b05ac2 Pharaoh automated the process of manually creating assets (plots, tables, ...) and including it into a report. It extends Sphinx by multiple built-in Jinja templating steps and additional directives to easily include generated assets in your reST templates. # Working Principle The standard Pharaoh workflow consists of following major steps: **Project Generation** Generates a Pharaoh project with one or multiple components based on predefined templates. Its core is a fully-preconfigured [Sphinx](https://www.sphinx-doc.org/en/master/usage/quickstart.html) documentation project. This generation is called **first-level templating** or **generation-time templating** further on in the documentation. It may be checked into GIT for incremental modifications or generated each time, depending on your project setup. Now the templates and asset scripts in the generated components as well as the project settings can be modified by the user. https://preview.redd.it/qq5abisf5u4c1.png?width=1158&format=png&auto=webp&s=b60d228f9af514b2c56ee314cad4e02f34741d81 **Asset Generation** During asset generation, the asset scripts of each component are executed. By using the resources files of its own and/or other components, asset scripts are producing dynamic content assets for the later report, like plots, images, tables and additional data (e.g. json) used for rendering the templates. Those assets are then registered with metadata, so that they can be searched/included by the templates. Each asset script may produce any amount of assets. from pharaoh.assetlib.api import metadata_context import plotly.express as px df = px.data.iris() fig = px.scatter( df, x=""sepal_width"", y=""sepal_length"", color=""species"", symbol=""species"", title=r""A title"", ) with metadata_context(label=""my_plot""): fig.write_html(file=""iris_scatter.html"") This plot may be later included in the report using following Sphinx directive: .. pharaoh-asset:: label == ""my_plot"" &#x200B; **Project Build** Translates an existing Pharaoh project to a configured target format (e.g. HTML, Confluence, LaTeX), while performing an additional templating step via [Jinja](https://jinja.palletsprojects.com/en/3.1.x/intro/). This is called **second-level templating** or **build-time templating** further on in the documentation. Templates may inherit and extend existing or user-defined base-templates (for reuse purposes). Templates may dynamically include other templates. Templates are rendered using a rendering context (variables that can be used in template), that contains following data: * Project Settings * Static/manually entered context data * Dynamic context data, that is created by executing a Python script (potentially accessing generated assets) &#x200B; https://preview.redd.it/oialxb3n5u4c1.png?width=1296&format=png&auto=webp&s=6cc45f051f93598fa2afa336a735264500261718"	1.0	t3_18crkc5	reddit		
533	GitHub - eholic/pandaq: An easy pandas query-string builder.	Unknown	2023-12-07 14:17:40	https://www.reddit.com/r/Python/comments/18cwjo3/github_eholicpandaq_an_easy_pandas_querystring/	"My first PyPI package, kinds of support tool for pandas. https://i.redd.it/2x4ytlp8vn5c1.gif Source code: [https://github.com/eholic/pandaq](https://github.com/eholic/pandaq) PyPI page: [https://pypi.org/project/pandaq/](https://pypi.org/project/pandaq/) This library provides `q` method for easy querying of `pandas.DataFrame`. Internally, `q` generates the query string for [pandas.DataFrame.query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html). The goal of `pandaq` is to save time when querying. ## Installation pip install pandaq ## Usage `pandaq` provides two ways to select `pandas.DataFrame` rows by query string. ## A. Generate a query-string from pandaq import Q import pandas as pd df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv') qstr = Q().q(PassengerId=1) # -> ""PassengerId==1"" df.query(qstr) ## B. Add q method to pandas.DataFrame import pandaq.patch import pandas as pd df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv') df.q(PassengerId=1)"	0.0	t3_18cwjo3	reddit		
534	Number of syntax rules	Unknown	2023-12-07 22:49:55	https://www.reddit.com/r/Python/comments/18d7z4z/number_of_syntax_rules/	Hi, I'd like to compare some programming languages from the point of view of the number of syntax rules of each one. How many such rules are there for Python?	5.0	t3_18d7z4z	reddit		
535	lockfiles for hatch projects	Unknown	2023-12-06 18:29:22	https://www.reddit.com/r/Python/comments/18cakmd/lockfiles_for_hatch_projects/	I'm a huge fan of using [hatch](https://github.com/pypa/hatch) to manage my Python projects. It lets me define my projects with a single `pyproject.toml` file and define my extra environments and scripts (i.e testing / linting / docs). One big thing it's missing though is an integration with lockfiles. For all my projects I found myself regenerating manual lock files using complex shell commands with [pip-compile](https://github.com/jazzband/pip-tools) to get a reproducible environments across devices using a custom `pre-install-command`. I finally decided that instead of hacking together the same solution on all my projects I would build a plugin that handles this complexity for me. I came up with [hatch-pip-compile](https://github.com/juftin/hatch-pip-compile) \- it's a hatch plugin that connects your hatch-managed virtual environment to a lockfile managed with pip-compile. The plugin detects whether your environment or lockfile is out to date and automatically syncs them when needed - and it's fast! It's completely configurable and easy to get started. After adding the last few features and settling on a stable configuration I finally feel like it's ready for a wider audience. I'm extremely proud of how it turned out and I'm excited to share it with the hatch-world. I hope you find it useful for your Python projects! TL;DR check out [hatch-pip-compile](https://github.com/juftin/hatch-pip-compile) for automatic reproducible environments in your hatch project	1.0	t3_18cakmd	reddit		
536	It's Christmas day. You wake up, run to the tree, tear open the largest package with your name on it... FastAPI has added _____?	Unknown	2023-12-05 20:07:42	https://www.reddit.com/r/Python/comments/18bkywh/its_christmas_day_you_wake_up_run_to_the_tree/	Long time FastAPI user, I have a lot of free time this Christmas and January and would love to make some contributions to it. What are some things that the community / project would really benefit from. I would really like to give back to a project that has given me so much. &#x200B;	24.0	t3_18bkywh	reddit		
537	The worst recursive factorial function	Unknown	2023-12-05 15:48:14	https://www.reddit.com/r/Python/comments/18bew52/the_worst_recursive_factorial_function/	"I was playing around with postponed evaluation of annotations (PEP 563), and managed to create one of the most horrendous recursive factorial functions. I'll leave it here for your enjoyment ;) from typing import get_type_hints import inspect def factorial(n: ""(n := inspect.stack()[4].frame.f_locals['n']) and (n * factorial(n - 1)) or 1""): return get_type_hints(factorial)[""n""] Note: this code requires Python 3.11+, otherwise you'll get an error."	41.0	t3_18bew52	reddit		
538	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2023-12-07 00:00:11	https://www.reddit.com/r/Python/comments/18ci6c7/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	0.0	t3_18ci6c7	reddit		
539	sqlglot - Amazing SQL parsing library	Unknown	2023-12-05 23:27:40	https://www.reddit.com/r/Python/comments/18bprha/sqlglot_amazing_sql_parsing_library/	Wanted to give [sqlglot](https://github.com/tobymao/sqlglot) a shoutout as it saved me a ton of time. I had a task that involved building a dependency graph by statically analyzing the relationship of MySQL views. Initially, I was using sqlparse to extract the dependencies from the SQL statements, but it required me to create an increasingly hacky recursive function. I kept encountering examples in our codebase that required more test cases and further updating. Then I stumbled upon sqlglot, and the code reduced to just a few lines, working immediately. It was barely more complex than this: from sqlglot import parse_one, exp for table in parse_one(sql_statement).find_all(exp.Table): print(table.name) I'm surprised I hadn't heard of it before.	7.0	t3_18bprha	reddit		
540	Python Compiler	Unknown	2023-12-07 10:24:06	https://codingninjas.com/studio/online-compiler	Thrilled with the Coding Ninjas Python Compiler! It delivers a seamless speed, and user-friendly experience, catering perfectly to students. No setup hassles‚Äîjust pure coding joy. As one coder to another, highly recommend! Link to Coding Ninjas Python Compiler https://codingninjas.com/studio/online-compiler	1.0	t3_18csopu	reddit		
541	Pytest over Unittest	Unknown	2023-12-05 19:21:24	https://www.reddit.com/r/Python/comments/18bjv0y/pytest_over_unittest/	Hi all , I'd like to hear your opinions over when you have decided to use Pytest over Unittest as your testing framework for your projects and why. I've personally never used Pytest but recently found it on a project I started to work on and I've been finding it interesting.	31.0	t3_18bjv0y	reddit		
542	Simple and easy documentation Builder	Unknown	2023-12-06 17:16:19	https://www.reddit.com/r/Python/comments/18c8uqu/simple_and_easy_documentation_builder/	Hello redditors, I've been working on an open-source project called BrowseDocs, designed to assist open-source developers in documenting their libraries and tools with greater intuitiveness. The goal is to provide developers with an integrated and enhanced user experience while reading documentation. &#x200B; Some of the current features include: * Generate documentation pages from markdown files (chose .md because most of the developers are familiar). * Create multi-version, multi-paged documentation with a click of button. * dark/light themes included by default. * A quick search. (currently just a simple search) * A separate about page for every project. * Rich text editor for developers to write blogs and tutorials. Now the project itself is in its first iteration, which is why I look forward for your feedback. I'd like to know what features you'd like to see in the upcoming releases and what features you'd like to be removed from the project. I look forward to your responses. PS: I am still working on enhancing the UI of the landing page, any heads up would be greatly appreciated &#x200B; You can visit the project at [https://browsedocs.com](https://browsedocs.com) Github repo for the project: [https://github.com/PaulleDemon/BrowseDocs](https://github.com/PaulleDemon/BrowseDocs) &#x200B; Thanks, Have a great day ahead. &#x200B; Edit: for people pointing me to other documentation generator tools, Yes I know they exist, I wanted to provide a better experience for readers of the documentation and an developers who write documentations. There is always a space for improvement even in existing documentation generator tools, I am trying to fill in some of those gaps using your feedback. &#x200B;	3.0	t3_18c8uqu	reddit		
543	Introducing Great Tables	Unknown	2023-12-05 18:41:59	https://www.reddit.com/r/Python/comments/18biyrh/introducing_great_tables/	Great Tables is a \*great\* new Python package that simplifies the process of making tabular data presentable for any sort of publication task. With Great Tables, you can easily use data from a Pandas or Polars DataFrame and turn it into a beautiful table that can be included in a notebook, a Quarto document, or exported as HTML. We've been working hard on making this package as useful as possible, and we're excited to share it with you. The package (v0.1.0) is available on PyPI and can be installed with: `pip install great_tables` The documentation site at [https://posit-dev.github.io/great-tables/articles/intro.html](https://posit-dev.github.io/great-tables/articles/intro.html) has a \*Get Started\* tutorial that will walk you through the basics of using the package. The API documentation is also available on the site and it is chock full of examples that'll show you how to use the various features of the package. We want Great Tables to be really useful for your work and so we really value any and all feedback. Send us a note in [https://github.com/posit-dev/great-tables/issues](https://github.com/posit-dev/great-tables/issues) or [https://github.com/posit-dev/great-tables/discussions](https://github.com/posit-dev/great-tables/discussions) anytime you like! &#x200B; https://preview.redd.it/vwuu06h5ui4c1.png?width=988&format=png&auto=webp&s=50e381e692bc16775eb1f6a4f6de309d75048139 \------- This was originally posted by Rich Iannone over at LinkedIn. He asked me to post this here.	2.0	t3_18biyrh	reddit		
544	üö® Minecraft Server Builder üö®	Unknown	2023-12-06 11:59:20	https://www.reddit.com/r/Python/comments/18c2fsn/minecraft_server_builder/	Building an optimised minecraft purpur server is made easier using the dank.tool! GitHub: https://github.com/SirDank/dank.tool Demonstration: https://youtube.com/shorts/RbX5UNuwAYw?si=7JyDZkadGj8brEwY	0.0	t3_18c2fsn	reddit		
545	I made a Python Advent Calendar	Unknown	2023-12-05 17:31:11	https://www.reddit.com/r/Python/comments/18bhb81/i_made_a_python_advent_calendar/	"I've been wanting to do this for so long! I've been teaching Python for data science & software engineering for 8 years. There's a lot of common mistakes I see people making, especially reinventing the wheel. If you've ever had the experience of spending weeks writing some beautiful code to solve a problem, only to later discover a package that does the exact same thing, this is for you! You can see all my posts so far here: [https://py-advent-calendar.beehiiv.com/](https://py-advent-calendar.beehiiv.com/) Posts are replicated in full with working code samples as Jupyter Notebooks on GitHub here: [https://github.com/CoefficientSystems/python-advent-calendar/](https://github.com/CoefficientSystems/python-advent-calendar/) Here's what I've written so far: * **Day 1: Mastering Notifications with Python.** Send notifications to your desktop or mobile. Handy when you have long-running computational tasks and you want to make a brew or go for a walk. üå≥ * **Day 2: Pandas Flavor Chains.** We introduce the Modern Pandas style, the benefits of method chaining, and a lovely library called pandas\_flavor. * **Day 3: Transform Your Data Transformations.** The PyJanitor library includes a bunch of useful helper methods for data cleaning in pandas, such as an auto-column-name-cleaner. * **Day 4: Everyone's utils.py**. Python has lots of functionality built-in, but not everything makes the cut for the standard library. Those functions that didn't make it are all in the boltons library. * **Day 5: Cache Me If You Can.** We cover caching techniques in the standard library, and an introduction to using joblib for caching, parallelisation, and serialisation. I'd love to have some feedback on this too, or if you have suggestions for those ""under-rated libraries more people should know about"" let me know and I'll take a look! &#x200B; https://preview.redd.it/gouao1dzdi4c1.png?width=1024&format=png&auto=webp&s=e01c5c8d5ced51be61054fd29828e77eeb0202a2"	3.0	t3_18bhb81	reddit		
546	Just completed this gift	Unknown	2023-12-05 21:01:12	https://www.reddit.com/r/Python/comments/18bm8ko/just_completed_this_gift/	"My SO is a huge Sacramento Kings fan so I wanted to make her a gift. The stadium has a recent tradition of lighting a giant beam after a win. Shout out to [u/fireplug911](https://www.reddit.com/user/fireplug911/) as I came across their [awesome project](https://www.reddit.com/r/kings/comments/zn0xl3/here_are_the_files_to_3d_print_your_own_golden_1/) and decided to take it a slight step further by incorporating an mcu running micropython to monitor the Kings schedule and automatically ""light the beam"". https://i.redd.it/08rlqwr0jj4c1.gif Nothing too crazy -- just wanted to share as I thought ya'll might get a kick out of it! Also, [repo](https://github.com/Tbruno25/micro-beam) if interested, although it leaves a lot to be desired."	0.0	t3_18bm8ko	reddit		
547	Generate SQL from Natural Language according Meta Data of Database in Python using LLM in Very Few Codes	Unknown	2023-12-06 01:01:25	https://www.reddit.com/r/Python/comments/18brs71/generate_sql_from_natural_language_according_meta/	"## Show Case Story: Answering questions from bosses or coworkers about business with unprepared data is too often a situation for a business analyst or a data scientist. Sometimes it takes too much time and chops your work time into too small chunks. But what more can you do? **Bosses and coworkers cannot write SQL for themselves, so they are counting on you!** But what if you have an AI-Agent to **write SQL from natural language questions**? Does that help you a lot? Take a look at this showcase and explore how it can help. ## Information of Data Tables We got a MySQL database with two tables like this: |||Table 1: user_info||| ---|---|---|---|--- column name | column desc | data type | data example | annotation user_id | user identity code | string | ""u_123456"" | gender | user gender | int | 0 | 0: female, 1: male age | user age | int | 18 | customer_level | user's customer level | int | 4 | 1: copper, 2: silver, 3: gold, 4: platinum, 5: vvip reg_time | when did user register | timestamp | 1701340135721 | reg_channel | which channel did user come from | int | 0 | 0: natural, 1: game ads, 2: video ads, 3: streaming live ads, 4: offline post ads |||Table 2: order ||| ---|---|---|---|--- column name | column desc | data type | data example | annotation order_id | order identity code | string | ""o_456789"" | customer_user_id | user identity of who paid this order | string | ""u_234567"" | item_name | item's name | string | ""HD Vanilla Icecream"" | item_class_tags | tags of item | JSON String | [""food"", ""snack"", ""frozen""] | standard JSON string format price | price of 1 item | float | 1.35 | unit: $ order_item_number | how many items in this order | int | 3 | order_time | when was this order created | timestamp | 1701341789164 | pay_time | when was this order paid | timestamp | 1701341835323 | value should be 0 if not paid As you can see, those two tables are very common in E-commerce system. ## How to Code? Download Our Package from Pypi using `pip`: ```shell pip install -U Agently ``` Then you can start coding: ```python import Agently from datetime import datetime agent_factory\ .set_settings(""model.OpenAI.auth"", { ""api_key"": """" }) # Create a meta data dict of those two tables: ## Usually we can get the meta data from our system by requesting APIs ## instead of typing them down below manually meta_data = [ { ""table_name"": ""user_info"", ""columns"": [ { ""name"": ""user_id"", ""desc"": ""user identity code"", ""data type"": ""string"", ""example"": ""u_123456"" }, { ""name"": ""gender"", ""desc"": ""user gender"", ""data type"": ""int"", ""example"": 0, ""annotation"": ""0: female, 1: male"" }, { ""name"": ""age"", ""desc"": ""user age"", ""data type"": ""int"", ""example"": 18 }, { ""name"": ""customer_level"", ""desc"": ""user's customer level"", ""data type"": ""int"", ""example"": 4, ""annotation"": ""1: copper, 2: silver, 3: gold, 4: platinum, 5: vvip"" }, { ""name"": ""reg_time"", ""desc"": ""when did user register"", ""data type"": ""timestamp"", ""example"": 1701340135721 }, { ""name"": ""reg_channel"", ""desc"": ""which channel did user come from"", ""data type"": ""int"", ""example"": 0, ""annotation"": ""0: natural, 1: game ads, 2: video ads, 3: streaming live ads, 4: offline post ads"" }, ], }, { ""table_name"": ""order"", ""columns"": [ { ""name"": ""order_id"", ""desc"": ""order identity code"", ""data type"": ""string"", ""example"": ""o_456789"" }, { ""name"": ""customer_user_id"", ""desc"": ""user identity of who paid this order"", ""data type"": ""string"", ""example"": ""u_234567"" }, { ""name"": ""item_name"", ""desc"": ""item's name"", ""data type"": ""string"", ""example"": ""HD Vanilla Icecream"" }, { ""name"": ""item_class_tags"", ""desc"": ""tags of item"", ""data type"": ""JSON String"", ""example"": ""[\""food\"", \""snack\"", \""frozen\""]"", ""annotation"": ""standard JSON string format"" }, { ""name"": ""price"", ""desc"": ""price of 1 item"", ""data type"": ""float"", ""example"": 1.35, ""annotation"": ""unit: $"" }, { ""name"": ""order_item_number"", ""desc"": ""how many items in this order"", ""data type"": ""int"", ""example"": 3 }, { ""name"": ""order_time"", ""desc"": ""when was this order created"", ""data type"": ""timestamp"", ""example"": 1701341789164 }, { ""name"": ""pay_time"", ""desc"": ""when was this order paid"", ""data type"": ""timestamp"", ""example"": 1701341835323 }, ], } ] ## As you can see, this `meta_data` dict has a complex structure ## But it's OK, because Agently framework can help you handle the complex structure ## You can pass the dict to .info() or .input() like a variable directly ## instead of trying to figure out how to prompt or how to make model understand agent = agent_factory.create_agent() print(""Ask questions about data using natural language.\nInput '#exit' if you want to stop.\n------\n"") while True: user_input = input(""[Question]: "") if user_input == ""#exit"": break result = agent\ .input(user_input)\ .info({ ""database meta data"": meta_data })\ .info({ ""current date"": datetime.now().date() })\ .instruct(""Generate SQL for MySQL database according {database meta data} to answer {input}"")\ .output({ ""thinkings"": [(""String"", ""Your thinkings step by step about how to query data to answer {input}"")], ""SQL"": (""String"", ""SQL String without explanation""), })\ .start() ## And of course the result is a structure dict you can use it easily print(""[Thinking Process]: "") for thinking in result[""thinkings""]: print(thinking) print(""[SQL]: \n"", result[""SQL""]) print(""------\n"") ``` Output Log Example: [User]: What's the user revenue of those who came because ads in games last month? [Thinking Process]: We need to retrieve the revenue of users who came because ads in games last month. To do this, we need to join the 'user_info' and 'order' tables on the user identity code. We also need to filter the orders based on the 'reg_channel' column from the 'user_info' table, where the value is 1 (game ads) and the 'pay_time' column from the 'order' table is within the last month. We can calculate the revenue by multiplying the 'price' column with the 'order_item_number' column in the 'order' table. Finally, we need to group the revenue by user and sum it to get the total revenue of those users. [SQL]: SELECT ui.user_id, SUM(o.price * o.order_item_number) AS revenue FROM user_info ui JOIN order o ON ui.user_id = o.customer_user_id WHERE ui.reg_channel = 1 AND o.pay_time >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH) GROUP BY ui.user_id [User]: How many vvip bought frozen snack this year? [Thinking Process]: I need to find out the user IDs who bought frozen snack this year Then, I can count the number of VVIP users among them [SQL]: SELECT COUNT(*) AS vvip_buyers FROM user_info WHERE user_id IN (SELECT DISTINCT customer_user_id FROM orders WHERE YEAR(FROM_UNIXTIME(pay_time/1000)) = YEAR(CURRENT_DATE()) AND JSON_CONTAINS(item_class_tags, 'frozen')) AND customer_level = 5 ## Try by yourself Colab Document: [Use Google Colab to try it by yourself](https://github.com/Maplemx/Agently/blob/main/playground/sql_generator.ipynb) ## Want to Explore More? - Visit Our [Github Homepage](http://Agently.tech) --- You can discuss this showcase or tell us what other showcases you want to see üôÇ"	4.0	t3_18brs71	reddit		
548	A brand new Starlette+Svelte+Tailwind Python based webdev framework: Easy to use, fast, and responsive	Unknown	2023-12-06 03:19:26	https://www.reddit.com/r/Python/comments/18buivv/a_brand_new_starlettesveltetailwind_python_based/	Announcing release of Ofjustpy webdev framework ([https://github.com/ofjustpy/ofjustpy/](https://github.com/ofjustpy/ofjustpy/)). Its super easy to use, is fast and is designed to be responsive. Checkout live demo at ([https://ofjustpy.webworks.monallabs.in/](https://ofjustpy.webworks.monallabs.in/)) which is build entirely using Ofjustpy. There are many optimizations baked into the framework that make serving webpages space efficient, speed, and respond quickly to changes. Whats next: I am exploring which svelte UI library to incorporate. If you have preference/choices let me know. I am also going to benchmark the framework against other frameworks. What numbers would be meaningful. Let me know in comments.	2.0	t3_18buivv	reddit		
549	Remote execution of code	Unknown	2023-12-05 10:14:26	https://www.reddit.com/r/Python/comments/18b8wtj/remote_execution_of_code/	Is there a python IDE which can execute the code on a remote server and get the result back? So on the server there should be running a remote daemon for handling the requests. And the solution should be ready to use out of the box. If possible SSH should not be used. Edit: thanks for hints about SSH firewalls, blocked SSH, SSH port numbers, intensive use of SSH, no-SSH-trolls, SSH denier and so on. My solution seems to be jupyter desktop. Thanks u/NewDateline	31.0	t3_18b8wtj	reddit		
550	Will Pandas have streaming in Future??	Unknown	2023-12-05 05:59:20	https://www.reddit.com/r/Python/comments/18b5bsg/will_pandas_have_streaming_in_future/	As Pandas has switched to arrow backend from version 2.0, is there a possibility that we can see Lazy Evaluation or streaming in Pandas so that we'll be able to process datasets larger than memory on a machine as we have in Polars??	11.0	t3_18b5bsg	reddit		
551	GrandTourer ‚Äì a CLI tool for easily launching applications on macOS	Unknown	2023-12-05 23:29:05	https://www.reddit.com/r/Python/comments/18bpsly/grandtourer_a_cli_tool_for_easily_launching/	# GrandTourer GrandTourer is a CLI tool for easily launching applications on MacOS with a two letter command: ```gt```. It's a drop-in replacement for ```open -a``` - with string matching. ## Installation Use the package manager [pip](https://pip.pypa.io/en/stable/) to install GrandTourer. GrandTourer requires Python >=3.8. ```bash pip install grandtourer ``` ## Usage Type gt followed by the first few letters of your application name. Spaces, capitals, it doesn't matter - GrandTourer will find the relevant application. If there is more than one possible match, you might need to include a few more letters. ```shell $ gt saf # Launches Safari $ gt skadawb No apps found $ gt cal Did you mean one of the following? calculator calendar ``` Source Code on GitHub: [https://github.com/WillDenby/GrandTourer](https://github.com/WillDenby/GrandTourer) PyPI Page: [https://pypi.org/project/GrandTourer/](https://pypi.org/project/GrandTourer/)	2.0	t3_18bpsly	reddit		
552	Open-Sourcing High-Frequency Trading and Market-Making Backtesting Tool	Unknown	2023-12-05 13:20:53	https://www.reddit.com/r/Python/comments/18bbtj6/opensourcing_highfrequency_trading_and/	[https://www.github.com/nkaz001/hftbacktest](https://www.github.com/nkaz001/hftbacktest) I know that numerous backtesting tools exist. But most of them do not offer comprehensive tick-by-tick backtesting, taking latencies and order queue positions into account. Consequently, I developed a new backtesting tool that concentrates on thorough tick-by-tick backtesting while incorporating latencies, order queue positions, and complete order book reconstruction. **Key features:** * Working in Numba JIT function. * Complete tick-by-tick simulation with a variable time interval. * Full order book reconstruction based on L2 feeds(Market-By-Price). * Backtest accounting for both feed and order latency, using provided models or your own custom model. * Order fill simulation that takes into account the order queue position, using provided models or your own custom model. **Example:** Here's an example of how to code your algorithm using HftBacktest. For more examples including market-making and comprehensive tutorials, please visit the documentation page [here](https://hftbacktest.readthedocs.io/en/latest/index.html). @njit def simple_two_sided_quote(hbt, stat): max_position = 5 half_spread = hbt.tick_size * 20 skew = 1 order_qty = 0.1 last_order_id = -1 order_id = 0 # Checks every 0.1s while hbt.elapse(100_000): # Clears cancelled, filled or expired orders. hbt.clear_inactive_orders() # Obtains the current mid-price and computes the reservation price. mid_price = (hbt.best_bid + hbt.best_ask) / 2.0 reservation_price = mid_price - skew * hbt.position * hbt.tick_size buy_order_price = reservation_price - half_spread sell_order_price = reservation_price + half_spread last_order_id = -1 # Cancel all outstanding orders for order in hbt.orders.values(): if order.cancellable: hbt.cancel(order.order_id) last_order_id = order.order_id # All order requests are considered to be requested at the same time. # Waits until one of the order cancellation responses is received. if last_order_id >= 0: hbt.wait_order_response(last_order_id) # Clears cancelled, filled or expired orders. hbt.clear_inactive_orders() last_order_id = -1 if hbt.position < max_position: # Submits a new post-only limit bid order. order_id += 1 hbt.submit_buy_order( order_id, buy_order_price, order_qty, GTX ) last_order_id = order_id if hbt.position > -max_position: # Submits a new post-only limit ask order. order_id += 1 hbt.submit_sell_order( order_id, sell_order_price, order_qty, GTX ) last_order_id = order_id # All order requests are considered to be requested at the same time. # Waits until one of the order responses is received. if last_order_id >= 0: hbt.wait_order_response(last_order_id) # Records the current state for stat calculation. stat.record(hbt)	1.0	t3_18bbtj6	reddit		
553	Live Plotting and Transformations with Jupyter using Python	Unknown	2023-12-05 13:48:49	https://www.reddit.com/r/Python/comments/18bcce4/live_plotting_and_transformations_with_jupyter/	A use case we have been working on with data scientists is to create dashboards sourced from live data, e.g. stock tickers, logs, chats, and news. Get from Github: [https://github.com/pathwaycom/pathway-examples/blob/main/showcases/live-data-jupyter.ipynb](https://github.com/pathwaycom/pathway-examples/blob/main/showcases/live-data-jupyter.ipynb) Why do I think it‚Äôs cool? You can run algorithms and analysis on streaming data (from Kafka, RedPanda, Azure/EventHub etc.) directly in Jupyter, - you get an interactive way of working and prototyping with live data, - you can do replays on static data. See how it works: https://i.redd.it/4iq1ipf6dh4c1.gif Read more here: [https://pathway.com/developers/showcases/live\_data\_jupyter](https://pathway.com/developers/showcases/live_data_jupyter)	1.0	t3_18bcce4	reddit		
554	Wednesday Daily Thread: Beginner questions	Unknown	2023-12-06 00:00:12	https://www.reddit.com/r/Python/comments/18bqi1q/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	1.0	t3_18bqi1q	reddit		
555	Johnnydep (v1.20.4) Package Update	Unknown	2023-12-06 08:27:49	https://www.reddit.com/r/Python/comments/18bzixi/johnnydep_v1204_package_update/	In case you're new to the Johnnydep package, it displays package dependency trees, which is very helpful for developers that only use pip for package updates, etc. With the latest release (v1.20.4), the Johnnydep package has resolved a show stopper error previously experienced by Windows users from the previous update. [https://pypi.org/project/johnnydep/](https://pypi.org/project/johnnydep/) &#x200B;	4.0	t3_18bzixi	reddit		
556	Performance	Unknown	2023-12-05 20:24:32	https://www.reddit.com/r/Python/comments/18bldaz/performance/	Hi all. Anyone know how of a place where I can get some benchmarking on ‚Äúnormal‚Äù python scripts tuned for thread processing on one machine vs untuned out of box functionality of scripts running on a databrick cluster?	1.0	t3_18bldaz	reddit		
557	Youtube voice exchange tool	Unknown	2023-12-05 19:56:29	https://www.reddit.com/r/Python/comments/18bkp29/youtube_voice_exchange_tool/	Made a [tool](https://github.com/KoljaB/TurnVoice) that can replace voices in youtube videos with another one. Usage from CLI with youtube id or url, as easy as: turnvoice RK91Ji6GCZ8 This would turn musks voice [into female](https://youtu.be/XOV7Yoj9jVg). * can also translate * use any target voice, clone with short \~10s audio sample * free/open-source (noncommercial) * full local (no internet connection or api keys needed) Star the repo pls.	1.0	t3_18bkp29	reddit		
558	Transforming Live Data Streams in Jupyter Notebooks: A Pythonic Approach to Trading Algorithms	Unknown	2023-12-05 12:26:23	https://www.reddit.com/r/Python/comments/18bavun/transforming_live_data_streams_in_jupyter/	Hey Pythonistas, this project explores an interesting concept in Jupyter notebooks ‚Äì implementing a trading algorithm using Bollinger Bands. **GitHub Link for quick access:** [https://github.com/pathwaycom/pathway-examples/blob/main/showcases/live-data-jupyter.ipynb](https://github.com/pathwaycom/pathway-examples/blob/main/showcases/live-data-jupyter.ipynb) The core idea revolves around processing live data streams to compute the Volume Weighted Average Price (VWAP) and its standard deviation, creating dynamic trading bands. The algorithm triggers buy/sell actions based on the price's proximity to these bands, coupled with a volume threshold for reliability. Python's powerful libraries are used, like \`datetime\`, \`bokeh\`, and \`panel\`, along with \`pathway\`, an open source Pythonic framework for real-time data processing. For a detailed walkthrough of the code here's the [original blogpost](https://pathway.com/developers/showcases/live_data_jupyter). Dive in and explore how Python can power up your data processing and visualization skills! **Quick glimpse of what the notebook covers:** * Data Source Setup: Streamlining data from a CSV file and setting up a computational graph. * Rolling Statistics: Calculating 1-minute and 20-minute VWAP and standard deviation. * Alert Triggering: Joining statistics and setting up conditions for trading alerts. * Visualization: Creating an interactive Bokeh plot and Panel table for real-time data visualization. The result is a real-time updated dashboard, showcasing the practical application of Python in financial data analysis and algorithmic trading. Looking forward to your thoughts and discussions!	1.0	t3_18bavun	reddit		
559	[New Project]Speed Up Your LLM Native Application Development in Python	Unknown	2023-12-05 16:27:54	https://www.reddit.com/r/Python/comments/18bfupr/new_projectspeed_up_your_llm_native_application/	"Hey guys, I'd like to introduce our work recently - an Development Framework to speed up LLM Based Application development. You can create an LLM based instance in your python code and interact with it using natural language and passing structure data as variables without transform as the same time. It's super easy! **Quick Start Code Example** Pypi Installation: ```shell pip install -U Agently ``` Quick Start Code ```python # Import Package and Init Settings import Agently agent = Agently.create_agent() agent\ .set_settings(""current_model"", ""OpenAI"")\ .set_settings(""model.OpenAI.auth"", { ""api_key"": """" }) # Interact with the agent instance like calling a function result = agent\ .input(""Give me 3 words"")\ .output([(""String"", ""one word"")])\ .start() print(result) ``` **Why this framework?** - üöÄ **Develop Fast**: You can create LLM native application in very few code. - üí¨ **Easy to Use**: It's easy to interact with the instance in code using structure data and chained-calls syntax - üß© **Easy to Enhance**: You can enhance the instance using plugins instead of rebuild a whole new agent. - üîì **Open Source**: Oh Yes, it is open-source project and plugin development friendly, contributors are welcome to come and share your ideas! Github: [Click here to visit](http://agently.tech) Welcome to share your opinions about this project with us!üòÑ"	0.0	t3_18bfupr	reddit		
560	Why is ¬¥print¬¥ not recommended in linters?	Unknown	2023-12-04 12:14:11	https://www.reddit.com/r/Python/comments/18aiy0v/why_is_print_not_recommended_in_linters/	I am writing a mini-program for basic payment calculation, and after the calculation the results are printed in the terminal. However, I get the following warnings from Ruff (the Python linter that I use): src/pf_example/food_payment.py:39:5: T201 `print` found src/pf_example/food_payment.py:51:5: T201 `print` found src/pf_example/food_payment.py:52:5: T201 `print` found src/pf_example/food_payment.py:54:5: T201 `print` found src/pf_example/food_payment.py:56:9: T201 `print` found I know that I can turn off this check in the settings, BUT I don't why print is bad in the code. What would be the alternatives if not using print?	23.0	t3_18aiy0v	reddit		
561	automate your prompt queries. set up as many consecutive prompts as you want, input as much prompt data into those prompts as you want.	Unknown	2023-12-05 07:18:03	https://www.reddit.com/r/Python/comments/18b6jbg/automate_your_prompt_queries_set_up_as_many/	>**Abstract AI** [https://github.com/AbstractEndeavors/abstract-ai](https://github.com/AbstractEndeavors/abstract-ai) https://preview.redd.it/inumu7jagf4c1.png?width=1920&format=png&auto=webp&s=b7edd74d8e48202766f7991db976ba11b98d57ec # Dynamic Data Chunking & API Query Handler ## Overview This repository presents a sophisticated code example engineered to efficiently process extensive datasets via an intelligent chunking algorithm, tailored for API interactions where data size and query constraints are predominant. It assures a smooth operation with minimal user input. ## Key Features ## Dual Input System * `Request` and `Prompt Data` sections for straightforward data incorporation. * Automatic division of prompt data into manageable chunks based on user-specified parameters. ## Intelligent Chunking * Dynamically segments data considering the set percentage for expected completion per API query and the maximum token limit. * Executes iterative queries through a response handler class until data processing completes. ## Iterative Query Execution * Handles documents split into multiple chunks (e.g., 14 chunks result in at least 14 API queries), with real-time adaptive query decisions. ## Instruction Set * `bot_notation`: allows the module to create notes about the current data chunk to be recieved upon the next query, this is such that they can keep context, and understand why the previous selections were made. * `additional_response`: Allows repeated query execution until a specified condition is met, bypassing token limitations. * `select_chunks`: allows the module to review either the previous or next chunk of data alongside the current or by itself, if needed, the loop will essentially impliment additional\_response for this. * `token_size_adjustment`: allows the module to adjust the size of the chunks being sent, this is a neat feature because they do get finicky about this and it can be used in combination with any of the above. * `abort`: Authorizes termination of the query loop to conserve resources. * `suggestions`: Provides a system for leaving future improvement notes. ## Autonomy & Efficiency Empowers modules with significant autonomy for managing large data volumes efficiently, ensuring the output is streamlined and user post-processing is minimal. ## User Convenience Simplifies user involvement by automating data chunking and handling multiple prompts in a single operation. The modules are also equipped to independently address typical query-related issues. ## Conclusion Developers seeking to automate and refine data handling for API-centric applications will find this repository a valuable asset. It's crafted to mitigate common data processing challenges and implement proactive solutions for enhanced user and module performance.	1.0	t3_18b6jbg	reddit		
562	Bring LLMs directly into your database!	Unknown	2023-12-05 12:12:20	https://www.reddit.com/r/Python/comments/18banrs/bring_llms_directly_into_your_database/	Hi Python community, Today, we are launching our SuperDuperDB, a completely open-source framework for integrating AI directly with major databases, including streaming inference, scalable model training, and vector search. This tool should greatly help this community in integrating AI directly into their favourite database! I would greatly appreciate your support: Please share the launch post on LinkedIn: [https://www.linkedin.com/feed/update/urn:li:activity:7137754336897449984](https://www.linkedin.com/feed/update/urn:li:activity:7137754336897449984) (tag anyone who could be interested in the project) Share the repo with your network and communities: [https://github.com/SuperDuperDB/superduperdb](https://github.com/SuperDuperDB/superduperdb) ***(leave a star if you didn‚Äôt yet, of course :)***	0.0	t3_18banrs	reddit		
563	Tetris game in python and Pygame	Unknown	2023-12-04 16:29:37	https://www.reddit.com/r/Python/comments/18anw7j/tetris_game_in_python_and_pygame/	I made a Tetris game using Python and Pygame Code: [https://github.com/DataWizual/Tetris](https://github.com/DataWizual/Tetris) Here's the video explaining how I did it: [https://www.youtube.com/watch?v=yoswOWjkRF4](https://www.youtube.com/watch?v=yoswOWjkRF4)	0.0	t3_18anw7j	reddit		
564	Explainable (Structured) Machine Learning Algorithm	Unknown	2023-12-05 03:01:18	https://www.reddit.com/r/Python/comments/18b22xk/explainable_structured_machine_learning_algorithm/	Just for some respite from the discussion of our soon-to-be AI overlords (LLMs), I'm one of the contributors to an open-source Python package, Xplainable ([https://github.com/xplainable/xplainable](https://github.com/xplainable/xplainable)). Xplainable is a novel (structured) machine learning algorithm that's inherently explainable, as opposed to being a post-hoc explainer (like SHAP or Lime). From our initial baseline testing, Xplainable is competitive with XGBoost and LightGBM for classification metrics like AUC and F1. This is despite the challenge of direct comparisons due to varying feature engineering requirements and dataset challenges. In regression tasks, Xplainable's performance ranges from surpassing Decision Trees to matching XGBoost in terms of MAE. We've also just released V1.1, which inherently handles NaN values, allowing you to fit it directly to your data without the need to drop or impute missing values. * Examples trained on Kaggle datasets here - [Xplainable Tutorials](https://docs.xplainable.io/docs/category/tutorials). I've included the Altair html output so you can view the explainer plots in the docs webpage.	0.0	t3_18b22xk	reddit		
565	Senior Python Devs: How much do you actually know vs googling?	Unknown	2023-12-03 21:40:55	https://www.reddit.com/r/Python/comments/18a46i0/senior_python_devs_how_much_do_you_actually_know/	So Senior Python Devs.... how much of python do you actually know like the back of your hand and how much do you google? What are some terms you use when googling? Explain how your knowledge scaled. What was most difficult learning experience while learning Python? And whatever else you can think of...	73.0	t3_18a46i0	reddit		
566	Tuesday Daily Thread: Advanced questions	Unknown	2023-12-05 00:00:12	https://www.reddit.com/r/Python/comments/18aybv2/tuesday_daily_thread_advanced_questions/	# Weekly Wednesday Thread: Advanced Questions üêç Dive deep into Python with our Advanced Questions thread! This space is reserved for questions about more advanced Python topics, frameworks, and best practices. ## How it Works: 1. **Ask Away**: Post your advanced Python questions here. 2. **Expert Insights**: Get answers from experienced developers. 3. **Resource Pool**: Share or discover tutorials, articles, and tips. ## Guidelines: * This thread is for **advanced questions only**. Beginner questions are welcome in our [Daily Beginner Thread](#daily-beginner-thread-link) every Thursday. * Questions that are not advanced may be removed and redirected to the appropriate thread. ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **How can you implement a custom memory allocator in Python?** 2. **What are the best practices for optimizing Cython code for heavy numerical computations?** 3. **How do you set up a multi-threaded architecture using Python's Global Interpreter Lock (GIL)?** 4. **Can you explain the intricacies of metaclasses and how they influence object-oriented design in Python?** 5. **How would you go about implementing a distributed task queue using Celery and RabbitMQ?** 6. **What are some advanced use-cases for Python's decorators?** 7. **How can you achieve real-time data streaming in Python with WebSockets?** 8. **What are the performance implications of using native Python data structures vs NumPy arrays for large-scale data?** 9. **Best practices for securing a Flask (or similar) REST API with OAuth 2.0?** 10. **What are the best practices for using Python in a microservices architecture? (..and more generally, should I even use microservices?)** Let's deepen our Python knowledge together. Happy coding! üåü	0.0	t3_18aybv2	reddit		
567	map-nl: Quickly create PC4 maps of the Netherlands	Unknown	2023-12-04 08:09:35	https://www.reddit.com/r/Python/comments/18afn9r/mapnl_quickly_create_pc4_maps_of_the_netherlands/	"I just released a new small hobby project: map-nl [Github](https://github.com/fpgmaas/map-nl) It is a Python package that helps users quickly create maps of the Netherlands at the Postal Code 4 level. Nothing groundbreaking, but fun to develop and hopefully useful to some nonetheless. For example, creating a choropleth map of the Netherlands becomes as simple as: import pandas as pd from map_nl import ChoroplethMapNL df = pd.read_csv(""https://raw.githubusercontent.com/fpgmaas/map-nl/main/data/woz-pc4.csv"") m = ChoroplethMapNL(geojson_simplify_tolerance=0.0001).plot( df, pc4_column_name=""pc4"", value_column_name=""WOZ"", legend_name=""Average WOZ Value"" ) m.save(""map.html"") Curious for your thoughts, please let me know if you have any feedback!"	0.0	t3_18afn9r	reddit		
568	Visual Explanation of Python Panda Library	Unknown	2023-12-04 06:08:40	https://www.reddit.com/r/Python/comments/18adyz9/visual_explanation_of_python_panda_library/	Coveres all the basic concepts of panda library like import/export create modify dataframe, add/edit/update/delete rows/coloumns, select through columns names, index, loc, iloc, iat, query and other advance methods, group by, multi-level index, and much more. [https://solothought.com/tutorial/python-pandas-visual/](https://solothought.com/tutorial/python-pandas-visual/) \#pandas #quickreference https://preview.redd.it/83x7u8rty74c1.jpg?width=2693&format=pjpg&auto=webp&s=4d5090f0703fded41dae3b2e41af6aab173971c6	3.0	t3_18adyz9	reddit		
569	Modern cookiecutter template for Python CLI apps	Unknown	2023-12-04 12:11:33	https://www.reddit.com/r/Python/comments/18aiwky/modern_cookiecutter_template_for_python_cli_apps/	I've just built cookiecutter python cli template that use &#x200B; üñ•Ô∏è Typer as main cli libarary. üì¶ Poetry for dependancy management. ü§ñ Pre-commit with Ruff for code formatting and MyPy as type checker. ü¶∫ Pytest for unit testing. üêã Docker setup. &#x200B; Checkout at [https://github.com/chamoda/cookiecutter-typer](https://github.com/chamoda/cookiecutter-typer)	1.0	t3_18aiwky	reddit		
570	Introducing 'Python-GPT-PO': A Simple Yet Powerful Python Tool for Multilingual File Translation Using OpenAI's GPT Models	Unknown	2023-12-04 11:17:49	https://www.reddit.com/r/Python/comments/18ai3rn/introducing_pythongptpo_a_simple_yet_powerful/	"Hey r/Python, I've developed a tool, ""python-gpt-po,"" now available on GitHub. It's a simple, Python-based application for translating files into multiple languages using OpenAI's GPT models. This tool is great for individuals or small teams needing straightforward, cost-effective translation. [CLI in Action](https://i.redd.it/tx1qxfsyh94c1.gif) **Features** * Bulk Translation Mode: Enhances efficiency by facilitating the translation of multiple text entries simultaneously. * Individual Translation Mode: Offers the flexibility to translate entries one at a time for greater precision. * Configurable Batch Size: Allows users to set the number of entries to be translated in each batch during bulk translation. * Comprehensive Logging: Logs detailed information for progress monitoring and debugging purposes. * Fuzzy Entry Exclusion: Enables the option to omit 'fuzzy' entries from translation in .po files. * Flexible API Key Configuration: Supports providing the OpenAI API key either through command-line arguments or a .env file. &#x200B; It's integrated with ChatGPT for accurate, context-aware translations, bridging language barriers efficiently. I'm eager for your feedback, suggestions, or questions! Check it out and share your thoughts! [https://github.com/pescheckit/python-gpt-po/](https://github.com/pescheckit/python-gpt-po/)"	1.0	t3_18ai3rn	reddit		
571	RMount allows you to create a file-system like connection to a cloud storage i.e. S3 or SSH server	Unknown	2023-12-03 20:43:05	https://www.reddit.com/r/Python/comments/18a2sn8/rmount_allows_you_to_create_a_filesystem_like/	RMount is a lot more robust than existing libraries and methods to create a remote connection to a cloud server. For example you would want to use Python to connect to a cloud storage such as S3 or a remote server via SSH and treat it like a local file-system. *Note* It has been a pain to write this library given how many things can go wrong when treating a cloud storage as a local file-system. For example, connection drops, or writing of large files can make the system unresponsive and lead to loss of data. The library is supposed to take care of such cases but it is always subject to unforeseen failures. [Feel free to try it out](https://github.com/fostiropoulos/rmount) or `pip install rmount` [Examples](https://github.com/fostiropoulos/rmount/tree/main/examples) [Google Colab](https://colab.research.google.com/github/fostiropoulos/rmount/blob/main/examples/s3.ipynb)	4.0	t3_18a2sn8	reddit		
572	I've made a simple DSL inside Python to make patterns for matching in spaCy easier	Unknown	2023-12-04 18:09:40	https://www.reddit.com/r/Python/comments/18aq6bi/ive_made_a_simple_dsl_inside_python_to_make/	Link: [https://github.com/InAnYan/spacy\_pat\_match\_dsl](https://github.com/InAnYan/spacy_pat_match_dsl) Inspired by [lrparsing](https://pypi.org/project/lrparsing/). Example: ```python class Example(PatternsGrammar): single_unit = lower('meter') | lower('second') | lower('kilometer') | lower('meter') | lower('kilometers') compound_unit = single_unit + lower('per') + single_unit UNIT = single_unit | compound_unit QUANTITY = pos('NUM') + UNIT ``` For more details read the example file in the repository. Pros: * Simple. * No dependencies. * DSL is integrated to Python. Cons: * Untested. * Unoptimized. What are your thoughts? Probably there is somebody who made something similar. UPDATE: Forgot the term. It is an embedded DSL.	1.0	t3_18aq6bi	reddit		
573	check this out	Unknown	2023-12-04 14:08:54	https://www.reddit.com/r/Python/comments/18akyt6/check_this_out/	check out clrflow, a new text formatting and tools module for python, personally better than colorama or other modules like pycolor. it includes gradients, loading bars, text alignments, custom print functions, ... currently working on v1.2 with some performance and file size improvements, will most likely be coming to c(#/++) in the future [https://github.com/rver38/clrflow](https://github.com/rver38/clrflow) pip install clrflow	0.0	t3_18akyt6	reddit		
574	Request for comments: LLM FOO for easing OpenAI tool use.	Unknown	2023-12-04 14:02:21	https://www.reddit.com/r/Python/comments/18aku6p/request_for_comments_llm_foo_for_easing_openai/	Facing complexities with OpenAI's APIs? I made a small project called LLM FOO to help with tool / function calling. The project aims to reduce verbosity and ease documentation efforts with the tool decorator. It's a work in progress, and I'm eager for your insights and contributions. Let's improve it together! Link to the project is here [https://github.com/robocorp/llmfoo](https://github.com/robocorp/llmfoo) and you can also install it from pypi \`pip install llmfoo\`.	1.0	t3_18aku6p	reddit		
575	SSHaMan: A TUI for Managing SSH connections using Python & Textual	Unknown	2023-12-04 02:17:15	https://www.reddit.com/r/Python/comments/18a9w1m/sshaman_a_tui_for_managing_ssh_connections_using/	Good evening everyone. I just wrapped up my first open source project. It's functional at the moment, but definitely still alpha. It's a tool that you can use to manage SSH connections from inside the terminal. I'm a pretty avid self-hoster with... a lot... of self-hosted services and keeping track of all my VMs is kind of painful with a .bashrc file. So I thought I'd feed two birds with one scone here and make an application to help that and also learn the Textual framework. This application presently doesn't do much that their demos don't do (I was able to repurpose their FileTree and just extend a few buttons), but I plan on digging in and learning Textual in subsequent versions to make this even more useful. I would welcome any feedback! If anyone would like to contribute, I am absolutely open to PRs as well! EDIT: D'oh! Forgot the link! https://github.com/cornyhorse/sshaman	2.0	t3_18a9w1m	reddit		
576	guiml - The future GUI framework for python?	Unknown	2023-12-04 17:35:54	https://www.reddit.com/r/Python/comments/18apdup/guiml_the_future_gui_framework_for_python/	You know the problem when you want to develop a game, but instead of developing a game you end up developing a game engine? Well I kind of have this problem. I didn't quite want to code a game, but a slightly fancy todo app with integrated pomodoro timer, history of time spend and XP for some gameification of my everyday work tasks. Python is great for some quick programming, except that when you start GUI development. GUI development in python always feels less pythonic to me and with pythonic I mean simple and quick to write. At that point I should have done the sane thing and start learning some Qt for python or Kivy. Actually, I tried. But when looking at the docs I was starting to daydream and wishing for something more like vue.js or Angular, which are quite successful in web development. Using one of them would also have been very sensible, but then I would have to use typescript. Of course, there are also nice frameworks that bridge the gap between python and the web, such that you don't have to touch any html or javascript. But then it still runs in the browser and not as a stand alone app (unless you ship the browser with your app, hello electron), which makes things like file access needlessly difficult. Clearly, given these circumstances it is completely reasonable to start developing my own GUI framework. From scratch. Who needs mature GUI libraries, which have been developed over the last two decades, like Qt or GTK+ anyway, right? Right? Ok, maybe lets not start completely from scratch. Using something to do windows and input (pyglet), something for drawing (cairo), text setting (pango) and loading svg (librsvg) should still be fair game. A few hundred hours later... I am done! What? No not with the fancy todo list I originally wanted to program (which a skilled programmer could probably do in a few hours). I am done with the first version of my GUI framework, that doesn't crash immediately. And a plain and simple todo list. [guiml](https://github.com/StephanGocht/guiml) [guiml tutorial](https://guiml.readthedocs.io/en/latest/tutorial.html) Did something like this happen to you before? Would you also like to have nicer GUI frameworks for python, or are you happy with what we have? Do you know of other libraries that provide a component based markup language for python GUIs? If you have some time to spend I would also be happy about some feedback about the project. How do you like the idea/ concepts (see the tutorial)? Do you think this is something you would use if it were more mature? Also, feel free to ask question, so I can fill an FAQ.	1.0	t3_18apdup	reddit		
577	Fastest Screen Capturing library for Python checkout windows-capture	Unknown	2023-12-03 16:44:02	https://www.reddit.com/r/Python/comments/189xeqy/fastest_screen_capturing_library_for_python/	"I was building an AI for ""help"" in video games and I found out that most Python screen-capturing libraries are very slow so I made one in Rust here is the repository: https://github.com/NiiightmareXD/windows-capture/tree/main/windows-capture-python And here is the [benchmark](https://github.com/NiiightmareXD/windows-capture/tree/main/windows-capture-python#benchmark)"	9.0	t3_189xeqy	reddit		
578	Introducing Dependency Injections in Robyn with a Twist!	:robyn-logo: Robyn Maintainer	2023-12-03 22:09:33	https://www.reddit.com/r/Python/comments/18a4tpv/introducing_dependency_injections_in_robyn_with_a/	"Hi All üëã I'm excited to share that the latest version of Robyn now includes dependency injections but with a unique syntax. For those who might not be familiar, [Robyn](https://github.com/sparckles/Robyn) is a fast, asynchronous Python backend web framework that operates with a Rust runtime, combining the best of both worlds for efficient and robust web development. Here's a sample example to illustrate the new feature: ```python from robyn import Robyn, ALLOW\_CORS app = Robyn(__file__) GLOBAL_DEPENDENCY = ""GLOBAL DEPENDENCY"" ROUTER_DEPENDENCY = ""ROUTER DEPENDENCY"" app.inject(ROUTER_DEPENDENCY=ROUTER_DEPENDENCY) app.inject_global(GLOBAL_DEPENDENCY=GLOBAL_DEPENDENCY) @app.get(""/sync/global_di"") def sync_global_di(request, router_dependencies, global_dependencies): return global_dependencies[""GLOBAL_DEPENDENCY""] ``` It was one of the most requested features, and I'm curious to hear what the community thinks about this new approach. Any feedback or thoughts are greatly appreciated! And as usual, the project can be found at: https://github.com/sparckles/Robyn"	2.0	t3_18a4tpv	reddit		
579	Python code using ChatGPT	Unknown	2023-12-04 22:54:32	https://www.reddit.com/r/Python/comments/18awu6l/python_code_using_chatgpt/	"Something I had meant to ask ever since I started using chatgpt extensively to code complex algorithms. How much does a python developer ""remember"" popular libraries and syntaxes, and how much are they able to code up themselves without referring to chatgpt? Let's say your project turned out to be 1000 lines of code, what percentage of it did you ask chatgpt to generate for you? My programming experience: I have been in python journey for almost a year. Have complete a formal TAFE course on foundational python and a bunch of courses informally on YouTube, etc. I did VBA projects a few years ago, by looking up Google on individual requirements within that project for instance, how to create a bar chart in cell D2 of tab ""charts"" , or how to save a workbook after password protecting it and close it, etc etc. I am not a programmer; I am a data analyst. I am proud of myself to be an avid learner. With python, there is so much to remember, so I have given up remembering, instead I ask chatgpt to generate a code based on a complex requirement. Issue is that when I have to think on my feet how to start writing the code, even after the whole year of practicing, I still cant. Perhaps because I know I have chatgpt to support me or I am short on time to deliver so I resort to chatgpt for a quick resolution. I feel if one day google or chatgpt is taken away from me, I can't write a simple algorithm. I am nervous."	17.0	t3_18awu6l	reddit		
580	Framework-agnostic e-mail sending library	Unknown	2023-12-03 10:51:44	https://www.reddit.com/r/Python/comments/189r4z3/frameworkagnostic_email_sending_library/	Hey everyone. For as long as I was using Python, the only good e-mail sending library was django's builtin one. Then someone created flask-mailman, which is based on django's one but integrated with flask. I've been writing some framework-agnostic code lately (non-web service) and will soon have to deal with sending e-mails again. Do you know of any good options or are we still tied to using a web framework if we want to send some e-mails?	8.0	t3_189r4z3	reddit		
581	Why OOP is a terrible idea for mathematical research? (SageMath, I am looking at you)	Unknown	2023-12-04 20:51:47	https://www.reddit.com/r/Python/comments/18au01e/why_oop_is_a_terrible_idea_for_mathematical/	Before we start, let me explain some background. &#x200B; I am a mathematician specialized in combinatorics and representation theory. I code fairly often. Recently, I wanted to do some affine Coxeter system computation. The SageMath module took me almost 2 weeks before it exhausts my SSD space for virtual memory. Then I rewrote the code in C11, and after -O2 optimization, it only took 3.6 seconds! The Python code and the C code are using the same exact algorithms. &#x200B; * What exactly makes OOP so bad? I don't think OOP is adding too much overhead to the algorithm itself. Rather, OOP distorts memory layout and significantly increases memory footprint. Say, you need 10\^8 or more tiny objects containing a small matrix and a bunch of parameters. In OOP, these objects are everywhere around the heap and attached to them are a bunch of pointers (to, e.g. vtable) and some reference counters. On the other hand, in C, these objects are tightly stored together with zero memory overhead. Matrices are compressed using bit fields (because matrix entries are very small integers). &#x200B; * It seems pure C is still the best option for mathematical research for any sort of complicated computation It seems to me that memory layout and memory footprint are often more impactful than the algorithms. In OOP languages, you have very little control over these stuff; rather, OOP paradigm bloats memory usage and reduces locality. To actually solve the problem, you really have to manually design how *everything* is stored, and pass the address of objects to functions/methods. Even C++ is not great at this. You do need pure C. &#x200B; * The problem with SageMath I appreciate how SageMath incorporates various existing math libraries under the interface of Python and JupyterLab. It is indeed a good idea to write low level libraries in pure C and then interactively use them from Python. The real issue of SageMath is it tries to encapsulate every mathematical object, no matter how small they are, as Python objects. They made integers objects. They made polynomials objects, containing a bunch of other objects (the coefficients). The made individual Coxeter group element objects. The memory bloat in SageMath is just insane. Looking at their code, they have sophisticated OOP hierarchy to make all abstract and concrete math concepts Python objects. They may be using the best algorithm available written in C/C++. Their algorithms are fine. But SageMath is still very wrong. They should completely ignore Python objects, and wrap C/C++ function as it is.	16.0	t3_18au01e	reddit		
582	Monday Daily Thread: Project ideas!	Unknown	2023-12-04 00:00:11	https://www.reddit.com/r/Python/comments/18a77w3/monday_daily_thread_project_ideas/	"# Weekly Thread: Project Ideas üí° Welcome to our weekly Project Ideas thread! Whether you're a newbie looking for a first project or an expert seeking a new challenge, this is the place for you. ## How it Works: 1. **Suggest a Project**: Comment your project idea‚Äîbe it beginner-friendly or advanced. 2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code. 3. **Explore**: Looking for ideas? Check out Al Sweigart's [""The Big Book of Small Python Projects""](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration. ## Guidelines: * Clearly state the difficulty level. * Provide a brief description and, if possible, outline the tech stack. * Feel free to link to tutorials or resources that might help. # Example Submissions: ## Project Idea: Chatbot **Difficulty**: Intermediate **Tech Stack**: Python, NLP, Flask/FastAPI/Litestar **Description**: Create a chatbot that can answer FAQs for a website. **Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM) # Project Idea: Weather Dashboard **Difficulty**: Beginner **Tech Stack**: HTML, CSS, JavaScript, API **Description**: Build a dashboard that displays real-time weather information using a weather API. **Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8) ## Project Idea: File Organizer **Difficulty**: Beginner **Tech Stack**: Python, File I/O **Description**: Create a script that organizes files in a directory into sub-folders based on file type. **Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/) Let's help each other grow. Happy coding! üåü"	0.0	t3_18a77w3	reddit		
583	Node Editor options	Unknown	2023-12-03 23:11:45	https://www.reddit.com/r/Python/comments/18a67kc/node_editor_options/	Hi all, I'm planning on creating a software that requires node editing functionality. I've used Dear PyGUI before and I'm aware that it supports node editing (however, I haven't used it before). &#x200B; I was wondering what are the other available options and what are their pros & cons. &#x200B; Thanks!	4.0	t3_18a67kc	reddit		
584	Python module that would operate computer like self operating computer using computer vision	Unknown	2023-12-04 02:44:55	https://www.reddit.com/r/Python/comments/18aaemr/python_module_that_would_operate_computer_like/	So we have multiple modules like selenium etc that automate the computer / web page interaction. Now, self operating computer script that was just released tries to explain to gpt4 what it is looking at on computer screen with gpt 4 vision and allows it to use the computer according to a prompt. It seems to me that there should be a module that does the same. Explains to llm what it can do with prompts using mouse / keyboard. Customized to different os's. So then all we would have to do is load the module and be able to do something similar for our own special needs. Is anyone working on something like this?	1.0	t3_18aaemr	reddit		
585	Open-source custom League of Legends announcer, first release.	Unknown	2023-12-03 19:10:31	https://www.reddit.com/r/Python/comments/18a0nyc/opensource_custom_league_of_legends_announcer/	Dear Python community, today I have released a Python program which enables the user to play their own custom sounds in parallel with in-game events based on the official LoL API. Further program functionality includes; Turn on LoL executable, mute native game announcer, adjust custom announcer volume and auto-accept matches. If you are interested please check my Github project link and try it out, any feedback is welcome. Note: This has been created as a code learning project and is not perfect. [Github project](https://github.com/Sightless123/LoL-Announcer) &#x200B; https://preview.redd.it/cztdww6cp44c1.png?width=1093&format=png&auto=webp&s=9ca54a5a5839db5602bd9b2120bce146947774f7 Best regards, Sightless\_123	0.0	t3_18a0nyc	reddit		
586	Just found a library for Rust-style errors in Python	Unknown	2023-12-02 16:44:00	https://www.reddit.com/r/Python/comments/1897m0y/just_found_a_library_for_ruststyle_errors_in/	Was dreading having to do try/except for a bunch of stuff and found this Rust-style errors lib: [https://github.com/rustedpy/result/](https://github.com/rustedpy/result/) Anyone else converted?	22.0	t3_1897m0y	reddit		
587	Fast TTS library	Unknown	2023-12-02 23:06:54	https://www.reddit.com/r/Python/comments/189fme6/fast_tts_library/	If you want to use TTS consider [RealtimeTTS](https://github.com/KoljaB/RealtimeTTS). * open-source * built for speed, flexibility, and ease of use * handles both text and llm streams * supports popular TTS providers incl free ones See [demo](https://www.youtube.com/watch?v=K91NZig24BI). More infos: see comment below.	3.0	t3_189fme6	reddit		
588	Function composition in a different way	Unknown	2023-12-02 23:04:41	https://www.reddit.com/r/Python/comments/189fknz/function_composition_in_a_different_way/	Let's pick lottery numbers. That is to pick 6 numbers from 1 to 45. _The Lotto_ [_(korean lottery)_](https://en.lottolyzer.com/home/south-korea/6_slash_45-lotto) ```python >>> range(1, 46) | choice(size=6) | sort [2, 8, 22, 24, 37, 39] # This is one game. People usually buy five games at once. >>> [ range(1, 46) | choice(size=6) | sort for _ in range(5) ] [[4, 6, 11, 38, 41, 45], [5, 8, 23, 25, 26, 40], [13, 18, 23, 25, 37, 44], [17, 21, 24, 32, 41, 43], [5, 9, 13, 25, 30, 38]] # Or equivalently, # Feel Unix pipelines >>> replicate(5, range(1, 46)) | map(choice(size=6)) | map(sort) | unpack # Feel Haskell-like mathematical symbol >>> (unpack . map(sort . fx(choice(size=6))))(replicate(5, range(1, 46))) ``` [https://github.com/thyeem/foc](https://github.com/thyeem/foc) I recently wrote a post with `foc` on `r/haskell`, and I got some good ideas from there. If you're interested, take a look. Any opinions are welcome.	3.0	t3_189fknz	reddit		
589	How often do you need to shrink multimedia files in your web projects?	Unknown	2023-12-03 06:22:30	https://www.reddit.com/r/Python/comments/189nepx/how_often_do_you_need_to_shrink_multimedia_files/	\[SURVEY\] Hi everyone. If you do follow this practices, which tools do you use ? (Would you prefer a bulit in Python solution?) Any different to images or not so common file format caused you trouble? Perhaps stumbled upon some annoying file conversion? Is it something you do recurrently? I read you!	2.0	t3_189nepx	reddit		
590	Sunday Daily Thread: What's everyone working on this week?	Unknown	2023-12-03 00:00:12	https://www.reddit.com/r/Python/comments/189go42/sunday_daily_thread_whats_everyone_working_on/	# Weekly Thread: What's Everyone Working On This Week? üõ†Ô∏è Hello /r/Python! It's time to share what you've been working on! Whether it's a work-in-progress, a completed masterpiece, or just a rough idea, let us know what you're up to! ## How it Works: 1. **Show & Tell**: Share your current projects, completed works, or future ideas. 2. **Discuss**: Get feedback, find collaborators, or just chat about your project. 3. **Inspire**: Your project might inspire someone else, just as you might get inspired here. ## Guidelines: * Feel free to include as many details as you'd like. Code snippets, screenshots, and links are all welcome. * Whether it's your job, your hobby, or your passion project, all Python-related work is welcome here. ## Example Shares: 1. **Machine Learning Model**: Working on a ML model to predict stock prices. Just cracked a 90% accuracy rate! 2. **Web Scraping**: Built a script to scrape and analyze news articles. It's helped me understand media bias better. 3. **Automation**: Automated my home lighting with Python and Raspberry Pi. My life has never been easier! Let's build and grow together! Share your journey and learn from others. Happy coding! üåü	9.0	t3_189go42	reddit		
591	Thoughts Robust Python Write Clean and Maintainable Code	Unknown	2023-12-02 17:45:59	https://www.reddit.com/r/Python/comments/1898xjj/thoughts_robust_python_write_clean_and/	Anyone who read Robust Python Write Clean and Maintainable Code by Patrick Viafore? What is your thought about this?	2.0	t3_1898xjj	reddit		
592	Let's Debug Your Neural Network: Gradient-based Symbolic Execution for NN	Unknown	2023-12-02 16:08:18	https://www.reddit.com/r/Python/comments/1896vj9/lets_debug_your_neural_network_gradientbased/	I have developed [Gymbo](https://github.com/Koukyosyumei/Gymbo), a proof of concept for a Gradient-based Symbolic Execution Engine implemented from scratch. Symbolic is a method used to analyze a program and identify the inputs that trigger the execution of each program section. It works well when debugging the behavior of neural networks. For example, Gymbo allows you to find adversarial examples easily. It is interesting to see that the logic-based method effectively analyzes neural networks. Gymbo also offers easy-to-use Python API compatible with sklearn and PyTorch. I am looking forward to your feedback!	1.0	t3_1896vj9	reddit		
593	What was for you the biggest thing that happened in the Python ecosystem in 2023?	Unknown	2023-12-01 16:13:26	https://www.reddit.com/r/Python/comments/188fo2w/what_was_for_you_the_biggest_thing_that_happened/	Of course, there was Python 3.12, but I'm not only talking about version releases or libraries but also about projects that got big this year, events, etc... EDIT : so nobody cared about pandas 2, mojo or python in Excel ?	30.0	t3_188fo2w	reddit		
594	"2,000 free sign ups available for the ""Automate the Boring Stuff with Python"" online course. (Dec 2023)"	"Author of ""Automate the Boring Stuff"""	2023-12-01 20:07:35	https://www.reddit.com/r/Python/comments/188l3cu/2000_free_sign_ups_available_for_the_automate_the/	"EDIT: The codes are all used up. But you can still watch the videos on the course page by scrolling down and clicking Preview (the preview is the entire video, and previews have been enabled for all videos.) You just won't have access to the forums and quizzes. If you want to learn to code, I've released 2,000 free sign ups for my course following my Automate the Boring Stuff with Python book (each has 1,000 sign ups, use the other one if one is sold out): ~~https:// udemy. com/course/automate/?couponCode=DEC2023FREE~~ ~~https:// udemy. com/course/automate/?couponCode=DEC2023FREE2~~ If you are reading this after the sign ups are used up, you can always find [the first 15 of the course's 50 videos are free on YouTube if you want to preview them.](https://www.youtube.com/watch?v=1F_OgqRuSdI&list=PL0-84-yl1fUnRuXGFe_F7qSH1LEnn9LkW) YOU CAN ALSO WATCH THE VIDEOS WITHOUT SIGNING UP FOR THE COURSE. All of the videos on the course webpage have ""preview"" turned on. Scroll down to find and click ""Expand All Sections"" and then click the preview link. You won't have access to the forums and other materials, but you can watch the videos. **NOTE: Be sure to BUY the course for $0, and not sign up for Udemy's subscription plan. The subscription plan is free for the first seven days and then they charge you. It's selected by default. If you are on a laptop and can't click the BUY checkbox, try shrinking the browser window. Some have reported it works in mobile view.** **I'm also working on another Udemy course** that follows my recent book ""Beyond the Basic Stuff with Python"". So far I have [the first 15 of the planned 56 videos done. You can watch them for free on YouTube.](https://www.youtube.com/watch?v=kSrnLbioN6w&list=PL0-84-yl1fUmeV_2bBSguF_S0TVZk8wow&index=1) **Frequently Asked Questions:** (*read this before posting questions*) * This course is for beginners and assumes no previous programming experience, but the second half is useful for experienced programmers who want to learn about various third-party Python modules. * If you don't have time to take the course now, that's fine. Signing up gives you lifetime access so you can work on it at your own pace. * This Udemy course covers roughly the same content as the 1st edition book (the book has a little bit more, but all the basics are covered in the online course), which you can read for free online at https://inventwithpython.com * The 2nd edition of Automate the Boring Stuff with Python is free online: https://automatetheboringstuff.com/2e/ * I do plan on updating the Udemy course, but it'll take a while because I have other book projects I'm working on. If you sign up for this Udemy course, you'll get the updated content automatically once I finish it. It won't be a separate course. * It's totally fine to start on the first edition and then read the second edition later. I'll be writing a blog post to guide first edition readers to the parts of the second edition they should read. * **You're not too old to learn to code. You don't need to be ""good at math"" to be good at coding.** * Signing up is the first step. Actually finishing the course is the next. :) [There are several ways to get/stay motivated.](https://www.reddit.com/r/learnprogramming/wiki/faq#wiki_how_can_i_get.2Fstay_motivated_to_learn_programming.3F) I suggest getting a ""gym buddy"" to learn with. Check out /r/ProgrammingBuddies"	24.0	t3_188l3cu	reddit		
595	Python‚Äôs standard logging API violates PEP-8 and this PEP proposes to fix this.	Unknown	2023-12-01 13:34:27	https://www.reddit.com/r/Python/comments/188bziq/pythons_standard_logging_api_violates_pep8_and/	Feedback and criticism of all sort is welcome on [this PEP](https://github.com/metaperl/peps/blob/main/peps/pep-8106.rst) which represents a first stab at addressing this issue.	25.0	t3_188bziq	reddit		
596	Best framework for a phone routing api endpoint?	Unknown	2023-12-01 22:20:36	https://www.reddit.com/r/Python/comments/188o5h0/best_framework_for_a_phone_routing_api_endpoint/	I'm in the process of restructuring our phone routing API endpoint while we're in a relatively slow part of the year, with the hopes of making it much, much faster than it was originally built to be. It has to connect with our mysql db multiple times as well as 10+ different external endpoints checking the status of all the call centers we potentially route to, and right now we use Flask... I initially was getting ready to switch to FastAPI but as I dug around a little I learned Django could be better for how frequently it connects with databases. Would love to hear what you guys would pick for this and why.	6.0	t3_188o5h0	reddit		
597	Saturday Daily Thread: Resource Request and Sharing! Daily Thread	Unknown	2023-12-02 00:00:26	https://www.reddit.com/r/Python/comments/188qc6t/saturday_daily_thread_resource_request_and/	"# Weekly Thread: Resource Request and Sharing üìö Stumbled upon a useful Python resource? Or are you looking for a guide on a specific topic? Welcome to the Resource Request and Sharing thread! ## How it Works: 1. **Request**: Can't find a resource on a particular topic? Ask here! 2. **Share**: Found something useful? Share it with the community. 3. **Review**: Give or get opinions on Python resources you've used. ## Guidelines: * Please include the type of resource (e.g., book, video, article) and the topic. * Always be respectful when reviewing someone else's shared resource. ## Example Shares: 1. **Book**: [""Fluent Python""](https://www.amazon.com/Fluent-Python-Concise-Effective-Programming/dp/1491946008) \- Great for understanding Pythonic idioms. 2. **Video**: [Python Data Structures](https://www.youtube.com/watch?v=pkYVOmU3MgA) \- Excellent overview of Python's built-in data structures. 3. **Article**: [Understanding Python Decorators](https://realpython.com/primer-on-python-decorators/) \- A deep dive into decorators. ## Example Requests: 1. **Looking for**: Video tutorials on web scraping with Python. 2. **Need**: Book recommendations for Python machine learning. Share the knowledge, enrich the community. Happy learning! üåü"	1.0	t3_188qc6t	reddit		
598	Am I stupid because I don‚Äôt have a CS degree?	Unknown	2023-12-02 22:26:09	https://www.reddit.com/r/Python/comments/189es24/am_i_stupid_because_i_dont_have_a_cs_degree/	I am currently employed in data management and have been using Python (specifically pandas and numpy) and SQL regularly for work for the past 6 years but sometimes I feel stupid because I lack the terminology since I am self-taught and do not have a CS degree even though I have created and deployed web apps, created internal dashboards, automated tasks/reports, etc. All of my supervisors have been happy with my work so am I just overthinking things or am I stupid because I don‚Äôt have a CS degree?	66.0	t3_189es24	reddit		
599	This week pip Trends newsletter is out. Interesting things are covered this week, Some of them: Python Lambda, Chatbot to speak with YouTube Videos	Unknown	2023-12-02 03:27:24	https://www.reddit.com/r/Python/comments/188uhn1/this_week_pip_trends_newsletter_is_out/	[https://open.substack.com/pub/piptrends/p/fastapi-has-65k-github-stars-now?r=1rq0kg&utm\_campaign=post&utm\_medium=web](https://open.substack.com/pub/piptrends/p/fastapi-has-65k-github-stars-now?r=1rq0kg&utm_campaign=post&utm_medium=web)	0.0	t3_188uhn1	reddit		
600	the eval game	Unknown	2023-12-01 05:55:10	https://www.reddit.com/r/Python/comments/1884j69/the_eval_game/	"[https://oskaerik.github.io/theevalgame/](https://oskaerik.github.io/theevalgame/) I made a Python game inspired by ""The Password Game"", highlighting some of the more obscure aspects of the language. Give it a try and test your skills (or maybe creativity...) üòâ I'm happy to receive any feedback!"	11.0	t3_1884j69	reddit		
601	List of Python projects and source code	Unknown	2023-12-01 06:34:37	https://www.reddit.com/r/Python/comments/18857ql/list_of_python_projects_and_source_code/	We Just started a new app that will help beginners to practice and also provide a list of python projects with examples for free. If you are interested to check out please download it and provide your valuable review. This will be great for us If you are interested you can download it from the below link https://play.google.com/store/apps/details?id=com.ideasorblogs.app	6.0	t3_18857ql	reddit		
602	Best method to share Python environments on Windows machine	Unknown	2023-12-01 01:53:51	https://www.reddit.com/r/Python/comments/187ztin/best_method_to_share_python_environments_on/	We have a group of users on a single Windows server (using separate accounts) hoping to create, share and use Python virtual environments for their work. I know Anaconda has a solution for this: [https://docs.anaconda.com/free/navigator/tutorials/manage-environments/](https://docs.anaconda.com/free/navigator/tutorials/manage-environments/) But I'm curious if there is another, better, easier way to do this which will minimize my time as the sysadmin! Thanks in advance!	21.0	t3_187ztin	reddit		
603	I made YouTube in Python	Unknown	2023-12-01 12:05:01	https://www.reddit.com/r/Python/comments/188aa4r/i_made_youtube_in_python/	[https://www.youtube.com/watch?v=GA0621y0zUQ](https://www.youtube.com/watch?v=GA0621y0zUQ) &#x200B; Youtube clone with 1. Views 2. Likes 3. Dislikes 4. Comment 5. Multiple channels under same user 6. Subscribers	7.0	t3_188aa4r	reddit		
604	If Python is built in C/C++, why can't it be built from source cross-platform?	Unknown	2023-11-30 15:09:58	https://www.reddit.com/r/Python/comments/187kr86/if_python_is_built_in_cc_why_cant_it_be_built/	I was looking into the built from source docs: [https://devguide.python.org/getting-started/setup-building/](https://devguide.python.org/getting-started/setup-building/) And it seems we don't have simple cross platform make command or CLI command. Why is this so hard to do? Is there a tutorial where I can compile from source on linux for windows, linux and mac?	19.0	t3_187kr86	reddit		
605	Revolutionize localization with LangSync	Unknown	2023-12-01 13:37:30	https://www.reddit.com/r/Python/comments/188c1uf/revolutionize_localization_with_langsync/	Revolutionize localization with LangSync: AI-Powered, Swift, Simple, and Accurate. Elevate your projects with just one click. Goodbye to the old, hello to effortless localization! üöÄ \- [LangSync Website](https://langsync.app)	0.0	t3_188c1uf	reddit		
606	Advent of Code 2023 - r/Python Edition	:litestar-logo: Litestar Maintainer & :ruff-logo: Ruff Fanboy 	2023-11-30 21:11:52	https://www.reddit.com/r/Python/comments/187t9yq/advent_of_code_2023_rpython_edition/	# Join the 2023 Advent of Code Challenge with Python! Hey Pythonistas! üêç It's almost that exciting time of the year again! The [Advent of Code 2023](https://adventofcode.com/2023) is just around the corner, and we're inviting everyone to join in the fun! ## What is Advent of Code? Advent of Code is an annual online event that runs from December 1st to December 25th. Each day, a new coding challenge is released‚Äîtwo puzzles that are part of a continuing story. It's a fantastic way to improve your coding skills and get into the holiday spirit! You can read more about it [here](https://adventofcode.com/2023/about). ## Why Python? Python is a great choice for these challenges due to its readability and wide range of libraries. Whether you're a beginner or an experienced coder, Python makes solving these puzzles both fun and educational. ## How to Participate? 1. [**Sign Up/In**](https://adventofcode.com/2023/auth/login)**.** 2. Join the r/Python private leaderboard with code `2186960-67024e32` 3. Start solving the puzzles released each day using ***Python.*** 4. **Share your solutions and discuss strategies with the community.** ## Join the r/Python Leaderboard! We can have up to 200 people in a private leaderboard, so this may go over poorly - but you can join us with the following code: `2186960-67024e32` ## How to Share Your Solutions? You can join the [Python Discord](https://discord.gg/python) to discuss the challenges, share your solutions, or you can post in the r/AdventOfCode mega-thread for solutions. There will be a stickied post for each day's challenge. Please follow their subreddit-specific rules. Also, shroud your solutions in spoiler tags >!like this!< ## Resources ## Community * [Python official Documentation](https://docs.python.org) for Python documentation. * [r/LearnPython](https://www.reddit.com/r/learnpython/) for Python learning resources and discussions. * [Python Discord](https://discord.gg/python) for Python discussions and help. ## AoC * [2023 Leaderboard](https://adventofcode.com/2023/leaderboard) * [AoC++](https://adventofcode.com/2023/support) to support the project * [AoC Subreddit](https://www.reddit.com/r/adventofcode/) for general discussions * [AoC Shop](https://advent-of-code.creator-spring.com/) for merch ## Python Discord The [Python Discord](https://discord.gg/python) will also be participating in this year's Advent of Code. Join it to discuss the challenges, share your solutions, and meet other *Pythonistas*. You will also find they've set up a Discord bot for joining in the fun by linking your AoC account.Check out their [Advent of Code FAQ channel](https://discord.com/channels/267624335836053506/1047672643584786442). Let's code, share, and celebrate this festive season with Python and the global coding community! üåü Happy coding! üéÑ P.S. - Any issues in this thread? Send us a modmail.	1.0	t3_187t9yq	reddit		
607	[I Wrote] An autohotkey like program in Python	Unknown	2023-11-30 18:42:18	https://www.reddit.com/r/Python/comments/187pqm3/i_wrote_an_autohotkey_like_program_in_python/	I'm not really a fan of the scripting syntax but the motivation wasn't anything wrong with Autohotkey. More that something from Python source is likley to be acceptable in a corporate environment rather than randomly installed programs. At the moment it just does: ## Text replacement. So typing something like: `:shruggie` will get replaced with `¬Ø\\\_(„ÉÑ)\_/¬Ø` ## Hotkeys The included demo uppercases a selected piece of text when `ctrl+shift+u` is pressed ## Plugins The above features are imlemented with a plugin system, so you can hang anything you can code in Python off a text replacement or hotkey. ## Todo I need to implement an option to make items threadable, returning control to the user ASAP. It's built on: * icoextract * pyautogui * pyperclip * infi.systray * pynput and can be built with nuitka Have fun and happy to take suggestions. [Repo here](https://github.com/bmrussell/pyautokey)	1.0	t3_187pqm3	reddit		
608	Friday Daily Thread: r/Python Meta and Free-Talk Fridays	Unknown	2023-12-01 00:02:45	https://www.reddit.com/r/Python/comments/187xdg6/friday_daily_thread_rpython_meta_and_freetalk/	# Weekly Thread: Meta Discussions and Free Talk Friday üéôÔ∏è Welcome to Free Talk Friday on /r/Python! This is the place to discuss the r/Python community (meta discussions), Python news, projects, or anything else Python-related! ## How it Works: 1. **Open Mic**: Share your thoughts, questions, or anything you'd like related to Python or the community. 2. **Community Pulse**: Discuss what you feel is working well or what could be improved in the /r/python community. 3. **News & Updates**: Keep up-to-date with the latest in Python and share any news you find interesting. ## Guidelines: * All topics should be related to Python or the /r/python community. * Be respectful and follow Reddit's [Code of Conduct](https://www.redditinc.com/policies/content-policy). ## Example Topics: 1. **New Python Release**: What do you think about the new features in Python 3.11? 2. **Community Events**: Any Python meetups or webinars coming up? 3. **Learning Resources**: Found a great Python tutorial? Share it here! 4. **Job Market**: How has Python impacted your career? 5. **Hot Takes**: Got a controversial Python opinion? Let's hear it! 6. **Community Ideas**: Something you'd like to see us do? tell us. Let's keep the conversation going. Happy discussing! üåü	0.0	t3_187xdg6	reddit		
609	Fliq: lightweight library for high-performance lazy processing of iterables, in fluent syntax	Unknown	2023-11-30 17:19:26	https://www.reddit.com/r/Python/comments/187ns2d/fliq_lightweight_library_for_highperformance_lazy/	Hey everyone, First - I love this community! Very cool seeing what people are publishing here and the helpful comment üôè I wanted to share an open source project I have been working on. I would love feedback to help make this python library awesome. I hope you'll find this useful :) With Fliq, instead of writing code that looks like this: next(map(lambda x: x * 2, filter(lambda x: x % 2 == 0, [1, 2, 3, 4, 5])), -1) You can write this: from fliq import q (q([1, 2, 3, 4, 5]) .where(lambda x: x % 2 == 0) .select(lambda x: x * 2) .first_or_default(default=-1)) Here is some info: Fliq is a lightweight Python library for high-performance lazy processing of iterables. Inspired by [Django's ORM](https://docs.djangoproject.com/en/4.2/topics/db/queries/) and [LINQ](https://learn.microsoft.com/en-us/dotnet/standard/linq/), it provides a fluent syntax for lazily-evaluated operations on iterables, and it is tested to have on-par performance with the standard library. Also, for all you type-a-holics, Fliq is fully equipped with generic type hints, so it supports mypy in strict mode. * Documentation: [https://oribarilan.github.io/fliq](https://oribarilan.github.io/fliq) * Source Code: [https://github.com/oribarilan/fliq](https://github.com/oribarilan/fliq) ## Fliq is * üí° **Intuitive** to use. Built for readability and usability. Fully typed. * ü™∂ **Lightweight** wrapper for the standard library. No dependencies or bloat. * ‚ö°Ô∏è **Efficient** as the standard library. Abstraction overhead is kept to a minimum. * ‚è≥ **Lazy** evaluated, executed only *when* needed and only *as* needed. * üîó **Versatile** by supporting any iterable type, including infinite iterables. * üß© **Compatible** with APIs consuming iterables. No integration or setup required.	2.0	t3_187ns2d	reddit		
610	Test runner for command line projects (based on Click's CliRunner)	Unknown	2023-11-30 21:55:22	https://www.reddit.com/r/Python/comments/187ud1g/test_runner_for_command_line_projects_based_on/	I love [Click](https://palletsprojects.com/p/click/) for building Python command line interfaces (CLI). One useful feature Click provides is a [CliRunner()](https://click.palletsprojects.com/en/8.1.x/testing/) helper class for testing. CliRunner invokes the script in an isolated environment allowing you to pass arguments, etc. Occasionally I work on projects that don't use Click. For example, if I want to provide a `python3 -m packagename` script that is ancillary to the primary use case and don't want to include Click as a dependency or want something that can be distributes as a stand-alone script without dependencies. In this case I'd usually use [argparse](https://docs.python.org/3/library/argparse.html). However, CLIs built with argparse (or old-fashioned manual `sys.argv` parsing) aren't nearly as easy to test. Thus I created [clirunner](https://github.com/RhetTbull/clirunner) which is a port of Click's test helper to work with non-Click apps. The API is compatible with Click's CliRunner so if you ever move a project to use Click, the tests should need little to no modification.	1.0	t3_187ud1g	reddit		
611	Generate SQL according Meta Data of Database	Unknown	2023-12-01 01:14:50	https://www.reddit.com/r/Python/comments/187yzic/generate_sql_according_meta_data_of_database/	"# Project [Agently Development Framework](https://github.com/Maplemx/Agently) - Speed up your AI Agent Native application development # What's the problem? Answering questions from bosses or coworkers about business with unprepared data is too often a situation for a business analyst or a data scientist. Sometimes it takes too much time and chops your work time into too small chunks. But what more can you do? Bosses and coworkers cannot write SQL for themselves, so they are counting on you! But what if you have an agent to write SQL from natural language questions? Does that help you a lot? Take a look at this showcase and explore how Agently agent can help. # Information of Data Tables We got a MySQL database with two tables like this: |||Table 1: user_info||| ---|---|---|---|--- column name | column desc | data type | data example | annotation user_id | user identity code | string | ""u_123456"" | gender | user gender | int | 0 | 0: female, 1: male age | user age | int | 18 | customer_level | user's customer level | int | 4 | 1: copper, 2: silver, 3: gold, 4: platinum, 5: vvip reg_time | when did user register | timestamp | 1701340135721 | reg_channel | which channel did user come from | int | 0 | 0: natural, 1: game ads, 2: video ads, 3: streaming live ads, 4: offline post ads |||Table 2: order ||| ---|---|---|---|--- column name | column desc | data type | data example | annotation order_id | order identity code | string | ""o_456789"" | customer_user_id | user identity of who paid this order | string | ""u_234567"" | item_name | item's name | string | ""HD Vanilla Icecream"" | item_class_tags | tags of item | JSON String | [""food"", ""snack"", ""frozen""] | standard JSON string format price | price of 1 item | float | 1.35 | unit: $ order_item_number | how many items in this order | int | 3 | order_time | when was this order created | timestamp | 1701341789164 | pay_time | when was this order paid | timestamp | 1701341835323 | value should be 0 if not paid As you can see, those two tables are very common in E-commerce system. # How to solve it? ```python import Agently from datetime import datetime agent_factory\ .set_settings(""model.OpenAI.auth"", { ""api_key"": """" }) # Create a meta data dict of those two tables: ## Usually we can get the meta data from our system by requesting APIs ## instead of typing them down below manually meta_data = [ { ""table_name"": ""user_info"", ""columns"": [ { ""name"": ""user_id"", ""desc"": ""user identity code"", ""data type"": ""string"", ""example"": ""u_123456"" }, { ""name"": ""gender"", ""desc"": ""user gender"", ""data type"": ""int"", ""example"": 0, ""annotation"": ""0: female, 1: male"" }, { ""name"": ""age"", ""desc"": ""user age"", ""data type"": ""int"", ""example"": 18 }, { ""name"": ""customer_level"", ""desc"": ""user's customer level"", ""data type"": ""int"", ""example"": 4, ""annotation"": ""1: copper, 2: silver, 3: gold, 4: platinum, 5: vvip"" }, { ""name"": ""reg_time"", ""desc"": ""when did user register"", ""data type"": ""timestamp"", ""example"": 1701340135721 }, { ""name"": ""reg_channel"", ""desc"": ""which channel did user come from"", ""data type"": ""int"", ""example"": 0, ""annotation"": ""0: natural, 1: game ads, 2: video ads, 3: streaming live ads, 4: offline post ads"" }, ], }, { ""table_name"": ""order"", ""columns"": [ { ""name"": ""order_id"", ""desc"": ""order identity code"", ""data type"": ""string"", ""example"": ""o_456789"" }, { ""name"": ""customer_user_id"", ""desc"": ""user identity of who paid this order"", ""data type"": ""string"", ""example"": ""u_234567"" }, { ""name"": ""item_name"", ""desc"": ""item's name"", ""data type"": ""string"", ""example"": ""HD Vanilla Icecream"" }, { ""name"": ""item_class_tags"", ""desc"": ""tags of item"", ""data type"": ""JSON String"", ""example"": ""[\""food\"", \""snack\"", \""frozen\""]"", ""annotation"": ""standard JSON string format"" }, { ""name"": ""price"", ""desc"": ""price of 1 item"", ""data type"": ""float"", ""example"": 1.35, ""annotation"": ""unit: $"" }, { ""name"": ""order_item_number"", ""desc"": ""how many items in this order"", ""data type"": ""int"", ""example"": 3 }, { ""name"": ""order_time"", ""desc"": ""when was this order created"", ""data type"": ""timestamp"", ""example"": 1701341789164 }, { ""name"": ""pay_time"", ""desc"": ""when was this order paid"", ""data type"": ""timestamp"", ""example"": 1701341835323 }, ], } ] # Create an agent to help ## As you can see, this `meta_data` dict has a complex structure ## But it's OK, because Agently framework can help you handle the complex structure ## You can pass the dict to .info() or .input() like a variable directly ## instead of trying to figure out how to prompt or how to make model understand agent = agent_factory.create_agent() print(""Ask questions about data using natural language.\nInput '#exit' if you want to stop.\n------\n"") while True: user_input = input(""[Question]: "") if user_input == ""#exit"": break result = agent\ .input(user_input)\ .info({ ""database meta data"": meta_data })\ .info({ ""current date"": datetime.now().date() })\ .instruct(""Generate SQL for MySQL database according {database meta data} to answer {input}"")\ .output({ ""thinkings"": [(""String"", ""Your thinkings step by step about how to query data to answer {input}"")], ""SQL"": (""String"", ""SQL String without explanation""), })\ .start() print(""[Thinking Process]: "") for thinking in result[""thinkings""]: print(thinking) print(""[SQL]: \n"", result[""SQL""]) print(""------\n"") ``` [[Use Google Colab to try by yourself](https://github.com/Maplemx/Agently/blob/main/playground/sql_generator.ipynb)]"	0.0	t3_187yzic	reddit		
612	Josh hutcherson but every time it hits the borders it doubles	Unknown	2023-11-30 18:52:42	https://www.reddit.com/r/Python/comments/187pzcc/josh_hutcherson_but_every_time_it_hits_the/	just felt bored and wrote this program. i really don't know why i did it repo: [https://github.com/Null-byte-00/Josh](https://github.com/Null-byte-00/Josh) youtube video: [https://www.youtube.com/watch?v=-CEfLUzrF5E](https://www.youtube.com/watch?v=-CEfLUzrF5E)	2.0	t3_187pzcc	reddit		
613	Qwikcrud - An AI-powered command-line tool that generates RESTful APIs and admin interfaces based on one-line requirement	Unknown	2023-11-30 15:27:28	https://www.reddit.com/r/Python/comments/187l534/qwikcrud_an_aipowered_commandline_tool_that/	Qwikcrud is a powerful command-line tool designed to enhance your backend development experience by automating the generation of comprehensive RESTful APIs and admin interfaces. ## How does qwikcrud work? Instead of relying on AI to generate the entire code, it empowers AI to identify entities and relationships. The magic then happens with a pre-defined template. This ensures you get robust and efficient code. ## Ready to give it a go? - [Check out the Source Code on GitHub](https://github.com/jowilf/qwikcrud) - [Watch the Demo](https://youtu.be/XYuLDk0bjQA) - [Explore an Example of Generated Code](https://github.com/jowilf/qwikcrud/tree/main/examples/fastapi/task-management) ## Curious to try the code generated in the demo online? - [https://qwikcrud-demo.jowilf.com/](https://qwikcrud-demo.jowilf.com/) I'd love to hear your thoughts and experiences with qwikcrud. Give it a try on your projects and share your feedback ‚Äì what worked, what you loved, and any suggestions you have.	0.0	t3_187l534	reddit		
614	Flask-Muck: Flask REST Framework that generates complete CRUD APIs for your SqlAlchemy models in as little as 9 lines of code.	Unknown	2023-11-29 07:39:13	https://www.reddit.com/r/Python/comments/186k0td/flaskmuck_flask_rest_framework_that_generates/	"""With Flask-Muck you don't have to worry about the CRUD. Flask-Muck is a batteries-included framework for automatically generating RESTful APIs with Create, Read, Update and Delete (CRUD) endpoints in a Flask/SqlAlchemy application stack."" Links: [GitHub](https://github.com/dtiesling/flask-muck/tree/v0.0.3b6) \- [Documentation](https://dtiesling.github.io/flask-muck/) \- [PyPi](https://pypi.org/project/Flask-Muck/) &#x200B; After years of working in Flask/SqlAlchemy/Marshmallow apps I felt like there was a large gap in the tooling for creating standard REST APIs that perform basic CRUD operations. Especially when comparing it to the Django ecosystem. I'm hoping this can remove large amounts of boilerplate for folks and get some projects bootstrapped and off the ground much more quickly. The library has just been released so I'm leaving the beta tag on the pypi releases for a bit until I get some strong feedback. I'm currently looking for early adopters to use it in personal/side projects. If anyone would like to take it for a spin and report back on any missing features or bugs I'd love to work with you. I don't have much to offer but would be happy to credit you as a contributor to the project. &#x200B;"	10.0	t3_186k0td	reddit		
615	What are the best Python libraries to make map visualizations?	Unknown	2023-11-29 08:39:25	https://www.reddit.com/r/Python/comments/186kux2/what_are_the_best_python_libraries_to_make_map/	I am interested in making either world maps or maps of countries and their states/provinces and color them based on certain values. Are there any Python libraries that are good at this or how would you recommend going about this?	15.0	t3_186kux2	reddit		
616	Thursday Daily Thread: Python Careers, Courses, and Furthering Education!	Unknown	2023-11-30 00:00:11	https://www.reddit.com/r/Python/comments/18744bp/thursday_daily_thread_python_careers_courses_and/	# Weekly Thread: Professional Use, Jobs, and Education üè¢ Welcome to this week's discussion on Python in the professional world! This is your spot to talk about job hunting, career growth, and educational resources in Python. Please note, this thread is **not for recruitment**. --- ## How it Works: 1. **Career Talk**: Discuss using Python in your job, or the job market for Python roles. 2. **Education Q&A**: Ask or answer questions about Python courses, certifications, and educational resources. 3. **Workplace Chat**: Share your experiences, challenges, or success stories about using Python professionally. --- ## Guidelines: - This thread is **not for recruitment**. For job postings, please see r/PythonJobs or the recruitment thread in the sidebar. - Keep discussions relevant to Python in the professional and educational context. --- ## Example Topics: 1. **Career Paths**: What kinds of roles are out there for Python developers? 2. **Certifications**: Are Python certifications worth it? 3. **Course Recommendations**: Any good advanced Python courses to recommend? 4. **Workplace Tools**: What Python libraries are indispensable in your professional work? 5. **Interview Tips**: What types of Python questions are commonly asked in interviews? --- Let's help each other grow in our careers and education. Happy discussing! üåü	1.0	t3_18744bp	reddit		
617	I made a tool that generates template for you typed function like golang	Unknown	2023-11-29 14:43:58	https://www.reddit.com/r/Python/comments/186r0gt/i_made_a_tool_that_generates_template_for_you/	[https://x.com/RomanMatweenko/status/1729872546483642456?s=20](https://x.com/RomanMatweenko/status/1729872546483642456?s=20) Suppose you've got a simple [\#python](https://twitter.com/hashtag/python?src=hashtag_click) function with types typed in. It'd be cool to have a tool that can generate test templates based on the types, right, like GOlang can? That's what this [project](https://github.com/RomanMIzulin/python_unit_generator/tree/master) about [original function we want to generate test template](https://preview.redd.it/eyob5tnrta3c1.png?width=938&format=png&auto=webp&s=d084afe0ce2460d553c3c18684ca5a5b5060fbea) [simple result](https://preview.redd.it/zhybl0orta3c1.png?width=1506&format=png&auto=webp&s=5c1371b3573adf0932973ed5e4597d74fdc80e19) VScode extension [https://github.com/RomanMIzulin/python\_unittest\_vscode](https://github.com/RomanMIzulin/python_unittest_vscode) can be found in market by unittest\_generator	3.0	t3_186r0gt	reddit		
618	Easily implement parallel training.	Unknown	2023-11-29 12:59:46	https://www.reddit.com/r/Python/comments/186otvf/easily_implement_parallel_training/	Note is a system for deep learning and reinforcement learning, supporting TensorFlow and PyTorch platforms, supporting non-parallel training and parallel training. Note makes the building and training of neural networks easy. [https://github.com/NoteDancing/Note](https://github.com/NoteDancing/Note)	1.0	t3_186otvf	reddit		
619	pytest parametrize vs highly repetitive code?	Unknown	2023-11-28 20:55:47	https://www.reddit.com/r/Python/comments/1866gj0/pytest_parametrize_vs_highly_repetitive_code/	In pytest ... or, for that matter, in the world of QA automation ... the coder is usually given the ability to parameterize variables that control the test cases. But at what point of adding successive parameters does this become overkill? What are your rules-of-thumb when deciding to write another test function or add a parameter (and perhaps an if-statement or more) to an existing test function? I've always leaned toward the parameterize approach, only to, in the end, wish I hadn't made the primary test function so complicated and full of special-case if-statements. Having four or even eight nearly-identical test functions goes against the code-reuse theme that permeates s/w engineering. But sometimes code repetition is both easier to read and maintain - particularly in the domain of writing test automation.	25.0	t3_1866gj0	reddit		
620	What are the best libraries to work with graphs?	Unknown	2023-11-28 14:32:35	https://www.reddit.com/r/Python/comments/185xexg/what_are_the_best_libraries_to_work_with_graphs/	The title says it all. I am working on a project involving some number theory and graphs. What Python packages do you know that would make working with graphs the easiest? Perhaps SageMath?	30.0	t3_185xexg	reddit		
621	Introducing drf-api-action : Empowering Django REST Framework with Enhanced Functionality	Unknown	2023-11-29 12:30:38	https://www.reddit.com/r/Python/comments/186ob0a/introducing_drfapiaction_empowering_django_rest/	Hi all, **I created a project named drf-api-action** **which Obtains web framework benefits for making API Python packages** **What is** **drf-api-action**? drf-api-action is an ingenious extension to the popular rest\_framework package, introducing a decorator called api\_action. Building upon the foundation of the existing action decorator, this new addition transforms a REST API call in a class view into a straightforward function call. With this decorator, you can effortlessly create an instance of the view and explicitly invoke its functions. **‚ú® Key Benefits:** 1. **Arguments Validation:** Ensure the integrity of your API by effortlessly validating arguments. 2. **Pagination:** Seamlessly implement pagination within your API functions. 3. **Clear Separation of Concerns:** Enjoy a clean separation between function signature and business logic, enhancing code readability. 4. **Database Model Accessibility:** Easily access Django DB models in other libraries or web services. And Many More!: drf-api-action opens the door to many advantages, making DRF even more versatile for your projects. **GitHub Repository:** [Check out the code on GitHub](https://github.com/Ori-Roza/drf-api-action) **Dive Deeper on Medium:** [Read the in-depth article on Medium](https://medium.com/@ori75660/obtaining-web-frameworks-benefits-for-making-api-python-packages-11709a848da5) **If you find drf-api-action beneficial for your projects, show your support by starring the GitHub repository. Your stars help us grow and motivate us to continue enhancing this powerful tool for the DRF community.** # #Django #RESTFramework #APIs #OpenSource #PythonDevelopment #drfapiaction	0.0	t3_186ob0a	reddit		
622	RediPy: A multi-backend Redis library	Unknown	2023-11-28 16:50:51	https://www.reddit.com/r/Python/comments/1860luq/redipy_a_multibackend_redis_library/	https://github.com/JosuaKrause/redipy Hi, I created RediPy, a multi-backend Redis library, to use Redis functionality even in small projects where running a Redis server would be overkill. RediPy can connect to a Redis server but also offers an in-memory Redis implementation that runs in the same process. It's (almost) a drop-in replacement for the official Redis library, however, there are minor tweaks to the API to make it more ergonomic and auto-complete friendly. Lastly, Redis scripts can be entirely defined in Python without the need to use (or know) Lua. Let me know what you think!	2.0	t3_1860luq	reddit		
623	Can't wait for this year's Spotify Wrapped? Learn how to create your own!	Unknown	2023-11-28 17:56:22	https://www.reddit.com/r/Python/comments/18625lu/cant_wait_for_this_years_spotify_wrapped_learn/	"Out of curiosity I recently requested my complete streaming history from Spotify. I ended up creating my own Spotify Wrapped like statistics. It's really easy and everybody can do it. You won't need to use the Spotify API. Instead, start by requesting your complete Spotify streaming history (this includes every song you ever listened to since you have created your account). You can do this here for free: [https://www.spotify.com/account/privacy](https://www.spotify.com/account/privacy/?ref=hackernoon.com) You will get multiple JSON files with a ton of Objects, each representing a song you played. It will look something like this: { ""ts"": ""2023-01-30T16:36:40Z"", ""username"": """", ""platform"": ""linux"", ""ms_played"": 239538, ""conn_country"": ""DE"", ""ip_addr_decrypted"": """", ""user_agent_decrypted"": """", ""master_metadata_track_name"": ""Wonderwall - Remastered"", ""master_metadata_album_artist_name"": ""Oasis"", ""master_metadata_album_album_name"": ""(What's The Story) Morning Glory? (Deluxe Remastered Edition)"", ""spotify_track_uri"": ""spotify:track:7ygpwy2qP3NbrxVkHvUhXY"", ""episode_name"": null, ""episode_show_name"": null, ""spotify_episode_uri"": null, ""reason_start"": ""remote"", ""reason_end"": ""remote"", ""shuffle"": false, ""skipped"": false, ""offline"": false, ""offline_timestamp"": 0, ""incognito_mode"": false } So as you can see, you get a ton of information about when, where, on which device, and for how long you listened to which song or podcast episode. I used Python, Pandas, and Matplotlib to extract stats like the most listened-to songs, artists, and albums. &#x200B; You can read more about it in my latest post on Hackernoon: [https://hackernoon.com/spotify-wrapped-hack-create-your-own-stats-before-the-official-release](https://hackernoon.com/spotify-wrapped-hack-create-your-own-stats-before-the-official-release) Or check out the complete code on GitHub: [https://github.com/lkrimphove/JupyterNotebooks/tree/main/SpotifyAnalysis](https://github.com/lkrimphove/JupyterNotebooks/tree/main/SpotifyAnalysis)"	2.0	t3_18625lu	reddit		
624	Wednesday Daily Thread: Beginner questions	Unknown	2023-11-29 00:00:09	https://www.reddit.com/r/Python/comments/186b1u1/wednesday_daily_thread_beginner_questions/	# Weekly Thread: Beginner Questions üêç Welcome to our Beginner Questions thread! Whether you're new to Python or just looking to clarify some basics, this is the thread for you. ## How it Works: 1. **Ask Anything**: Feel free to ask any Python-related question. There are no bad questions here! 2. **Community Support**: Get answers and advice from the community. 3. **Resource Sharing**: Discover tutorials, articles, and beginner-friendly resources. ## Guidelines: * This thread is specifically for **beginner questions**. For more advanced queries, check out our [Advanced Questions Thread](#advanced-questions-thread-link). ## Recommended Resources: * If you don't receive a response, consider exploring r/LearnPython or join the [Python Discord Server](https://discord.gg/python) for quicker assistance. ## Example Questions: 1. **What is the difference between a list and a tuple?** 2. **How do I read a CSV file in Python?** 3. **What are Python decorators and how do I use them?** 4. **How do I install a Python package using pip?** 5. **What is a virtual environment and why should I use one?** Let's help each other learn Python! üåü	2.0	t3_186b1u1	reddit		
625	Generate realistic dummy data in CSV and JSON format	Unknown	2023-11-28 13:43:52	https://www.reddit.com/r/Python/comments/185wcsn/generate_realistic_dummy_data_in_csv_and_json/	"We often need dummy data for various reasons. I created Ficto, an open-source Python package that generates realistic datasets in CSV and JSON formats. All you need to do is: 1. Install this package using \`pip install ficto\`, 2. Create a data config yaml file where you pass the different column parameters 3. Run \`ficto -d config.yaml -n 10000 -f csv\` (where -d takes data config filepath, -n takes the number of rows you want to generate and -f takes the type of file you want, such as, csv or json.) Ficto will generate a CSV file with 10 thousand rows and the columns you wanted. If one of your columns generates ""First Names"", and another generates ""Genders"", Ficto will assign genders based on the first name. Similarly, if one of the columns generates only ""US"" and ""Australia"" as the country name, the phone number, address, currency, passport number, etc. will be generated as per the country format. The same goes for ""Designation"" and ""Department"". GitHub: [https://github.com/taeefnajib/ficto](https://github.com/taeefnajib/ficto) (Would appreciate it if you \`star\` it) PyPI: [https://pypi.org/project/ficto](https://pypi.org/project/ficto) Docs: [https://ficto.readthedocs.io/en/latest/](https://ficto.readthedocs.io/en/latest/) Hope it can help the community. I'll add more features, file formats, and value types to it. If you want to contribute, you are more than welcome."	5.0	t3_185wcsn	reddit		
626	MegaMock updated, new custom GPT for test generation	Unknown	2023-11-28 23:02:28	https://www.reddit.com/r/Python/comments/1869ope/megamock_updated_new_custom_gpt_for_test/	"Hello everyone! Earlier this year I released a library called ""MegaMock"" which aims to improve the `mock` and `patch` developer experience. You can do things like: ```python mock = MegaMock.it(MySpec) # auto-spec'ed patch = MegaPatch.it(some_func) # actual function, not string, also auto-spec'ed mock.some_func # IDE autocompletes attributes because its a union type ``` The latest changes alter the interface slightly and ironed out some bugs I uncovered dog-fooding the library. In addition to releasing a new version, I've created a ChatGPT+ custom GPT. In addition to having it explain the what, why, and how of the library, it also has been instructed to follow a step-by-step process to creating tests. This is experimental, but I welcome people to try it and let me know if it's helpful. Tests don't need to use mocking. You can find everything, including the GPT prompts and data, in the repo: https://github.com/JamesHutchison/megamock The custom GPT is here: https://chat.openai.com/g/g-DtZiDVQsz-python-megamock-test-generation-assistant Note that a ChatGPT+ subscription is required."	0.0	t3_1869ope	reddit		
627	aidebug now on PyPi	Unknown	2023-11-28 03:42:41	https://www.reddit.com/r/Python/comments/185mpgw/aidebug_now_on_pypi/	Hey **DevCommunity**! üñ•Ô∏è I'm thrilled to share that my AI Coding Assistant AIDebug, is now available for download with pip and can be found on PyPI! üîó Installation Link: \`pip install aidebug\` üåü Key Features: \- Project Management \- Project Configuration \- Code Execution with Automatic Error Detection \- AI Debugging Assistance \- AI Feature Requests \- AI Code Documentation üõ†Ô∏è How to Get Started: 1. Install AIDebug with \`pip install aidebug\` 2. Set your OpenAI API key: \`export OPENAI\_API\_KEY=your\_openai\_api\_key\` 3. Run your project: \`aidebug\` 4. Explore the powerful commands: \`help\` üì¶ PyPI Link: https://pypi.org/project/aidebug/ üåê GitHub Link: https://github.com/00-Python/AI-Debug Boost your coding workflow with intelligent debugging, feature requests, and even generate README files for your GitHub projects‚Äîall powered by OpenAI's GPT models! **#AIDebug** **#OpenAI** **#Python** **#Developers** **#CodingAssistance** **#ProductivityBoost**	1.0	t3_185mpgw	reddit		
628	An Atmospheric Visualization Tool	Unknown	2023-11-28 02:43:18	https://www.reddit.com/r/Python/comments/185li1n/an_atmospheric_visualization_tool/	Repo: [https://github.com/Joshua-Elms/Atmosphere-Visualizer](https://github.com/Joshua-Elms/Atmosphere-Visualizer) I spent Thanksgiving avoiding real work by building this: a tool to download, process, and render videos of real historical atmospheric conditions. The README has a more thorough walkthrough, but in short, the data comes from the Copernicus Climate Data Store and can be accessed via a free API, which means that you should be able to get set up with this tool fairly quickly. It doesn't have any truly breathtaking code, and there are some rough edges around the [subprocess.run](https://subprocess.run)() ffmpeg stuff that should really be replaced with the ffmpeg python bindings, but other than that I think it's fairly clean and extensible. The sample image shown here is just one slice (it outputs timelapse videos), and the displayed value is the total column water vapor (tcwv), measured in kg / m\^2, which is just the total mass of water vapor above any given square meter you might draw on the planet. Let me know what you think! https://preview.redd.it/h0hw0yv4503c1.png?width=2880&format=png&auto=webp&s=a73c6b43aeb85de8bfedfb06d551910bd400f421	1.0	t3_185li1n	reddit		
629	Introducing OpenMic - A Python Karaoke App	Unknown	2023-11-28 00:49:51	https://www.reddit.com/r/Python/comments/185j42b/introducing_openmic_a_python_karaoke_app/	Hello reddit, For the past month or so I've been working on making an open source karaoke app in Python. The problem I ran into while looking for other Karaoke apps was that most require purchasing the music files when all I really wanted was a wrapper for searching karaoke videos on youtube. The closest project I could find was [pikaraoke](https://github.com/vicwomg/pikaraoke), but it felt a little too clunky and I wanted something simple enough that I could run it off my steam deck. So, I made [OpenMic Karaoke.](https://github.com/17hoehbr/OpenMic-Karaoke) This project borrows heavily from the pikaraoke frontend, but all the code has been rewritten so that instead of depending on Pygame and VLC it just uses the web browser for both the TV side and the mobile interface. Mobile clients can add songs to the queue and control media on the TV via websockets handled by Flask-SocketIO. The project is still a very early WIP and contributions are welcome. I've been programming in Python for a few years now but this is my first project of this scale, and I have even less experience with HTML / CSS so expect bugs on the frontend. The biggest hurdle I'm running into right now is packaging the program. I've built an appimage on my local machine which I have uploaded to the releases but I can't seem to get the Github action to work. Ideally I would like to distribute it on Mac, Windows, Linux AppImage and Flatpak but I don't have a Mac or Windows device to test on currently. Constructive feedback is appreciated! Thank you.	0.0	t3_185j42b	reddit		
630	miniloop: a minimal, pedagogical event loop implementation	Unknown	2023-11-27 18:07:35	https://www.reddit.com/r/Python/comments/1859c26/miniloop_a_minimal_pedagogical_event_loop/	[https://github.com/roee30/miniloop](https://github.com/roee30/miniloop) The PEPs introducing the async/await syntax don't specify exactly how event loops work, considering it an implementation detail. If you are curious about how event loops work in Python, you might have noticed that reading through asyncio's code is challenging, to say the least. That's why I have created a minimal event loop implementation with step-by-step explanations. It shouldn't be used for anything other than learning purposes. It is 362 lines in size, but many of them are documentation. Before diving in, make sure you understand generators. You can use the `server.py` script to see the loop in action. The implementation is inspired by asyncio, but I only added what was necessary in order to make a simple server work. In particular, my `Task` and `Future` are roughly analogous to asyncio's, but I didn't find a need to implement `Handle`s (perhaps because I didn't implement the call\_\* methods). I learned a lot writing this and I hope it will be useful to others as well.	4.0	t3_1859c26	reddit		
631	Comprehensive Python Cheatsheet	Unknown	2023-11-27 06:15:02	https://www.reddit.com/r/Python/comments/184w9qu/comprehensive_python_cheatsheet/	Here's a pretty comprehensive python cheatsheet for anyone interested. Its definitely one of the most detailed ones available. [https://github.com/gto76/python-cheatsheet](https://github.com/gto76/python-cheatsheet)	12.0	t3_184w9qu	reddit		
632	Which book to choose for get know better Python?	Unknown	2023-10-25 17:15:04	https://www.reddit.com/r/Python/comments/17g9v63/which_book_to_choose_for_get_know_better_python/	"Hi, I need your advice about Python book. I consider buying: ""Python Tricks: A Buffet of Awesome Python Features"". Any recommendation about this book, it is helpful? And second question, that I should read any other book before that one? Thanks for your help :)"	32.0	t3_17g9v63	reddit		
633	I developed a realtime speech to text library	Unknown	2023-10-05 14:06:53	https://www.reddit.com/r/Python/comments/170iwzc/i_developed_a_realtime_speech_to_text_library/	Hey everyone. I've been working on a library I named *RealtimeSTT*. Its main goal is to transform spoken words into text as they're being said. **What it does**: * voice activity detection: can figure out when you start and stop talking * fast transcription: writes what you say right as you're saying it * wake word support: you're into voice assistants, it can wake up on a specific keyword **Demos**: 1. [Here's a video](https://youtu.be/BKOMqKp436Q?si=MAK5OTKZyV0cZMwa) where it translates different languages in real-time. 2. And [here's another](https://www.youtube.com/watch?v=MpcQOSE3vM4) showing the text appearing as it's spoken. **Code**: If you're curious, want to chip in, or just want to take a look, here's the [link to the Github](https://github.com/KoljaB/RealtimeSTT). Would love to hear your thoughts or get feedback. Thanks for reading!	9.0	t3_170iwzc	reddit		
634	TorchProbe: Fuzzing Dynamic Deep Learning Compilers	Qidong Su	2023-10-30 23:20:47	http://arxiv.org/abs/2310.20078v1	Static and dynamic computational graphs represent two distinct approaches to constructing deep learning frameworks. The former prioritizes compiler-based optimizations, while the latter focuses on programmability and user-friendliness. The recent release of PyTorch 2.0, which supports compiling arbitrary deep learning programs in Python, signifies a new direction in the evolution of deep learning infrastructure to incorporate compiler techniques in a more dynamic manner and support more dynamic language features like dynamic control flows and closures. Given PyTorch's seamless integration with Python, its compiler aims to support arbitrary deep learning code written in Python. However, the inherent dynamism of Python poses challenges to the completeness and robustness of the compiler. While recent research has introduced fuzzing to test deep learning compilers, there is still a lack of comprehensive analysis on how to test dynamic features. To address this issue, we propose several code transformations to generate test cases involving dynamic features. These transformations preserve the program's semantics, ensuring that any discrepancy between the transformed and original programs indicates the presence of a bug. Through our approach, we have successfully identified twenty previously unknown bugs in the PyTorch compiler and its underlying tensor compiler Triton.			arxiv	['Chuqin Geng', 'Gennady Pekhimenko', 'Xujie Si']	250.0
635	Critiquing Computing Artifacts through Programming Satirical Python Scripts	Aadarsh Padiyath	2023-12-05 19:17:25	http://arxiv.org/abs/2312.03090v1	"Computing artifacts tend to exclude marginalized students, so we must create new methods to critique and change them. We studied the potential for ""satirical programming"" to critique artifacts as part of culturally responsive computing (CRC) pedagogy. We conducted a one-hour session for three different BPC programs (N=51). We showed an example of a satirical Python script and taught elements of Python to create a script. Our findings suggest this method is a promising CRC pedagogical approach: 50% of marginalized students worked together to create a satirical script, and 80% enjoyed translating their ""glitches"" into satirical Python scripts."			arxiv	['Tamara Nelson-Fromm', 'Barbara Ericson']	251.0
636	TypeEvalPy: A Micro-benchmarking Framework for Python Type Inference Tools	Ashwin Prasad Shivarpatna Venkatesh	2023-12-28 08:13:27	http://arxiv.org/abs/2312.16882v2	In light of the growing interest in type inference research for Python, both researchers and practitioners require a standardized process to assess the performance of various type inference techniques. This paper introduces TypeEvalPy, a comprehensive micro-benchmarking framework for evaluating type inference tools. TypeEvalPy contains 154 code snippets with 845 type annotations across 18 categories that target various Python features. The framework manages the execution of containerized tools, transforms inferred types into a standardized format, and produces meaningful metrics for assessment. Through our analysis, we compare the performance of six type inference tools, highlighting their strengths and limitations. Our findings provide a foundation for further research and optimization in the domain of Python type inference.			arxiv	['Samkutty Sabu', 'Jiawei Wang', 'Amir M. Mir', 'Li Li', 'Eric Bodden']	252.0
637	HOPE: A Python Just-In-Time compiler for astrophysical computations	Joel Akeret	2014-10-16 09:28:18	http://arxiv.org/abs/1410.4345v2	The Python programming language is becoming increasingly popular for scientific applications due to its simplicity, versatility, and the broad range of its libraries. A drawback of this dynamic language, however, is its low runtime performance which limits its applicability for large simulations and for the analysis of large data sets, as is common in astrophysics and cosmology. While various frameworks have been developed to address this limitation, most focus on covering the complete language set, and either force the user to alter the code or are not able to reach the full speed of an optimised native compiled language. In order to combine the ease of Python and the speed of C++, we developed HOPE, a specialised Python just-in-time (JIT) compiler designed for numerical astrophysical applications. HOPE focuses on a subset of the language and is able to translate Python code into C++ while performing numerical optimisation on mathematical expressions at runtime. To enable the JIT compilation, the user only needs to add a decorator to the function definition. We assess the performance of HOPE by performing a series of benchmarks and compare its execution speed with that of plain Python, C++ and the other existing frameworks. We find that HOPE improves the performance compared to plain Python by a factor of 2 to 120, achieves speeds comparable to that of C++, and often exceeds the speed of the existing solutions. We discuss the differences between HOPE and the other frameworks, as well as future extensions of its capabilities. The fully documented HOPE package is available at http://hope.phys.ethz.ch and is published under the GPLv3 license on PyPI and GitHub.			arxiv	['Lukas Gamper', 'Adam Amara', 'Alexandre Refregier']	253.0
638	Static Analysis for AWS Best Practices in Python Code	Rajdeep Mukherjee	2022-05-09 17:26:10	http://arxiv.org/abs/2205.04432v1	"Amazon Web Services (AWS) is a comprehensive and broadly adopted cloud provider, offering over 200 fully featured services, including compute, database, storage, networking and content delivery, machine learning, Internet of Things and many others. AWS SDKs provide access to AWS services through API endpoints. However, incorrect use of these APIs can lead to code defects, crashes, performance issues, and other problems. This paper presents automated static analysis rules, developed in the context of a commercial service for detection of code defects and security vulnerabilities, to identify deviations from AWS best practices in Python applications that use the AWS SDK. Such applications use the AWS SDK for Python, called ""Boto3"", to access AWS cloud services. However, precise static analysis of Python applications that use cloud SDKs requires robust type inference for inferring the types of cloud service clients. The dynamic style of Boto3 APIs poses unique challenges for type resolution, as does the interprocedural style in which service clients are used in practice. In support of our best-practices goal, we present a layered strategy for type inference that combines multiple type-resolution and tracking strategies in a staged manner. From our experiments across >3,000 popular Python GitHub repos that make use of the AWS SDK, our layered type inference system achieves 85% precision and 100% recall in inferring Boto3 clients in Python client code. Additionally, we present a representative sample of eight AWS best-practice rules that detect a wide range of issues including pagination, polling, and batch operations. We have assessed the efficacy of these rules based on real-world developer feedback. Developers have accepted more than 85% of the recommendations made by five out of eight Python rules, and almost 83% of all recommendations."			arxiv	['Omer Tripp', 'Ben Liblit', 'Michael Wilson']	254.0
639	PyXNAT: XNAT in Python	Yannick Schwartz	2013-01-29 15:42:18	http://arxiv.org/abs/1301.6952v1	As neuroimaging databases grow in size and complexity, the time researchers spend investigating and managing the data increases to the expense of data analysis. As a result, investigators rely more and more heavily on scripting using high-level languages to automate data management and processing tasks. For this, a structured and programmatic access to the data store is necessary. Web services are a first step toward this goal. They however lack in functionality and ease of use because they provide only low level interfaces to databases. We introduce here PyXNAT, a Python module that interacts with The Extensible Neuroimaging Archive Toolkit (XNAT) through native Python calls across multiple operating systems. The choice of Python enables PyXNAT to expose the XNAT Web Services and unify their features with a higher level and more expressive language. PyXNAT provides XNAT users direct access to all the scientific packages in Python. Finally PyXNAT aims to be efficient and easy to use, both as a backend library to build XNAT clients and as an alternative frontend from the command line.			arxiv	['Alexis Barbot', 'Benjamin Thyreau', 'Vincent Frouin', 'Ga√´l Varoquaux', 'Aditya Siram', 'Daniel Marcus', 'Jean-Baptiste Poline']	255.0
640	JyNI - Using native CPython-Extensions in Jython	Stefan Richthofer	2014-04-25 10:56:33	http://arxiv.org/abs/1404.6390v2	Jython is a Java based Python implementation and the most seamless way to integrate Python and Java. However, it does not support native extensions written for CPython like NumPy or SciPy. Since most scientific Python code fundamentally depends on exactly such native extensions directly or indirectly, it usually cannot be run with Jython. JyNI (Jython Native Interface) aims to close this gap. It is a layer that enables Jython users to load native CPython extensions and access them from Jython the same way as they would do in CPython. In order to leverage the JyNI functionality, you just have to put it on the Java classpath when Jython is launched. It neither requires you to recompile the extension code, nor to build a customized Jython fork. That means, it is binary compatible with existing extension builds. At the time of writing, JyNI does not fully implement the Python C-API and it is only capable of loading simple examples that only involve most basic built-in types. The concept is rather complete though and our goal is to provide the C-API needed to load NumPy as soon as possible. After that we will focus on SciPy and others. We expect that our work will also enable Java developers to use CPython extensions like NumPy in their Java code.			arxiv	[]	256.0
641	KMCLib: A general framework for lattice kinetic Monte Carlo (KMC) simulations	Mikael Leetmaa	2014-05-06 10:44:37	http://arxiv.org/abs/1405.1221v1	KMCLib is a general framework for lattice kinetic Monte Carlo (KMC) simulations. The program can handle simulations of the diffusion and reaction of millions of particles in one, two, or three dimensions, and is designed to be easily extended and customized by the user to allow for the development of complex custom KMC models for specific systems without having to modify the core functionality of the program. Analysis modules and on-the-fly elementary step diffusion rate calculations can be implemented as plugins following a well-defined API. The plugin modules are loosely coupled to the core KMCLib program via the Python scripting language. KMCLib is written as a Python module with a backend C++ library. After initial compilation of the backend library KMCLib is used as a Python module; input to the program is given as a Python script executed using a standard Python interpreter. We give a detailed description of the features and implementation of the code and demonstrate its scaling behavior and parallel performance with a simple one-dimensional A-B-C lattice KMC model and a more complex three-dimensional lattice KMC model of oxygen-vacancy diffusion in a fluorite structured metal oxide. KMCLib can keep track of individual particle movements and includes tools for mean square displacement analysis, and is therefore particularly well suited for studying diffusion processes at surfaces and in solids.			arxiv	['Natalia V. Skorodumova']	257.0
642	A simple Python code for computing effective properties of 2D and 3D representative volume element under periodic boundary conditions	Fan Ye	2017-03-11 08:29:45	http://arxiv.org/abs/1703.03930v1	Multiscale optimization is an attractive research field recently. For the most of optimization tools, design parameters should be updated during a close loop. Therefore, a simple Python code is programmed to obtain effective properties of Representative Volume Element (RVE) under Periodic Boundary Conditions (PBCs). It can compute the mechanical properties of a composite with a periodic structure, in two or three dimensions. The computation method is based on the Asymptotic Homogenization Theory (AHT). With simple modifications, the basic Python code may be extended to the computation of the effective properties of more complex microstructure. Moreover, the code provides a convenient platform upon the optimization for the material and geometric composite design. The user may experiment with various algorithms and tackle a wide range of problems. To verify the effectiveness and reliability of the code, a three-dimensional case is employed to illuminate the code. Finally numerical results obtained by the code agree well with the available theoretical and experimental results			arxiv	['Hu Wang']	258.0
643	Evaluating Rapid Application Development with Python for Heterogeneous Processor-based FPGAs	Andrew G. Schmidt	2017-05-11 14:39:38	http://arxiv.org/abs/1705.05209v1	As modern FPGAs evolve to include more het- erogeneous processing elements, such as ARM cores, it makes sense to consider these devices as processors first and FPGA accelerators second. As such, the conventional FPGA develop- ment environment must also adapt to support more software- like programming functionality. While high-level synthesis tools can help reduce FPGA development time, there still remains a large expertise gap in order to realize highly performing implementations. At a system-level the skill set necessary to integrate multiple custom IP hardware cores, interconnects, memory interfaces, and now heterogeneous processing elements is complex. Rather than drive FPGA development from the hardware up, we consider the impact of leveraging Python to ac- celerate application development. Python offers highly optimized libraries from an incredibly large developer community, yet is limited to the performance of the hardware system. In this work we evaluate the impact of using PYNQ, a Python development environment for application development on the Xilinx Zynq devices, the performance implications, and bottlenecks associated with it. We compare our results against existing C-based and hand-coded implementations to better understand if Python can be the glue that binds together software and hardware developers.			arxiv	['Gabriel Weisz', 'Matthew French']	259.0
644	FluidSim: modular, object-oriented Python package for high-performance CFD simulations	Ashwin Vishnu Mohanan	2018-07-03 10:17:29	http://arxiv.org/abs/1807.01769v1	The Python package fluidsim is introduced in this article as an extensible framework for Computational Fluid Mechanics (CFD) solvers. It is developed as a part of FluidDyn project (Augier et al., 2018), an effort to promote open-source and open-science collaboration within fluid mechanics community and intended for both educational as well as research purposes. Solvers in fluidsim are scalable, High-Performance Computing (HPC) codes which are powered under the hood by the rich, scientific Python ecosystem and the Application Programming Interfaces (API) provided by fluiddyn and fluidfft packages (Mohanan et al., 2018). The present article describes the design aspects of fluidsim, viz. use of Python as the main language; focus on the ease of use, reuse and maintenance of the code without compromising performance. The implementation details including optimization methods, modular organization of features and object-oriented approach of using classes to implement solvers are also briefly explained. Currently, fluidsim includes solvers for a variety of physical problems using different numerical methods (including finite-difference methods). However, this metapaper shall dwell only on the implementation and performance of its pseudo-spectral solvers, in particular the two- and three-dimensional Navier-Stokes solvers. We investigate the performance and scalability of fluidsim in a state of the art HPC cluster. Three similar pseudo-spectral CFD codes based on Python (Dedalus, SpectralDNS) and Fortran (NS3D) are presented and qualitatively and quantitatively compared to fluidsim. The source code is hosted at Bitbucket as a Mercurial repository bitbucket.org/fluiddyn/fluidsim and the documentation generated using Sphinx can be read online at fluidsim.readthedocs.io.			arxiv	['Cyrille Bonamy', 'Miguel Calpe Linares', 'Pierre Augier']	260.0
645	AutoParallel: A Python module for automatic parallelization and distributed execution of affine loop nests	Cristian Ramon-Cortes	2018-10-26 11:17:21	http://arxiv.org/abs/1810.11268v1	The last improvements in programming languages, programming models, and frameworks have focused on abstracting the users from many programming issues. Among others, recent programming frameworks include simpler syntax, automatic memory management and garbage collection, which simplifies code re-usage through library packages, and easily configurable tools for deployment. For instance, Python has risen to the top of the list of the programming languages due to the simplicity of its syntax, while still achieving a good performance even being an interpreted language. Moreover, the community has helped to develop a large number of libraries and modules, tuning them to obtain great performance. However, there is still room for improvement when preventing users from dealing directly with distributed and parallel computing issues. This paper proposes and evaluates AutoParallel, a Python module to automatically find an appropriate task-based parallelization of affine loop nests to execute them in parallel in a distributed computing infrastructure. This parallelization can also include the building of data blocks to increase task granularity in order to achieve a good execution performance. Moreover, AutoParallel is based on sequential programming and only contains a small annotation in the form of a Python decorator so that anyone with little programming skills can scale up an application to hundreds of cores.			arxiv	['Ramon Amela', 'Jorge Ejarque', 'Philippe Clauss', 'Rosa M. Badia']	261.0
646	An Empirical Study of Flaky Tests in Python	Martin Gruber	2021-01-22 12:23:19	http://arxiv.org/abs/2101.09077v1	Tests that cause spurious failures without any code changes, i.e., flaky tests, hamper regression testing, increase maintenance costs, may shadow real bugs, and decrease trust in tests. While the prevalence and importance of flakiness is well established, prior research focused on Java projects, thus raising the question of how the findings generalize. In order to provide a better understanding of the role of flakiness in software development beyond Java, we empirically study the prevalence, causes, and degree of flakiness within software written in Python, one of the currently most popular programming languages. For this, we sampled 22352 open source projects from the popular PyPI package index, and analyzed their 876186 test cases for flakiness. Our investigation suggests that flakiness is equally prevalent in Python as it is in Java. The reasons, however, are different: Order dependency is a much more dominant problem in Python, causing 59% of the 7571 flaky tests in our dataset. Another 28% were caused by test infrastructure problems, which represent a previously undocumented cause of flakiness. The remaining 13% can mostly be attributed to the use of network and randomness APIs by the projects, which is indicative of the type of software commonly written in Python. Our data also suggests that finding flaky tests requires more runs than are often done in the literature: A 95% confidence that a passing test case is not flaky on average would require 170 reruns.			arxiv	['Stephan Lukasczyk', 'Florian Kroi√ü', 'Gordon Fraser']	262.0
647	Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence	Sebastian Raschka	2020-02-12 05:20:59	http://arxiv.org/abs/2002.04803v2	Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical ML and scalable general-purpose GPU computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.			arxiv	['Joshua Patterson', 'Corey Nolet']	263.0
648	Data Engineering for HPC with Python	Vibhatha Abeykoon	2020-10-13 11:53:11	http://arxiv.org/abs/2010.06312v1	Data engineering is becoming an increasingly important part of scientific discoveries with the adoption of deep learning and machine learning. Data engineering deals with a variety of data formats, storage, data extraction, transformation, and data movements. One goal of data engineering is to transform data from original data to vector/matrix/tensor formats accepted by deep learning and machine learning applications. There are many structures such as tables, graphs, and trees to represent data in these data engineering phases. Among them, tables are a versatile and commonly used format to load and process data. In this paper, we present a distributed Python API based on table abstraction for representing and processing data. Unlike existing state-of-the-art data engineering tools written purely in Python, our solution adopts high performance compute kernels in C++, with an in-memory table representation with Cython-based Python bindings. In the core system, we use MPI for distributed memory computations with a data-parallel approach for processing large datasets in HPC clusters.			arxiv	['Niranda Perera', 'Chathura Widanage', 'Supun Kamburugamuve', 'Thejaka Amila Kanewala', 'Hasara Maithree', 'Pulasthi Wickramasinghe', 'Ahmet Uyar', 'Geoffrey Fox']	264.0
649	APIScanner -- Towards Automated Detection of Deprecated APIs in Python Libraries	Aparna Vadlamani	2021-02-18 10:13:12	http://arxiv.org/abs/2102.09251v4	Python libraries are widely used for machine learning and scientific computing tasks today. APIs in Python libraries are deprecated due to feature enhancements and bug fixes in the same way as in other languages. These deprecated APIs are discouraged from being used in further software development. Manually detecting and replacing deprecated APIs is a tedious and time-consuming task due to the large number of API calls used in the projects. Moreover, the lack of proper documentation for these deprecated APIs makes the task challenging. To address this challenge, we propose an algorithm and a tool APIScanner that automatically detects deprecated APIs in Python libraries. This algorithm parses the source code of the libraries using abstract syntax tree (ASTs) and identifies the deprecated APIs via decorator, hard-coded warning or comments. APIScanner is a Visual Studio Code Extension that highlights and warns the developer on the use of deprecated API elements while writing the source code. The tool can help developers to avoid using deprecated API elements without the execution of code. We tested our algorithm and tool on six popular Python libraries, which detected 838 of 871 deprecated API elements. Demo of APIScanner: https://youtu.be/1hy_ugf-iek. Documentation, tool, and source code can be found here: https://rishitha957.github.io/APIScanner.			arxiv	['Rishitha Kalicheti', 'Sridhar Chimalakonda']	265.0
650	sympy2c: from symbolic expressions to fast C/C++ functions and ODE solvers in Python	Uwe Schmitt	2022-03-22 15:51:55	http://arxiv.org/abs/2203.11945v1	Computer algebra systems play an important role in science as they facilitate the development of new theoretical models. The resulting symbolic equations are often implemented in a compiled programming language in order to provide fast and portable codes for practical applications. We describe sympy2c, a new Python package designed to bridge the gap between the symbolic development and the numerical implementation of a theoretical model. sympy2c translates symbolic equations implemented in the SymPy Python package to C/C++ code that is optimized using symbolic transformations. The resulting functions can be conveniently used as an extension module in Python. sympy2c is used within the PyCosmo Python package to solve the Einstein-Boltzmann equations, a large system of ODEs describing the evolution of linear perturbations in the Universe. After reviewing the functionalities and usage of sympy2c, we describe its implementation and optimization strategies. This includes, in particular, a novel approach to generate optimized ODE solvers making use of the sparsity of the symbolic Jacobian matrix. We demonstrate its performance using the Einstein-Boltzmann equations as a test case. sympy2c is widely applicable and may prove useful for various areas of computational physics. sympy2c is publicly available at https://cosmology.ethz.ch/research/software-lab/sympy2c.html			arxiv	['Beatrice Moser', 'Christiane S. Lorenz', 'Alexandre Refregier']	266.0
651	Solar Energetic Particle Time Series Analysis with Python	Christian Palmroos	2022-10-19 10:01:28	http://arxiv.org/abs/2210.10432v2	Solar Energetic Particles (SEPs) are charged particles accelerated within the solar atmosphere or the interplanetary space by explosive phenomena such as solar flares or Coronal Mass Ejections (CMEs). Once injected into the interplanetary space, they can propagate towards Earth, causing space weather related phenomena. For their analysis, interplanetary in-situ measurements of charged particles are key. The recently expanded spacecraft fleet in the heliosphere not only provides much-needed additional vantage points, but also increases the variety of missions and instruments for which data loading and processing tools are needed. This manuscript introduces a series of Python functions that will enable the scientific community to download, load, and visualize charged particle measurements of the current space missions that are especially relevant to particle research as time series or dynamic spectra. In addition, further analytical functionality is provided that allows the determination of SEP onset times as well as their inferred injection times. The full workflow, which is intended to be run within Jupyter Notebooks and can also be approachable for Python laymen, will be presented with scientific examples. All functions are written in Python, with the source code publicly available at GitHub under a permissive license. Where appropriate, available Python libraries are used, and their application is described.			arxiv	['Jan Gieseler', 'Nina Dresing', 'Diana E. Morosan', 'Eleanna Asvestari', 'Aleksi Yli-Laurila', 'Daniel J. Price', 'Saku Valkila', 'Rami Vainio']	267.0
652	Using a DSL to read ROOT TTrees faster in Uproot	Aryan Roy	2023-03-03 20:24:34	http://arxiv.org/abs/2303.02202v1	Uproot reads ROOT TTrees using pure Python. For numerical and (singly) jagged arrays, this is fast because a whole block of data can be interpreted as an array without modifying the data. For other cases, such as arrays of std::vector<std::vector<float>>, numerical data are interleaved with structure, and the only way to deserialize them is with a sequential algorithm. When written in Python, such algorithms are very slow. We solve this problem by writing the same logic in a language that can be executed quickly. AwkwardForth is a Domain Specific Language (DSL), based on Standard Forth with I/O extensions for making Awkward Arrays, and it can be interpreted as a fast virtual machine without requiring LLVM as a dependency. We generate code as late as possible to take advantage of optimization opportunities. All ROOT types previously implemented with Python have been converted to AwkwardForth. Double and triple-jagged arrays, for example, are 400x faster in AwkwardForth than in Python, with multithreaded scaling up to 1 second/GB because AwkwardForth releases the Python GIL. We also investigate the possibility of JIT-compiling the generated AwkwardForth code using LLVM to increase the performance gains. In this paper, we describe design aspects, performance studies, and future directions in accelerating Uproot with AwkwardForth.			arxiv	['Jim Pivarski']	268.0
653	SunPy - Python for Solar Physics	The SunPy Community	2015-05-11 10:55:31	http://arxiv.org/abs/1505.02563v1	This paper presents SunPy (version 0.5), a community-developed Python package for solar physics. Python, a free, cross-platform, general-purpose, high-level programming language, has seen widespread adoption among the scientific community, resulting in the availability of a large number of software packages, from numerical computation (NumPy, SciPy) and machine learning (scikit-learn) to visualisation and plotting (matplotlib). SunPy is a data-analysis environment specialising in providing the software necessary to analyse solar and heliospheric data in Python. SunPy is open-source software (BSD licence) and has an open and transparent development workflow that anyone can contribute to. SunPy provides access to solar data through integration with the Virtual Solar Observatory (VSO), the Heliophysics Event Knowledgebase (HEK), and the HELiophysics Integrated Observatory (HELIO) webservices. It currently supports image data from major solar missions (e.g., SDO, SOHO, STEREO, and IRIS), time-series data from missions such as GOES, SDO/EVE, and PROBA2/LYRA, and radio spectra from e-Callisto and STEREO/SWAVES. We describe SunPy's functionality, provide examples of solar data analysis in SunPy, and show how Python-based solar data-analysis can leverage the many existing tools already available in Python. We discuss the future goals of the project and encourage interested users to become involved in the planning and development of SunPy.			arxiv	['Stuart J Mumford', 'Steven Christe', 'David P√©rez-Su√°rez', 'Jack Ireland', 'Albert Y Shih', 'Andrew R Inglis', 'Simon Liedtke', 'Russell J Hewett', 'Florian Mayer', 'Keith Hughitt', 'Nabil Freij', 'Tomas Meszaros', 'Samuel M Bennett', 'Michael Malocha', 'John Evans', 'Ankit Agrawal', 'Andrew J Leonard', 'Thomas P Robitaille', 'Benjamin Mampaey', 'Jose Iv√°n Campos-Rozo', 'Michael S Kirk']	269.0
654	Domain Knowledge Matters: Improving Prompts with Fix Templates for Repairing Python Type Errors	Yun Peng	2023-06-02 09:42:16	http://arxiv.org/abs/2306.01394v1	Although the dynamic type system of Python facilitates the developers in writing Python programs, it also brings type errors at run-time. There exist rule-based approaches for automatically repairing Python type errors. The approaches can generate accurate patches but they require domain experts to design patch synthesis rules and suffer from low template coverage of real-world type errors. Learning-based approaches alleviate the manual efforts in designing patch synthesis rules. Among the learning-based approaches, the prompt-based approach which leverages the knowledge base of code pre-trained models via pre-defined prompts, obtains state-of-the-art performance in general program repair tasks. However, such prompts are manually defined and do not involve any specific clues for repairing Python type errors, resulting in limited effectiveness. How to automatically improve prompts with the domain knowledge for type error repair is challenging yet under-explored. In this paper, we present TypeFix, a novel prompt-based approach with fix templates incorporated for repairing Python type errors. TypeFix first mines generalized fix templates via a novel hierarchical clustering algorithm. The identified fix templates indicate the common edit patterns and contexts of existing type error fixes. TypeFix then generates code prompts for code pre-trained models by employing the generalized fix templates as domain knowledge, in which the masks are adaptively located for each type error instead of being pre-determined. Experiments on two benchmarks, including BugsInPy and TypeBugs, show that TypeFix successfully repairs 26 and 55 type errors, outperforming the best baseline approach by 9 and 14, respectively. Besides, the proposed fix template mining approach can cover 75% of developers' patches in both benchmarks, increasing the best rule-based approach PyTER by more than 30%.			arxiv	['Shuzheng Gao', 'Cuiyun Gao', 'Yintong Huo', 'Michael R. Lyu']	270.0
655	Exploring Security Commits in Python	Shiyu Sun	2023-07-21 18:46:45	http://arxiv.org/abs/2307.11853v1	Python has become the most popular programming language as it is friendly to work with for beginners. However, a recent study has found that most security issues in Python have not been indexed by CVE and may only be fixed by 'silent' security commits, which pose a threat to software security and hinder the security fixes to downstream software. It is critical to identify the hidden security commits; however, the existing datasets and methods are insufficient for security commit detection in Python, due to the limited data variety, non-comprehensive code semantics, and uninterpretable learned features. In this paper, we construct the first security commit dataset in Python, namely PySecDB, which consists of three subsets including a base dataset, a pilot dataset, and an augmented dataset. The base dataset contains the security commits associated with CVE records provided by MITRE. To increase the variety of security commits, we build the pilot dataset from GitHub by filtering keywords within the commit messages. Since not all commits provide commit messages, we further construct the augmented dataset by understanding the semantics of code changes. To build the augmented dataset, we propose a new graph representation named CommitCPG and a multi-attributed graph learning model named SCOPY to identify the security commit candidates through both sequential and structural code semantics. The evaluation shows our proposed algorithms can improve the data collection efficiency by up to 40 percentage points. After manual verification by three security experts, PySecDB consists of 1,258 security commits and 2,791 non-security commits. Furthermore, we conduct an extensive case study on PySecDB and discover four common security fix patterns that cover over 85% of security commits in Python, providing insight into secure software maintenance, vulnerability detection, and automated program repair.			arxiv	['Shu Wang', 'Xinda Wang', 'Yunlong Xing', 'Elisa Zhang', 'Kun Sun']	271.0
656	Anisotropy of the Microwave Sky at 90 GHz: Results from Python II	J. E. Ruhl	1995-08-16 18:11:20	http://arxiv.org/abs/astro-ph/9508065v1	We report on additional observations of degree scale anisotropy at 90~GHz from the Amundsen-Scott South Pole Station in Antarctica. Observations during the first season with the Python instrument yielded a statistically significant sky signal; in this paper we report the confirmation of that signal with data taken in the second year, and on results from an interleaving set of fields.			arxiv	['M. Dragovan', 'S. R. Platt', 'J. Kovac', 'G. Novak']	272.0
657	A Domain-Specific Language for Programming in the Tile Assembly Model	David Doty	2009-03-05 01:19:35	http://arxiv.org/abs/0903.0889v1	We introduce a domain-specific language (DSL) for creating sets of tile types for simulations of the abstract Tile Assembly Model. The language defines objects known as tile templates, which represent related groups of tiles, and a small number of basic operations on tile templates that help to eliminate the error-prone drudgery of enumerating such tile types manually or with low-level constructs of general-purpose programming languages. The language is implemented as a class library in Python (a so-called internal DSL), but is presented independently of Python or object-oriented programming, with emphasis on supporting the creation of visual editing tools for programmatically creating large sets of complex tile types without needing to write a program.			arxiv	['Matthew J. Patitz']	273.0
658	Continuum multi-physics modeling with scripting languages: the Nsim simulation compiler prototype for classical field theory	Thomas Fischbacher	2009-07-09 15:32:47	http://arxiv.org/abs/0907.1587v1	"We demonstrate that for a broad class of physical systems that can be described using classical field theory, automated runtime translation of the physical equations to parallelized finite-element numerical simulation code is feasible. This allows the implementation of multiphysics extension modules to popular scripting languages (such as Python) that handle the complete specification of the physical system at script level. We discuss two example applications that utilize this framework: the micromagnetic simulation package ""Nmag"" as well as a short Python script to study morphogenesis in a reaction-diffusion model."			arxiv	['Hans Fangohr']	274.0
659	A tool stack for implementing Behaviour-Driven Development in Python Language	Hugo Lopes Tavares	2010-07-10 14:14:30	http://arxiv.org/abs/1007.1722v1	This paper presents a tool stack for the implementation, specification and test of software following the practices of Behavior Driven Development (BDD) in Python language. The usage of this stack highlights the specification and validation of the software's expected behavior, reducing the error rate and improving documentation. Therefore, it is possible to produce code with much less defects at both functional and unit levels, in addition to better serving to stakeholders' expectations.			arxiv	['Gustavo Guimaraes Rezende', 'Vanderson Mota dos Santos', 'Rodrigo Soares Manhaes', 'Rogerio Atem de Carvalho']	275.0
660	Use of Python and Phoenix-M Interface in Robotics	Shubham Chakraborty	2010-12-31 12:36:54	http://arxiv.org/abs/1101.0245v1	In this paper I will show how to use Python programming with a computer interface such as Phoenix-M 1 to drive simple robots. In my quest towards Artificial Intelligence(AI) I am experimenting with a lot of different possibilities in Robotics. This one will try to mimic the working of a simple insect's nervous system using hard wiring and some minimal software usage. This is the precursor to my advanced robotics and AI integration where I plan to use a new paradigm of AI based on Machine Learning and Self Consciousness via Knowledge Feedback and Update Process.			arxiv	[]	276.0
661	"Corrigendum to ""Universal factorization of 3n-j (j>2) symbols..."" [J. Phys. A: Math. Gen. 37 (2004) 3259]"	Richard J. Mathar	2011-02-24 23:14:27	http://arxiv.org/abs/1102.5125v3	Ten values of 12-j symbols of the first kind published earlier are challenged by values calculated with an independent Python program. The program first implements a narrow class of square roots of rational numbers, utilizing Python's unlimited representation of big integers. Wigner's 3jm symbols, 6-j, 9-j, 12-j and 15-j symbols are then calculated by their familiar representations as sums over products of these.			arxiv	[]	277.0
662	Design and Implementation of a Simple Web Search Engine	Andri Mirzal	2011-12-13 06:46:26	http://arxiv.org/abs/1112.2807v2	We present a simple web search engine for indexing and searching html documents using python programming language. Because python is well known for its simple syntax and strong support for main operating systems, we hope it will be beneficial for learning information retrieval techniques, especially web search engine technology.			arxiv	[]	278.0
663	Function call overhead benchmarks with MATLAB, Octave, Python, Cython and C	Andr√© Gaul	2012-02-13 14:14:00	http://arxiv.org/abs/1202.2736v1	We consider the overhead of function calls in the programming languages MATLAB/Octave, Python, Cython and C. In many applications a function has to be called very often inside a loop. One such application in numerical analysis is the finite element method where integrals have to be computed on each element in a loop. The called functions can often be evaluated efficiently but the function call itself may be time-consuming. We present a benchmark whose goal is to identify and quantify optimization potentials with respect to time consumption caused by function calls in the mentioned programming languages.			arxiv	[]	279.0
664	Numerical methods with Sage	Lauri Ruotsalainen	2012-08-20 07:35:38	http://arxiv.org/abs/1208.3929v1	Numpy and SciPy are program libraries for the Python scripting language, which apply to a large spectrum of numerical and scientific computing tasks. The Sage project provides a multiplatform software environment which enables one to use, in a unified way, a large number of software components, including Numpy and Scipy, and which has Python as its command language. We review several examples, typical for scientific computation courses, and their solution using these tools in the Sage environment.			arxiv	['Matti Vuorinen']	280.0
665	CS Circles: An In-Browser Python Course for Beginners	David Pritchard	2012-09-10 22:12:53	http://arxiv.org/abs/1209.2166v2	Computer Science Circles is a free programming website for beginners that is designed to be fun, easy to use, and accessible to the broadest possible audience. We teach Python since it is simple yet powerful, and the course content is well-structured but written in plain language. The website has over one hundred exercises in thirty lesson pages, plus special features to help teachers support their students. It is available in both English and French. We discuss the philosophy behind the course and its design, we describe how it was implemented, and we give statistics on its use.			arxiv	['Troy Vasiga']	281.0
666	Session Types Go Dynamic or How to Verify Your Python Conversations	Rumyana Neykova	2013-12-10 08:04:18	http://arxiv.org/abs/1312.2704v1	This paper presents the first implementation of session types in a dynamically-typed language - Python. Communication safety of the whole system is guaranteed at runtime by monitors that check the execution traces comply with an associated protocol. Protocols are written in Scribble, a choreography description language based on multiparty session types, with addition of logic formulas for more precise behaviour properties. The presented framework overcomes the limitations of previous works on the session types where all endpoints should be statically typed so that they do not permit interoperability with untyped participants. The advantages, expressiveness and performance of dynamic protocol checking are demonstrated through use case and benchmarks.			arxiv	[]	282.0
667	SfePy - Write Your Own FE Application	Robert Cimrman	2014-04-25 10:56:35	http://arxiv.org/abs/1404.6391v2	SfePy (Simple Finite Elements in Python) is a framework for solving various kinds of problems (mechanics, physics, biology, ...) described by partial differential equations in two or three space dimensions by the finite element method. The paper illustrates its use in an interactive environment or as a framework for building custom finite-element based solvers.			arxiv	[]	283.0
668	BayesPy: Variational Bayesian Inference in Python	Jaakko Luttinen	2014-10-03 14:51:09	http://arxiv.org/abs/1410.0870v3	BayesPy is an open-source Python software package for performing variational Bayesian inference. It is based on the variational message passing framework and supports conjugate exponential family models. By removing the tedious task of implementing the variational Bayesian update equations, the user can construct models faster and in a less error-prone way. Simple syntax, flexible model construction and efficient inference make BayesPy suitable for both average and expert Bayesian users. It also supports some advanced methods such as stochastic and collapsed variational inference.			arxiv	[]	284.0
669	PyFAI: a Python library for high performance azimuthal integration on GPU	J√©r√¥me Kieffer	2014-12-19 15:06:50	http://arxiv.org/abs/1412.6367v1	The pyFAI package has been designed to reduce X-ray diffraction images into powder diffraction curves to be further processed by scientists. This contribution describes how to convert an image into a radial profile using the Numpy package, how the process was accelerated using Cython. The algorithm was parallelised, needing a complete re-design to benefit from massively parallel devices like graphical processing units or accelerators like the Intel Xeon Phi using the PyOpenCL library.			arxiv	['Giannis Ashiotis']	285.0
670	apsis - Framework for Automated Optimization of Machine Learning Hyper Parameters	Frederik Diehl	2015-03-10 15:09:25	http://arxiv.org/abs/1503.02946v2	The apsis toolkit presented in this paper provides a flexible framework for hyperparameter optimization and includes both random search and a bayesian optimizer. It is implemented in Python and its architecture features adaptability to any desired machine learning code. It can easily be used with common Python ML frameworks such as scikit-learn. Published under the MIT License other researchers are heavily encouraged to check out the code, contribute or raise any suggestions. The code can be found at github.com/FrederikDiehl/apsis.			arxiv	['Andreas Jauch']	286.0
671	A Python Code for the Emmanoulopoulos et al. [arXiv:1305.0304] Light Curve Simulation Algorithm	S D Connolly	2015-03-23 15:11:11	http://arxiv.org/abs/1503.06676v1	I have created, for public use, a Python code allowing the simulation of light curves with any given power spectral density and any probability density function, following the algorithm described in Emmanoulopoulos et al. 2013. The simulated products have exactly the same variability and statistical properties as the observed light curves. The code and its documentation are available at: https://github.com/samconnolly/DELightcurveSimulation Note that a Mathematica code of the algorithm is given in Emmanoulopoulos et al. [arXiv:1305.0304]			arxiv	[]	287.0
672	PyTransit: Fast and Easy Exoplanet Transit Modelling in Python	Hannu Parviainen	2015-04-28 11:40:53	http://arxiv.org/abs/1504.07433v1	We present a fast and user friendly exoplanet transit light curve modelling package PyTransit, implementing optimised versions of the Gimen\'ez and the Mandel & Agol transit models. The package offers an object-oriented Python interface to access the two models implemented natively in Fortran with OpenMP parallelisation. A partial OpenCL version of the quadratic Mandel-Agol model is also included for GPU-accelerated computations. The aim of PyTransit is to facilitate the analysis of photometric time series of exoplanet transits consisting of hundreds of thousands of datapoints, and of multi-passband transit light curves from spectrophotometric observations, as a part of a researcher's programming toolkit for building complex, problem-specific, analyses.			arxiv	[]	288.0
673	Probabilistic Programming in Python using PyMC	John Salvatier	2015-07-29 08:20:22	http://arxiv.org/abs/1507.08050v1	Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.			arxiv	['Thomas Wiecki', 'Christopher Fonnesbeck']	289.0
674	CONCEPT - The COsmological $N$-body CodE in PyThon	Jeppe Dakin	2015-10-26 13:47:08	http://arxiv.org/abs/1510.07621v1	CO$N$CEPT (COsmological $N$-body CodE in PyThon) is a free and open-source code for cosmological $N$-body simulations on massively parallel computers with distributed memory. Collisionless dark matter is the only implemented particle species. Gravity can be computed using the PP, PM or the P$^{3}$M algorithm. The goal of CO$N$CEPT is to make it pleasant to work with cosmological $N$-body simulations - for the cosmologist as well as for the source code developer. This is the user guide. The source code and additional documentation can be found at https://github.com/jmd-dk/concept/			arxiv	[]	290.0
675	A Python Engine for Teaching Artificial Intelligence in Games	Mark O. Riedl	2015-11-24 14:13:33	http://arxiv.org/abs/1511.07714v1	Computer games play an important role in our society and motivate people to learn computer science. Since artificial intelligence is integral to most games, they can also be used to teach artificial intelligence. We introduce the Game AI Game Engine (GAIGE), a Python game engine specifically designed to teach about how AI is used in computer games. A progression of seven assignments builds toward a complete, working Multi-User Battle Arena (MOBA) game. We describe the engine, the assignments, and our experiences using it in a class on Game Artificial Intelligence.			arxiv	[]	291.0
676	ELATE: An open-source online application for analysis and visualization of elastic tensors	Romain Gaillac	2016-02-19 15:09:29	http://arxiv.org/abs/1602.06175v2	We report on the implementation of a tool for the analysis of second-order elastic stiffness tensors, provided with both an open-source Python module and a standalone online application providing visualization tools of anisotropic mechanical properties. After describing the software features, how we compute the conventional elastic constants and how we represent them graphically, we explain our technical choices for the implementation. In particular, we focus on why a Python module is used to generate the HTML web page with embedded Javascript for dynamical plots.			arxiv	['Pluton Pullumbi', 'Fran√ßois-Xavier Coudert']	292.0
677	Benchmarking Python Tools for Automatic Differentiation	Andrei Turkin	2016-06-20 20:14:12	http://arxiv.org/abs/1606.06311v1	In this paper we compare several Python tools for automatic differentiation. In order to assess the difference in performance and precision, the problem of finding the optimal geometrical structure of the cluster with identical atoms is used as follows. First, we compare performance of calculating gradients for the objective function. We showed that the PyADOL-C and PyCppAD tools have much better performance for big clusters than the other ones. Second, we assess precision of these two tools by calculating the difference between the obtained at the optimal configuration gradient norms. We conclude that PyCppAD has the best performance among others, while having almost the same precision as the second- best performing tool - PyADOL-C.			arxiv	['Aung Thu']	293.0
678	Text comparison using word vector representations and dimensionality reduction	Hendrik Heuer	2016-07-02 17:17:22	http://arxiv.org/abs/1607.00534v1	"This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like ""capital city of"". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn."			arxiv	[]	294.0
679	A conformal bootstrap approach to critical percolation in two dimensions	Marco Picco	2016-07-25 12:00:02	http://arxiv.org/abs/1607.07224v2	We study four-point functions of critical percolation in two dimensions, and more generally of the Potts model. We propose an exact ansatz for the spectrum: an infinite, discrete and non-diagonal combination of representations of the Virasoro algebra. Based on this ansatz, we compute four-point functions using a numerical conformal bootstrap approach. The results agree with Monte-Carlo computations of connectivities of random clusters.			arxiv	['Sylvain Ribault', 'Raoul Santachiara']	295.0
680	PyCaMa: Python for cash management	Francisco Salas-Molina	2017-02-16 15:08:20	http://arxiv.org/abs/1702.05005v2	Selecting the best policy to keep the balance between what a company holds in cash and what is placed in alternative investments is by no means straightforward. We here introduce PyCaMa, a Python module for multiobjective cash management based on linear programming that allows to derive optimal policies for cash management with multiple bank accounts in terms of both cost and risk of policies.			arxiv	['Juan A. Rodr√≠guez-Aguilar', 'Pablo D√≠az-Garc√≠a']	296.0
681	Interfacing of High Temperature Z-meter Setup Using Python	Ashutosh Patel	2017-02-22 07:24:42	http://arxiv.org/abs/1702.06691v1	In this work, we interface high temperature Z-meter setup to automize the whole measurement process. A program is built on open source programming language Python which convert the manual measurement process into fully automated process without any cost addition. Using this program, simultaneous measurement of Seebeck coefficient, thermal conductivity and electrical resistivity are performed and using all three, figure-of-merit (ZT) is calculated. Developed program is verified by performing measurement over p-type Bi0.36Sb1.45Te3 sample and the data obtained are found to be in good agreement with the reported data.			arxiv	['Shashank Sisodia', 'Sudhir K. Pandey']	297.0
682	SpectRes: A Fast Spectral Resampling Tool in Python	A. C. Carnall	2017-05-15 11:36:37	http://arxiv.org/abs/1705.05165v1	I present a fast Python tool, SpectRes, for carrying out the resampling of spectral flux densities and their associated uncertainties onto different wavelength grids. The function works with any grid of wavelength values, including non-uniform sampling, and preserves the integrated flux. This may be of use for binning data to increase the signal to noise ratio, obtaining synthetic photometry, or resampling model spectra to match the sampling of observed data for spectral energy distribution fitting. The function can be downloaded from https://www.github.com/ACCarnall/SpectRes.			arxiv	[]	298.0
683	DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models	Anush Sankaran	2017-08-16 14:46:27	http://arxiv.org/abs/1708.04915v1	Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries.			arxiv	['Rahul Aralikatte', 'Senthil Mani', 'Shreya Khare', 'Naveen Panwar', 'Neelamadhav Gantayat']	299.0
684	DECal, a Python tool for the efficiency calculation of thermal neutron detectors based on thin-film converters	√Ålvaro Carmona Basa√±ez	2018-01-22 14:37:37	http://arxiv.org/abs/1801.07124v1	The Detector Efficiency Calculator (DECal) is a series of Python functions and tools designed to analytically calculate, visualise and optimise the detection efficiency of thermal neutron detectors, which are based on thin-film converters. The implementation presented in this article concerns 10B-based detectors in particular. The code can be run via a graphical user interface, as well as via the command line. The source code is openly available to interested users via a GitHub repository.			arxiv	['Kalliopi Kanaki', 'Francesco Piscitelli']	300.0
685	Mitigating Spreadsheet Model Risk with Python Open Source Infrastructure	Oliver Beavers	2018-01-29 21:32:44	http://arxiv.org/abs/1801.09771v1	"Across an aggregation of EuSpRIG presentation papers, two maxims hold true: spreadsheets models are akin to software, yet spreadsheet developers are not software engineers. As such, the lack of traditional software engineering tools and protocols invites a higher rate of error in the end result. This paper lays ground work for spreadsheet modelling professionals to develop reproducible audit tools using freely available, open source packages built with the Python programming language, enabling stakeholders to develop clearly defined model ""oracles"" with which to test and audit spreadsheet calculations against."			arxiv	[]	301.0
686	PyMUSE: a Python package for VLT/MUSE data	Ismael Pessa	2018-03-13 18:58:29	http://arxiv.org/abs/1803.05005v2	This is a companion Focus Demonstration article to the PyMUSE python package, demonstrating its usage and utilities for VLT/MUSE data analysis, that include a wide range of options for spectra extractions, the creation of different types of images, compatibilities with some commonly used software for astronomical data analysis, among others. PyMUSE is an open-source software and can be found on Github for free use and distribution.			arxiv	['Nicolas Tejos', 'Cristobal Moya']	302.0
687	Seglearn: A Python Package for Learning Sequences and Time Series	David M. Burns	2018-03-21 20:30:34	http://arxiv.org/abs/1803.08118v3	Seglearn is an open-source python package for machine learning time series or sequences using a sliding window segmentation approach. The implementation provides a flexible pipeline for tackling classification, regression, and forecasting problems with multivariate sequence and contextual data. This package is compatible with scikit-learn and is listed under scikit-learn Related Projects. The package depends on numpy, scipy, and scikit-learn. Seglearn is distributed under the BSD 3-Clause License. Documentation includes a detailed API description, user guide, and examples. Unit tests provide a high degree of code coverage.			arxiv	['Cari M. Whyne']	303.0
688	Transparent, Efficient, and Robust Word Embedding Access with WOMBAT	Mark-Christoph M√ºller	2018-07-02 14:47:30	http://arxiv.org/abs/1807.00717v1	We present WOMBAT, a Python tool which supports NLP practitioners in accessing word embeddings from code. WOMBAT addresses common research problems, including unified access, scaling, and robust and reproducible preprocessing. Code that uses WOMBAT for accessing word embeddings is not only cleaner, more readable, and easier to reuse, but also much more efficient than code using standard in-memory methods: a Python script using WOMBAT for evaluating seven large word embedding collections (8.7M embedding vectors in total) on a simple SemEval sentence similarity task involving 250 raw sentence pairs completes in under ten seconds end-to-end on a standard notebook computer.			arxiv	['Michael Strube']	304.0
689	Turbulucid: A Python Package for Post-Processing of Fluid Flow Simulations	Timofey Mukha	2018-07-25 16:05:52	http://arxiv.org/abs/1807.09688v1	A Python package for post-processing of plane two-dimensional data from computational fluid dynamics simulations is presented. The package, called turbulucid, provides means for scripted, reproducible analysis of large simulation campaigns and includes routines for both data extraction and visualization. For the former, the Visualization Toolkit (VTK) is used, allowing for post-processing of simulations performed on unstructured meshes. For visualization, several matplotlib-based functions for creating highly customizable, publication-quality plots are provided. To demonstrate turbulucid's functionality it is here applied to post-processing a simulation of a flow over a backward-facing step. The implementation and architecture of the package are also discussed, as well as its reuse potential.			arxiv	[]	305.0
690	NIMFA: A Python Library for Nonnegative Matrix Factorization	Marinka Zitnik	2018-08-06 06:28:35	http://arxiv.org/abs/1808.01743v1	NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks.			arxiv	['Blaz Zupan']	306.0
691	DPPy: Sampling DPPs with Python	Guillaume Gautier	2018-09-19 15:53:00	http://arxiv.org/abs/1809.07258v2	Determinantal point processes (DPPs) are specific probability distributions over clouds of points that are used as models and computational tools across physics, probability, statistics, and more recently machine learning. Sampling from DPPs is a challenge and therefore we present DPPy, a Python toolbox that gathers known exact and approximate sampling algorithms for both finite and continuous DPPs. The project is hosted on GitHub and equipped with an extensive documentation.			arxiv	['Guillermo Polito', 'R√©mi Bardenet', 'Michal Valko']	307.0
692	EmCa -- Electromagnetic-Cascades Simulation Package	Stephan Meighen-Berger	2019-07-16 09:59:23	http://arxiv.org/abs/1907.06924v2	Electromagnetic-Cascades (EmCa) is a Python package for the simulation of electromagnetic cascades in various materials. The showers are modeled using cascade equations and the relevant interactions, specifically pair production, Bremsstrahlung, Compton scattering and ionization. This methodology has the advantage of being computationally inexpensive and fast, unlike Monte Carlo methods. The code includes low and high energy material effects, allowing for a high range of validity of the simulation results. EmCa is easily extendable and offers a framework for testing different electromagnetic interaction models. In combination with MCEq, a Python package for hadronic particle showers using cascade equations, full simulations of atmospheric fluxes can be done.			arxiv	['Anatoli Fedynitch', 'Matthias Huber']	308.0
693	JeLLyFysh-Version1.0 -- a Python application for all-atom event-chain Monte Carlo	Philipp Hoellmer	2019-07-29 16:04:34	http://arxiv.org/abs/1907.12502v1	We present JeLLyFysh-Version1.0, an open-source Python application for event-chain Monte Carlo (ECMC), an event-driven irreversible Markov-chain Monte Carlo algorithm for classical N-body simulations in statistical mechanics, biophysics and electrochemistry. The application's architecture closely mirrors the mathematical formulation of ECMC. Local potentials, long-ranged Coulomb interactions and multi-body bending potentials are covered, as well as bounding potentials and cell systems including the cell-veto algorithm. Configuration files illustrate a number of specific implementations for interacting atoms, dipoles, and water molecules.			arxiv	['Liang Qin', 'Michael F. Faulkner', 'A. C. Maggs', 'Werner Krauth']	309.0
694	Python Implementation and Construction of Finite Abelian Groups	Paul Bradley	2017-11-15 21:26:09	http://arxiv.org/abs/1711.05814v1	Here we present a working framework to establish finite abelian groups in python. The primary aim is to allow new A-level students to work with examples of finite abelian groups using open source software. We include the code used in the implementation of the framework. We also prove some useful results regarding finite abelian groups which are used to establish the functions and help show how number theoretic results can blend with computational power when studying algebra. The groups established are based modular multiplication and addition. We include direct products of cyclic groups meaning the user has access to all finite abelian groups.			arxiv	['John Smethurst']	310.0
695	Pymc-learn: Practical Probabilistic Machine Learning in Python	Daniel Emaasit	2018-10-31 22:54:12	http://arxiv.org/abs/1811.00542v1	$\textit{Pymc-learn}$ is a Python package providing a variety of state-of-the-art probabilistic models for supervised and unsupervised machine learning. It is inspired by $\textit{scikit-learn}$ and focuses on bringing probabilistic machine learning to non-specialists. It uses a general-purpose high-level language that mimics $\textit{scikit-learn}$. Emphasis is put on ease of use, productivity, flexibility, performance, documentation, and an API consistent with $\textit{scikit-learn}$. It depends on $\textit{scikit-learn}$ and $\textit{pymc3}$ and is distributed under the new BSD-3 license, encouraging its use in both academia and industry. Source code, binaries, and documentation are available on http://github.com/pymc-learn/pymc-learn.			arxiv	[]	311.0
696	Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language	Matthew D. Hoffman	2018-11-29 02:13:37	http://arxiv.org/abs/1811.11926v1	Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies.			arxiv	['Matthew J. Johnson', 'Dustin Tran']	312.0
697	"Introduction to ""RORPack"": A Python Software Library for Robust Output Regulation"	Lassi Paunonen	2019-02-26 14:18:14	http://arxiv.org/abs/1902.09949v1	This document contains the mathematical introduction to RORPack - a Python software library for robust output tracking and disturbance rejection for linear PDE systems. The RORPack library is open-source and freely available at https://github.com/lassipau/rorpack/ The package contains functionality for automated construction of robust internal model based controllers, simulation of the controlled systems, visualisation of the results, as well as a collection of example cases on robust output regulation of controlled heat and wave equations.			arxiv	[]	313.0
698	EoN (Epidemics on Networks): a fast, flexible Python package for simulation, analytic approximation, and analysis of epidemics on networks	Joel C. Miller	2020-01-08 10:15:47	http://arxiv.org/abs/2001.02436v2	We provide a description of the Epidemics on Networks (EoN) python package designed for studying disease spread in static networks. The package consists of over $100$ methods available for users to perform stochastic simulation of a range of different processes including SIS and SIR disease, and generic simple or comlex contagions.			arxiv	['Tony TIng']	314.0
699	PySpike - A Python library for analyzing spike train synchrony	Mario Mulansky	2016-03-10 15:07:06	http://arxiv.org/abs/1603.03293v2	Understanding how the brain functions is one of the biggest challenges of our time. The analysis of experimentally recorded neural firing patterns (spike trains) plays a crucial role in addressing this problem. Here, the PySpike library is introduced, a Python package for spike train analysis providing parameter-free and time-scale independent measures of spike train synchrony. It allows to compute similarity and dissimilarity profiles, averaged values and distance matrices. Although mainly focusing on neuroscience, PySpike can also be applied in other contexts like climate research or social sciences. The package is available as Open Source on Github and PyPI.			arxiv	['Thomas Kreuz']	315.0
700	Adaptive Modular Exponentiation Methods v.s. Python's Power Function	Shiyu Ji	2017-07-06 04:12:25	http://arxiv.org/abs/1707.01898v1	In this paper we use Python to implement two efficient modular exponentiation methods: the adaptive m-ary method and the adaptive sliding-window method of window size k, where both m's are adaptively chosen based on the length of exponent. We also conduct the benchmark for both methods. Evaluation results show that compared to the industry-standard efficient implementations of modular power function in CPython and Pypy, our algorithms can reduce 1-5% computing time for exponents with more than 3072 bits.			arxiv	['Kun Wan']	316.0
701	Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling	Emmanuel Bacry	2017-07-10 18:18:24	http://arxiv.org/abs/1707.03003v2	Tick is a statistical learning library for Python~3, with a particular emphasis on time-dependent models, such as point processes, and tools for generalized linear models and survival analysis. The core of the library is an optimization module providing model computational classes, solvers and proximal operators for regularization. tick relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Source code and documentation can be downloaded from https://github.com/X-DataInitiative/tick			arxiv	['Martin Bompaire', 'St√©phane Ga√Øffas', 'Soren Poulsen']	317.0
702	Optimised finite difference computation from symbolic equations	Michael Lange	2017-07-12 15:51:35	http://arxiv.org/abs/1707.03776v1	Domain-specific high-productivity environments are playing an increasingly important role in scientific computing due to the levels of abstraction and automation they provide. In this paper we introduce Devito, an open-source domain-specific framework for solving partial differential equations from symbolic problem definitions by the finite difference method. We highlight the generation and automated execution of highly optimized stencil code from only a few lines of high-level symbolic Python for a set of scientific equations, before exploring the use of Devito operators in seismic inversion problems.			arxiv	['Navjot Kukreja', 'Fabio Luporini', 'Mathias Louboutin', 'Charles Yount', 'Jan H√ºckelheim', 'Gerard J. Gorman']	318.0
703	MDP environments for the OpenAI Gym	Andreas Kirsch	2017-09-26 14:52:23	http://arxiv.org/abs/1709.09069v1	The OpenAI Gym provides researchers and enthusiasts with simple to use environments for reinforcement learning. Even the simplest environment have a level of complexity that can obfuscate the inner workings of RL approaches and make debugging difficult. This whitepaper describes a Python framework that makes it very easy to create simple Markov-Decision-Process environments programmatically by specifying state transitions and rewards of deterministic and non-deterministic MDPs in a domain-specific language in Python. It then presents results and visualizations created with this MDP framework.			arxiv	[]	319.0
704	Efficient Pattern Matching in Python	Manuel Krebber	2017-09-29 20:14:47	http://arxiv.org/abs/1710.00077v1	Pattern matching is a powerful tool for symbolic computations. Applications include term rewriting systems, as well as the manipulation of symbolic expressions, abstract syntax trees, and XML and JSON data. It also allows for an intuitive description of algorithms in the form of rewrite rules. We present the open source Python module MatchPy, which offers functionality and expressiveness similar to the pattern matching in Mathematica. In particular, it includes syntactic pattern matching, as well as matching for commutative and/or associative functions, sequence variables, and matching with constraints. MatchPy uses new and improved algorithms to efficiently find matches for large pattern sets by exploiting similarities between patterns. The performance of MatchPy is investigated on several real-world problems.			arxiv	['Henrik Barthels', 'Paolo Bientinesi']	320.0
705	correlcalc: A `Generic' Recipe for Calculation of Two-point Correlation function	Yeluripati Rohin	2017-10-04 17:57:25	http://arxiv.org/abs/1710.01723v2	This article provides a method for quick computation of galaxy two-point correlation function(2pCF) from redshift surveys using python. One of the salient features of this approach is that it can be used for calculating galaxy clustering for any arbitrary geometry (or Cosmology) model. Being efficient enough to run fast on a low-spec desktop computer, this `recipe' can be used for quick validation of alternative models and for pedagogical purposes.			arxiv	[]	321.0
706	MatchingTools: a Python library for symbolic effective field theory calculations	Juan C. Criado	2017-10-17 18:00:34	http://arxiv.org/abs/1710.06445v2	MatchingTools is a Python library for doing symbolic calculations in effective field theory. It provides the tools to construct general models by defining their field content and their interaction Lagrangian. Once a model is given, the heavy particles can be integrated out at the tree level to obtain an effective Lagrangian in which only the light particles appear. After integration, some of the terms of the resulting Lagrangian might not be independent. MatchingTools contains functions for transforming these terms to rewrite them in terms of any chosen set of operators.			arxiv	[]	322.0
707	MatchPy: A Pattern Matching Library	Manuel Krebber	2017-10-16 22:29:25	http://arxiv.org/abs/1710.06915v1	Pattern matching is a powerful tool for symbolic computations, based on the well-defined theory of term rewriting systems. Application domains include algebraic expressions, abstract syntax trees, and XML and JSON data. Unfortunately, no lightweight implementation of pattern matching as general and flexible as Mathematica exists for Python Mathics,MacroPy,patterns,PyPatt. Therefore, we created the open source module MatchPy which offers similar pattern matching functionality in Python using a novel algorithm which finds matches for large pattern sets more efficiently by exploiting similarities between patterns.			arxiv	['Henrik Barthels', 'Paolo Bientinesi']	323.0
708	auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks	Michael Freitag	2017-12-12 16:43:33	http://arxiv.org/abs/1712.04382v2	auDeep is a Python toolkit for deep unsupervised representation learning from acoustic data. It is based on a recurrent sequence to sequence autoencoder approach which can learn representations of time series data by taking into account their temporal dynamics. We provide an extensive command line interface in addition to a Python API for users and developers, both of which are comprehensively documented and publicly available at https://github.com/auDeep/auDeep. Experimental results indicate that auDeep features are competitive with state-of-the art audio classification.			arxiv	['Shahin Amiriparian', 'Sergey Pugachevskiy', 'Nicholas Cummins', 'Bj√∂rn Schuller']	324.0
709	An MPI-Based Python Framework for Distributed Training with Keras	Dustin Anderson	2017-12-16 00:01:27	http://arxiv.org/abs/1712.05878v1	We present a lightweight Python framework for distributed training of neural networks on multiple GPUs or CPUs. The framework is built on the popular Keras machine learning library. The Message Passing Interface (MPI) protocol is used to coordinate the training process, and the system is well suited for job submission at supercomputing sites. We detail the software's features, describe its use, and demonstrate its performance on systems of varying sizes on a benchmark problem drawn from high-energy physics research.			arxiv	['Jean-Roch Vlimant', 'Maria Spiropulu']	325.0
710	CameraTransform: a Scientific Python Package for Perspective Camera Corrections	Richard Gerum	2017-12-20 12:13:44	http://arxiv.org/abs/1712.07438v1	Scientific applications often require an exact reconstruction of object positions and distances from digital images. Therefore, the images need to be corrected for perspective distortions. We present \textit{CameraTransform}, a python package that performs a perspective image correction whereby the height, tilt/roll angle and heading of the camera can be automatically obtained from the images if additional information such as GPS coordinates or object sizes are provided. We present examples of images of penguin colonies that are recorded with stationary cameras and from a helicopter.			arxiv	['Sebastian Richter', 'Alexander Winterl', 'Ben Fabry', 'Daniel Zitterbart']	326.0
711	Holographic Microscopy with Python and HoloPy	Solomon Barkley	2018-05-31 19:12:41	http://arxiv.org/abs/1806.00058v1	A holographic microscope captures interference patterns, or holograms, that encode three-dimensional (3D) information about the object being viewed. Computation is essential to extracting that 3D information. By wrapping low-level scattering codes and taking advantage of Python's data analysis ecosystem, HoloPy makes it easy for experimentalists to use modern, sophisticated inference methods to analyze holograms. The resulting data can be used to understand how small particles or microorganisms move and interact. The project illustrates how computational tools can enable experimental methods and new experiments.			arxiv	['Thomas G. Dimiduk', 'Jerome Fung', 'David M. Kaz', 'Vinothan N. Manoharan', 'Ryan McGorty', 'Rebecca W. Perry', 'Anna Wang']	327.0
712	Causal Discovery Toolbox: Uncover causal relationships in Python	Diviyan Kalainathan	2019-03-06 10:03:20	http://arxiv.org/abs/1903.02278v1	This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The 'cdt' package implements the end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'Bnlearn' and 'Pcalg' packages, together with algorithms for pairwise causal discovery such as ANM. 'cdt' is available under the MIT License at https://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox.			arxiv	['Olivier Goudet']	328.0
713	Fruitbat: A Python Package for Estimating Redshifts of Fast Radio Bursts	Adam J. Batten	2019-05-10 01:33:38	http://arxiv.org/abs/1905.04294v1	Fruitbat is an open source Python 2/3 package for estimating redshifts, energies and the galactic dispersion measure contributions of fast radio bursts (FRBs). Fruitbat combines various dispersion measure (DM) and redshift relations with the YMW16 galactic dispersion measure model into a single easy to use API.			arxiv	[]	329.0
714	Computer-aided modelling of complex physical systems with BondGraphTools	Peter Cudmore	2019-06-26 01:08:33	http://arxiv.org/abs/1906.10799v1	BondGraphTools is a Python library for scripted modelling of complex multi-physics systems. In contrast to existing modelling solutions, BondGraphTools is based upon the well established bond graph methodology, provides a programming interface for symbolic model composition, and is intended to be used in conjunction with the existing scientific Python toolchain. Here we discuss the design, implementation and use of BondGraphTools, demonstrate how it can be used to accelerate systems modelling with an example from optomechanics, and comment on current and future applications in cross-domain modelling, particularly in systems biology.			arxiv	['Peter J. Gawthrop', 'Michael Pan', 'Edmund J. Crampin']	330.0
715	metric-learn: Metric Learning Algorithms in Python	William de Vazelhes	2019-08-13 15:52:31	http://arxiv.org/abs/1908.04710v3	metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.			arxiv	['CJ Carey', 'Yuan Tang', 'Nathalie Vauquier', 'Aur√©lien Bellet']	331.0
716	Neural Network Distiller: A Python Package For DNN Compression Research	Neta Zmora	2019-10-27 10:42:15	http://arxiv.org/abs/1910.12232v1	This paper presents the philosophy, design and feature-set of Neural Network Distiller, an open-source Python package for DNN compression research. Distiller is a library of DNN compression algorithms implementations, with tools, tutorials and sample applications for various learning tasks. Its target users are both engineers and researchers, and the rich content is complemented by a design-for-extensibility to facilitate new research. Distiller is open-source and is available on Github at https://github.com/NervanaSystems/distiller.			arxiv	['Guy Jacob', 'Lev Zlotnik', 'Bar Elharar', 'Gal Novik']	332.0
717	PHS: A Toolbox for Parallel Hyperparameter Search	Peter Michael Habelitz	2020-02-26 12:17:54	http://arxiv.org/abs/2002.11429v2	We introduce an open source python framework named PHS - Parallel Hyperparameter Search to enable hyperparameter optimization on numerous compute instances of any arbitrary python function. This is achieved with minimal modifications inside the target function. Possible applications appear in expensive to evaluate numerical computations which strongly depend on hyperparameters such as machine learning. Bayesian optimization is chosen as a sample efficient method to propose the next query set of parameters.			arxiv	['Janis Keuper']	333.0
718	QSW_MPI: a framework for parallel simulation of quantum stochastic walks	Edric Matwiejew	2020-03-05 06:27:00	http://arxiv.org/abs/2003.02450v2	QSW_MPI is a python package developed for time-series simulation of continuous-time quantum stochastic walks. This model allows for the study of Markovian open quantum systems in the Lindblad formalism, including a generalisation of the continuous-time random walk and continuous-time quantum walk. Consisting of a python interface accessing parallelised Fortran libraries utilising sparse data structures, QSW_MPI is scalable to massively parallel computers, which makes possible the simulation of a wide range of walk dynamics on directed and undirected graphs of arbitrary complexity.			arxiv	['Jingbo Wang']	334.0
719	Petz map and Python's lunch	Ying Zhao	2020-03-06 19:31:35	http://arxiv.org/abs/2003.03406v2	We look at the interior operator reconstruction from the point of view of Petz map and study its complexity. We show that Petz maps can be written as precursors under the condition of perfect recovery. When we have the entire boundary system its complexity is related to the volume / action of the wormhole from the bulk operator to the boundary. When we only have access to part of the system, Python's lunch appears and its restricted complexity depends exponentially on the size of the subsystem one loses access to.			arxiv	[]	335.0
720	PySINDy: A Python package for the Sparse Identification of Nonlinear Dynamics from Data	Brian M. de Silva	2020-04-17 19:13:14	http://arxiv.org/abs/2004.08424v1	PySINDy is a Python package for the discovery of governing dynamical systems models from data. In particular, PySINDy provides tools for applying the sparse identification of nonlinear dynamics (SINDy) (Brunton et al. 2016) approach to model discovery. In this work we provide a brief description of the mathematical underpinnings of SINDy, an overview and demonstration of the features implemented in PySINDy (with code examples), practical advice for users, and a list of potential extensions to PySINDy. Software is available at https://github.com/dynamicslab/pysindy.			arxiv	['Kathleen Champion', 'Markus Quade', 'Jean-Christophe Loiseau', 'J. Nathan Kutz', 'Steven L. Brunton']	336.0
721	PESummary: the code agnostic Parameter Estimation Summary page builder	Charlie Hoy	2020-06-11 17:30:15	http://arxiv.org/abs/2006.06639v2	PESummary is a Python software package for processing and visualising data from any parameter estimation code. The easy to use Python executable scripts and extensive online documentation has resulted in PESummary becoming a key component in the international gravitational-wave analysis toolkit. PESummary has been developed to be more than just a post-processing tool with all outputs fully self-contained. PESummary has become central to making gravitational-wave inference analysis open and easily reproducible.			arxiv	['Vivien Raymond']	337.0
722	Pycro-manager: open-source software for integrated microscopy hardware control and image processing	Henry Pinkard	2020-06-19 19:06:37	http://arxiv.org/abs/2006.11330v1	{\mu}Manager, an open-source microscopy acquisition software, has been an essential tool for many microscopy experiments over the past 15 years, but is not easy to use for experiments in which image acquisition and analysis are closely coupled. This is because {\mu}Manager libraries are written in C++ and Java, whereas image processing is increasingly carried out with data science and machine learning tools most easily accessible through the Python programming language. We present Pycro-Manager, a tool that enables rapid development of such experiments, while also providing access to the wealth of existing tools within {\mu}Manager through Python.			arxiv	['Nico Stuurman', 'Laura Waller']	338.0
723	AnisoCADO: a python package for analytically generating adaptive optics point spread functions for the Extremely Large Telescope	Kieran Leschinski	2020-06-25 17:17:39	http://arxiv.org/abs/2006.14572v2	AnisoCADO is a Python package for generating images of the point spread function (PSF) for the european extremely large telescope (ELT). The code allows the user to set many of the most important atmospheric and observational parameters that influence the shape and strehl ratio of the resulting PSF, including but not limited to: the atmospheric turbulence profile, the guide star position for a single conjugate adaptive optics (SCAO) solution, differential telescope pupil transmission, etc. Documentation can be found at https://anisocado.readthedocs.io/en/latest/			arxiv	['Eric Gendron']	339.0
724	Spiral capacitor calculation using FEniCS	Slava Andrejev	2020-06-26 22:13:17	http://arxiv.org/abs/2006.16919v1	The paper shows how to optimize a water level sensor consisting of a cylinder with spiraling metal stripes on the side, using a powerful Python library FEniCS. It is shown how to reduce a 3D Laplace equation to a 2D, using a spiraling coordinate system; how to specify the correct boundary conditions for an open region; how to convert the partial differential equation to a variational form for FEniCS; and how to calculate the capacitance. Then the FEniCS code is shown that solves the Laplace equation and calculates the capacitance. The further numeric experiments show that there is an optimal combination of the spiral frequency and the width of the stripes that maximizes the sensitivity of the sensor. The Python code is given to calculate the optimum.			arxiv	[]	340.0
725	multivar_horner: a python package for computing Horner factorisations of multivariate polynomials	Jannik Michelfeit	2020-07-26 15:43:10	http://arxiv.org/abs/2007.13152v2	Many applications in the sciences require numerically stable and computationally efficient evaluation of multivariate polynomials. Finding beneficial representations of polynomials, such as Horner factorisations, is therefore crucial. multivar_horner, the python package presented here, is the first open source software for computing multivariate Horner factorisations. This work briefly outlines the functionality of the package and puts it into reference to previous work in the field. Benchmarks additionally prove the advantages of the implementation and Horner factorisations in general.			arxiv	[]	341.0
726	A Note on Particle Gibbs Method and its Extensions and Variants	Niharika Gauraha	2020-07-31 06:15:03	http://arxiv.org/abs/2007.15862v2	High-dimensional state trajectories of state-space models pose challenges for Bayesian inference. Particle Gibbs (PG) methods have been widely used to sample from the posterior of a state space model. Basically, particle Gibbs is a Particle Markov Chain Monte Carlo (PMCMC) algorithm that mimics the Gibbs sampler by drawing model parameters and states from their conditional distributions. This tutorial provides an introductory view on Particle Gibbs (PG) method and its extensions and variants, and illustrates through several examples of inference in non-linear state space models (SSMs). We also implement PG Samplers in two different programming languages: Python and Rust. Comparison of run-time performance of Python and Rust programs are also provided for various PG methods.			arxiv	[]	342.0
727	TB2J: a python package for computing magnetic interaction parameters	Xu He	2020-09-03 20:08:58	http://arxiv.org/abs/2009.01910v2	We present TB2J, a Python package for the automatic computation of magnetic interactions, including exchange and Dzyaloshinskii-Moriya interactions, between atoms of magnetic crystals from the results of density functional calculations. The program is based on the Green's function method with the local rigid spin rotation treated as a perturbation. As input,the package uses the output of either Wannier90, which is interfaced with many density functional theory packages,or of codes based on localized orbitals. A minimal user input is needed, which allows for easy integration into high-throughput workflows. The package is open source under BSD 2-Clause license, available at https://github.com/mailhexu/TB2J.			arxiv	['Nicole Helbig', 'Matthieu J. Verstraete', 'Eric Bousquet']	343.0
728	PySAD: A Streaming Anomaly Detection Framework in Python	Selim F. Yilmaz	2020-09-05 17:41:37	http://arxiv.org/abs/2009.02572v1	PySAD is an open-source python framework for anomaly detection on streaming data. PySAD serves various state-of-the-art methods for streaming anomaly detection. The framework provides a complete set of tools to design anomaly detection experiments ranging from projectors to probability calibrators. PySAD builds upon popular open-source frameworks such as PyOD and scikit-learn. We enforce software quality by enforcing compliance with PEP8 guidelines, functional testing and using continuous integration. The source code is publicly available on https://github.com/selimfirat/pysad.			arxiv	['Suleyman S. Kozat']	344.0
729	TorchKGE: Knowledge Graph Embedding in Python and PyTorch	Armand Boschin	2020-09-07 09:21:34	http://arxiv.org/abs/2009.02963v1	TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at https://github.com/torchkge-team/torchkge.			arxiv	[]	345.0
730	fugashi, a Tool for Tokenizing Japanese in Python	Paul McCann	2020-10-14 07:52:47	http://arxiv.org/abs/2010.06858v1	Recent years have seen an increase in the number of large-scale multilingual NLP projects. However, even in such projects, languages with special processing requirements are often excluded. One such language is Japanese. Japanese is written without spaces, tokenization is non-trivial, and while high quality open source tokenizers exist they can be hard to use and lack English documentation. This paper introduces fugashi, a MeCab wrapper for Python, and gives an introduction to tokenizing Japanese.			arxiv	[]	346.0
731	A Systematic Approach to Computing the Manipulator Jacobian and Hessian using the Elementary Transform Sequence	Jesse Haviland	2020-10-17 01:58:31	http://arxiv.org/abs/2010.08696v1	The elementary transform sequence (ETS) provides a universal method of describing the kinematics of any serial-link manipulator. The ETS notation is intuitive and easy to understand, while avoiding the complexity and limitations of Denvit-Hartenberg frame assignment. In this paper, we describe a systematic method for computing the manipulator Jacobian and Hessian (differential kinematics) using the ETS notation. Differential kinematics have many applications including numerical inverse kinematics, resolved-rate motion control and manipulability motion control. Furthermore, we provide an open-source Python library which implements our algorithm and can be interfaced with any serial-link manipulator (available at github.com/petercorke/robotics-toolbox-python).			arxiv	['Peter Corke']	347.0
732	HSTCosmicrays: A Python Package for Analyzing Cosmic Rays in HST Calibration Data	N. D. Miles	2020-11-23 18:24:04	http://arxiv.org/abs/2011.11604v1	HSTCosmicrays is a python-based pipeline designed to find and characterize cosmic rays found in dark frames (exposures taken with the shutter closed). Dark exposures are obtained routinely by all the Hubble Space Telescope (HST) instruments for calibration. The main processing pipeline runs locally or in the cloud on AWS. To date, we have characterized more than 1.2 billion cosmic rays in ~76,000 dark frames obtained with CCDs from the four active instruments ACS/HRC, ACS/WFC, STIS, WFC3/UVIS, and the legacy instrument WFPC2.			arxiv	['S. Deustua', 'G. Tancredi']	348.0
733	Design, Implementation, Comparison, and Performance analysis between Analog Butterworth and Chebyshev-I Low Pass Filter Using Approximation, Python and Proteus	Navid Fazle Rabbi	2021-01-25 17:17:17	http://arxiv.org/abs/2102.09048v1	Filters are broadly used in signal processing and communication systems in noise reduction. Butterworth, Chebyshev-I Analog Low Pass Filters are developed and implemented in this paper. The filters are manually calculated using approximations and verified using Python Programming Language. Filters are also simulated in Proteus 8 Professional and implemented in the Hardware Lab using the necessary components. This paper also denotes the comparison and performance analysis of filters using Manual Computations, Hardware, and Software.			arxiv	[]	349.0
734	PyTorch, Explain! A Python library for Logic Explained Networks	Pietro Barbiero	2021-05-25 06:41:54	http://arxiv.org/abs/2105.11697v2	"""PyTorch, Explain!"" is a Python module integrating a variety of state-of-the-art approaches to provide logic explanations from neural networks. This package focuses on bringing these methods to non-specialists. It has minimal dependencies and it is distributed under the Apache 2.0 licence allowing both academic and commercial use. Source code and documentation can be downloaded from the github repository: https://github.com/pietrobarbiero/pytorch_explain."			arxiv	['Gabriele Ciravegna', 'Dobrik Georgiev', 'Franscesco Giannini']	350.0
735	PSY-TaLiRo: A Python Toolbox for Search-Based Test Generation for Cyber-Physical Systems	Quinn Thibeault	2021-06-04 01:35:06	http://arxiv.org/abs/2106.02200v1	In this paper, we present the Python package PSY-TaLiRo which is a toolbox for temporal logic robustness guided falsification of Cyber-Physical Systems (CPS). PSY-TaLiRo is a completely modular toolbox supporting multiple temporal logic offline monitors as well as optimization engines for test case generation. Among the benefits of PSY-TaLiRo is that it supports search-based test generation for many different types of systems under test. All PSY-TaLiRo modules can be fully modified by the users to support new optimization and robustness computation engines as well as any System under Test (SUT).			arxiv	['Jacob Anderson', 'Aniruddh Chandratre', 'Giulia Pedrielli', 'Georgios Fainekos']	351.0
736	hankl: A lightweight Python implementation of the FFTLog algorithm for Cosmology	Minas Karamanis	2021-06-11 12:08:26	http://arxiv.org/abs/2106.06331v1	We introduce hankl, a lightweight Python implementation of the FFTLog algorithm for Cosmology. The FFTLog algorithm is an extension of the Fast Fourier Transform (FFT) for logarithmically spaced periodic sequences. It can be used to efficiently compute Hankel transformations, which are paramount for many modern cosmological analyses that are based on the power spectrum or the 2-point correlation function multipoles. The code is well-tested, open source, and publicly available.			arxiv	['Florian Beutler']	352.0
737	Feature Selection Tutorial with Python Examples	Padraig Cunningham	2021-06-11 14:49:43	http://arxiv.org/abs/2106.06437v1	In Machine Learning, feature selection entails selecting a subset of the available features in a dataset to use for model development. There are many motivations for feature selection, it may result in better models, it may provide insight into the data and it may deliver economies in data gathering or data processing. For these reasons feature selection has received a lot of attention in data analytics research. In this paper we provide an overview of the main methods and present practical examples with Python implementations. While the main focus is on supervised feature selection techniques, we also cover some feature transformation methods.			arxiv	['Bahavathy Kathirgamanathan', 'Sarah Jane Delany']	353.0
738	ADAPT : Awesome Domain Adaptation Python Toolbox	Antoine de Mathelin	2021-07-07 07:20:21	http://arxiv.org/abs/2107.03049v2	In this paper, we introduce the ADAPT library, an open source Python API providing the implementation of the main transfer learning and domain adaptation methods. The library is designed with a user friendly approach to facilitate the access to domain adaptation for a wide public. ADAPT is compatible with scikit-learn and TensorFlow and a full documentation is proposed online https://adapt-python.github.io/adapt/ with a substantial gallery of examples.			arxiv	['Mounir Atiq', 'Guillaume Richard', 'Alejandro de la Concha', 'Mouad Yachouti', 'Fran√ßois Deheeger', 'Mathilde Mougeot', 'Nicolas Vayatis']	354.0
739	Introducing $\texttt{NatPy}$, a simple and convenient Python module for dealing with natural units	Tomas L. Howson	2021-07-28 06:15:23	http://arxiv.org/abs/2108.07173v1	In high energy physics, the standard convention for expressing physical quantities is natural units. The standard paradigm sets $c = \hbar = \epsilon_0 = 1$ and hence implicitly rescales all physical quantities that depend on unit derivatives of these quantities. We introduce $\texttt{NatPy}$, a simple Python module that defines user-friendly unit objects that can be used and converted within any predefined system of natural units. In this note, we will first introduce, then overview, the algebraic methods utilised by the $\texttt{NatPy}$ module.			arxiv	['Andre Scaffidi']	355.0
740	VarStar Detect, a Python library dedicated to the semi-automatic detection of stellar variability	Jorge Perez Gonzalez	2021-09-01 08:07:56	http://arxiv.org/abs/2109.06347v1	VarStar Detect is a Python package available on PyPI optimized for the detection of variability inside photometric measurements. Based off of the Least Squares method of regression, VarStar Detect calculates the amplitude of a Fourier Polynomial fit of the data as a measure of variability to assess if the star is indeed variable. This work shows the mathematical background of the package and an analysis of the code's functionality on TESS Sector 1 Data.			arxiv	['Nicolas Carrizosa Arias', 'Andres Cadenas Blanco']	356.0
741	Natlog: a Lightweight Logic Programming Language with a Neuro-symbolic Touch	Paul Tarau	2021-09-17 01:47:57	http://arxiv.org/abs/2109.08291v1	We introduce Natlog, a lightweight Logic Programming language, sharing Prolog's unification-driven execution model, but with a simplified syntax and semantics. Our proof-of-concept Natlog implementation is tightly embedded in the Python-based deep-learning ecosystem with focus on content-driven indexing of ground term datasets. As an overriding of our symbolic indexing algorithm, the same function can be delegated to a neural network, serving ground facts to Natlog's resolution engine. Our open-source implementation is available as a Python package at https://pypi.org/project/natlog/ .			arxiv	[]	357.0
742	pygrank: A Python Package for Graph Node Ranking	Emmanouil Krasanakis	2021-10-18 13:13:21	http://arxiv.org/abs/2110.09274v1	We introduce pygrank, an open source Python package to define, run and evaluate node ranking algorithms. We provide object-oriented and extensively unit-tested algorithm components, such as graph filters, post-processors, measures, benchmarks and online tuning. Computations can be delegated to numpy, tensorflow or pytorch backends and fit in back-propagation pipelines. Classes can be combined to define interoperable complex algorithms. Within the context of this paper we compare the package with related alternatives and demonstrate its flexibility and ease of use with code examples.			arxiv	['Symeon Papadopoulos', 'Ioannis Kompatsiaris', 'Andreas Symeonidis']	358.0
743	GGLasso -- a Python package for General Graphical Lasso computation	Fabian Schaipp	2021-10-20 12:18:14	http://arxiv.org/abs/2110.10521v1	We introduce GGLasso, a Python package for solving General Graphical Lasso problems. The Graphical Lasso scheme, introduced by (Friedman 2007) (see also (Yuan 2007; Banerjee 2008)), estimates a sparse inverse covariance matrix $\Theta$ from multivariate Gaussian data $\mathcal{X} \sim \mathcal{N}(\mu, \Sigma) \in \mathbb{R}^p$. Originally proposed by (Dempster 1972) under the name Covariance Selection, this estimation framework has been extended to include latent variables in (Chandrasekaran 2012). Recent extensions also include the joint estimation of multiple inverse covariance matrices, see, e.g., in (Danaher 2013; Tomasi 2018). The GGLasso package contains methods for solving a general problem formulation, including important special cases, such as, the single (latent variable) Graphical Lasso, the Group, and the Fused Graphical Lasso.			arxiv	['Christian L. M√ºller', 'Oleg Vlasovets']	359.0
744	Being nice to the server: Wrapping a REST API for a cosmological distance/velocity calculator with Python	Juan Cabral	2021-10-23 15:44:07	http://arxiv.org/abs/2110.12249v1	In this paper we present PyCF3, a python client for the cosmological distance-velocity calculator CosmicFlow-3. The project has a cache and retry system designed with the objective of reducing the stress on the server and mitigating the waiting times of the users in the calculations. We also address Quality Assurance code standards and availability of the code.			arxiv	['Ehsan Kourkchi', 'Martin Beroiz', 'Erik Peterson', 'Bruno S√°nchez']	360.0
745	libcommute/pycommute: A quantum operator algebra domain-specific language and exact diagonalization toolkit	Igor Krivenko	2021-10-25 10:08:12	http://arxiv.org/abs/2110.12775v2	I present libcommute, a C++11/14/17 template library that implements a domain-specific language for easy manipulating of polynomial operators used in the quantum many-body theory, as well as a software development toolkit for exact diagonalization codes. The library is written with expressiveness, extensibility and performance in mind. It features simple syntax for commonly used abstractions and algorithms, is well documented and covered by unit tests. libcommute is supplemented with Python 3 bindings called pycommute. They are useful for solving small scale diagonalization problems, rapid prototyping and wrapping of high performance libcommute-based computational cores in Python.			arxiv	[]	361.0
746	rustworkx: A High-Performance Graph Library for Python	Matthew Treinish	2021-10-28 15:34:21	http://arxiv.org/abs/2110.15221v4	In rustworkx, we provide a high-performance, flexible graph library for Python. rustworkx is inspired by NetworkX but addresses many performance concerns of the latter. rustworkx is written in Rust and is particularly suited for performance-sensitive applications that use graph representations.			arxiv	['Ivan Carvalho', 'Georgios Tsilimigkounakis', 'Nahum S√°']	362.0
747	idSTLPy: A Python Toolbox for Active Perception and Control	Rafael Rodrigues da Silva	2021-11-04 15:32:43	http://arxiv.org/abs/2111.02943v1	This paper describes a Python toolbox for active perception and control synthesis of probabilistic signal temporal logic (PrSTL) formulas of switched linear systems with additive Gaussian disturbances and measurement noises. We implement a counterexample-guided synthesis strategy that combines Bounded Model Checking, linear programming, and sampling-based motion planning techniques. We illustrate our approach and the toolbox throughout the paper with a motion planning example for a vehicle with noisy localization. The code is available at \url{https://codeocean.com/capsule/0013534/tree}.			arxiv	['Kunal Yadav', 'Hai Lin']	363.0
748	VisualEnv: visual Gym environments with Blender	Andrea Scorsoglio	2021-11-15 21:43:43	http://arxiv.org/abs/2111.08096v2	In this paper VisualEnv, a new tool for creating visual environment for reinforcement learning is introduced. It is the product of an integration of an open-source modelling and rendering software, Blender, and a python module used to generate environment model for simulation, OpenAI Gym. VisualEnv allows the user to create custom environments with photorealistic rendering capabilities and full integration with python. The framework is described and tested on a series of example problems that showcase its features for training reinforcement learning agents.			arxiv	['Roberto Furfaro']	364.0
749	partycls: A Python package for structural clustering	Joris Paret	2021-11-19 10:09:29	http://arxiv.org/abs/2111.10133v1	partycls is a Python framework for cluster analysis of systems of interacting particles. By grouping particles that share similar structural or dynamical properties, partycls enables rapid and unsupervised exploration of the system's relevant features. It provides descriptors suitable for applications in condensed matter physics and integrates the necessary tools of unsupervised learning, such as dimensionality reduction, into a streamlined workflow. Through a simple and expressive interface, partycls allows one to open a trajectory file, perform a clustering based on the selected structural descriptor, and analyze and save the results with only a few lines of code.			arxiv	['Daniele Coslovich']	365.0
750	FCMpy: A Python Module for Constructing and Analyzing Fuzzy Cognitive Maps	Samvel Mkhitaryan	2021-11-24 19:21:14	http://arxiv.org/abs/2111.12749v1	FCMpy is an open source package in Python for building and analyzing Fuzzy Cognitive Maps. More specifically, the package allows 1) deriving fuzzy causal weights from qualitative data, 2) simulating the system behavior, 3) applying machine learning algorithms (e.g., Nonlinear Hebbian Learning, Active Hebbian Learning, Genetic Algorithms and Deterministic Learning) to adjust the FCM causal weight matrix and to solve classification problems, and 4) implementing scenario analysis by simulating hypothetical interventions (i.e., analyzing what-if scenarios).			arxiv	['Philippe J. Giabbanelli', 'Maciej K. Wozniak', 'Gonzalo Napoles', 'Nanne K. de Vries', 'Rik Crutzen']	366.0
751	AIMpy: A Python code to solve Schr√∂dinger-like equations with the asymptotic iteration method	Mesut Karako√ß	2021-12-06 11:13:47	http://arxiv.org/abs/2112.02934v1	"This paper is dedicated to present an open-source program so-called \emph{AIMpy} built on Python language. \emph{AIMpy} is a solver for Schr\""{o}dinger-like differential equations using Asymptotic Iteration Method (AIM). To confirm the code works seamlessly, it has been shown through the paper with recalculation of some previously studied eigenvalue examples that the code can reproduce their results very well."			arxiv	[]	367.0
752	pyscreener: A Python Wrapper for Computational Docking Software	David E. Graff	2021-12-17 17:40:47	http://arxiv.org/abs/2112.10575v1	pyscreener is a Python library that seeks to alleviate the challenges of large-scale structure-based design using computational docking. It provides a simple and uniform interface that is agnostic to the backend docking engine with which to calculate the docking score of a given molecule in a specified active site. Additionally, pyscreener features first-class support for task distribution, allowing users to seamlessly scale their code from a local, multi-core setup to a large, heterogeneous resource allocation.			arxiv	['Connor W. Coley']	368.0
753	repro_eval: A Python Interface to Reproducibility Measures of System-oriented IR Experiments	Timo Breuer	2022-01-19 14:00:30	http://arxiv.org/abs/2201.07599v1	In this work we introduce repro_eval - a tool for reactive reproducibility studies of system-oriented information retrieval (IR) experiments. The corresponding Python package provides IR researchers with measures for different levels of reproduction when evaluating their systems' outputs. By offering an easily extensible interface, we hope to stimulate common practices when conducting a reproducibility study of system-oriented IR experiments.			arxiv	['Nicola Ferro', 'Maria Maistro', 'Philipp Schaer']	369.0
754	MISeval: a Metric Library for Medical Image Segmentation Evaluation	Dominik M√ºller	2022-01-23 23:06:47	http://arxiv.org/abs/2201.09395v1	Correct performance assessment is crucial for evaluating modern artificial intelligence algorithms in medicine like deep-learning based medical image segmentation models. However, there is no universal metric library in Python for standardized and reproducible evaluation. Thus, we propose our open-source publicly available Python package MISeval: a metric library for Medical Image Segmentation Evaluation. The implemented metrics can be intuitively used and easily integrated into any performance assessment pipeline. The package utilizes modern CI/CD strategies to ensure functionality and stability. MISeval is available from PyPI (miseval) and GitHub: https://github.com/frankkramer-lab/miseval.			arxiv	['Dennis Hartmann', 'Philip Meyer', 'Florian Auer', 'I√±aki Soto-Rey', 'Frank Kramer']	370.0
755	Patapasco: A Python Framework for Cross-Language Information Retrieval Experiments	Cash Costello	2022-01-24 23:03:36	http://arxiv.org/abs/2201.09996v1	While there are high-quality software frameworks for information retrieval experimentation, they do not explicitly support cross-language information retrieval (CLIR). To fill this gap, we have created Patapsco, a Python CLIR framework. This framework specifically addresses the complexity that comes with running experiments in multiple languages. Patapsco is designed to be extensible to many language pairs, to be scalable to large document collections, and to support reproducible experiments driven by a configuration file. We include Patapsco results on standard CLIR collections using multiple settings.			arxiv	['Eugene Yang', 'Dawn Lawrie', 'James Mayfield']	371.0
756	DFORMPY: A Python Library for visualising and zooming on differential forms	Moustafa Gharamti	2022-01-16 10:51:04	http://arxiv.org/abs/2201.10517v1	We present the v1.0.1 release of DFormPy, the first Python library providing an interactive visualisation of differential forms. DFormPy is also capable of exterior algebra and vector calculus, building on the capabilities of NumPy and matplotlib. This short paper will demonstrate the functionalities of the library, briefly outlining the mathematics involved with our objects and the methods available to the user. DFormPy is an open source library with interactive GUI released under MIT license at https://github.com/MostaphaG/Summer_project-df			arxiv	['Maciej Jarema', 'Samuel Kirwin-Jones']	372.0
757	partitura: A Python Package for Handling Symbolic Musical Data	Maarten Grachten	2022-01-31 11:40:17	http://arxiv.org/abs/2201.13144v1	This demo paper introduces partitura, a Python package for handling symbolic musical information. The principal aim of this package is to handle richly structured musical information as conveyed by modern staff music notation. It provides a much wider range of possibilities to deal with music than the more reductive (but very common) piano roll-oriented approach inspired by the MIDI standard. The package is an open source project and is available on GitHub.			arxiv	['Carlos Cancino-Chac√≥n', 'Thassilo Gadermaier']	373.0
758	Counting Molecules: Python based scheme for automated enumeration and categorization of molecules in scanning tunneling microscopy images	Jack Hellerstedt	2022-03-03 20:27:45	http://arxiv.org/abs/2203.01998v1	Scanning tunneling and atomic force microscopies (STM/nc-AFM) are rapidly progressing to offer unprecedented spatial resolution of a diverse array of chemical species. In particular, they are employed to characterize on-surface chemical reactions by directly examining precursors and products. Chiral effects and self-assembled structures can also be investigated. This open source, modular, python based scheme automates the categorization of a variety of molecules present in medium sized (10$\times$10 to 100$\times$100 nm) scanned probe images.			arxiv	['Ale≈° Cahl√≠k', 'Martin ≈†vec', 'Oleksandr Stetsovych', 'Tyler Hennen']	374.0
759	yonder: A python package for data denoising and reconstruction	Peng Chen	2022-03-09 08:25:31	http://arxiv.org/abs/2203.08071v1	We present a standalone implementation of a data-deconvolution method based on singular value decomposition. The tool is written in python and packaged in the open-source yonder package. yonder receives as input two matrices, one for the data and another for the errors, and outputs a denoised version of the original dataset. In this Research Note, we briefly describe the methodology and show a demonstration of the yonder on a simulated dataset.			arxiv	['Rafael S. de Souza']	375.0
760	radioactivedecay: A Python package for radioactive decay calculations	Alex Malins	2022-03-18 06:22:51	http://arxiv.org/abs/2203.09761v1	radioactivedecay is a Python package for radioactive decay modelling. It contains functions to fetch decay data, define inventories of nuclides and perform decay calculations. The default nuclear decay dataset supplied with radioactivedecay is based on ICRP Publication 107, which covers 1252 radioisotopes of 97 elements. The code calculates an analytical solution to a matrix form of the decay chain differential equations using double or higher precision numerical operations. There are visualization functions for drawing decay chain diagrams and plotting activity decay curves.			arxiv	['Thom Lemoine']	376.0
761	Delicatessen: M-Estimation in Python	Paul N Zivich	2022-03-21 19:17:26	http://arxiv.org/abs/2203.11300v3	M-estimation is a general statistical framework that simplifies estimation. Here, we introduce delicatessen, a Python library that automates the tedious calculations of M-estimation, and supports both built-in user-specified estimating equations. To highlight the utility of delicatessen for quantitative data analysis, we provide several illustrations common to life science research: linear regression robust to outliers, estimation of a dose-response curve, and standardization of results.			arxiv	['Mark Klose', 'Stephen R Cole', 'Jessie K Edwards', 'Bonnie E Shook-Sa']	377.0
762	GEMA: An open-source Python library for self-organizing-maps	Alvaro J. Garcia-Tejedor	2022-02-17 10:49:01	http://arxiv.org/abs/2203.13190v1	Organizations have realized the importance of data analysis and its benefits. This in combination with Machine Learning algorithms has allowed to solve problems more easily, making these processes less time-consuming. Neural networks are the Machine Learning technique that is recently obtaining very good best results. This paper describes an open-source Python library called GEMA developed to work with a type of neural network model called Self-Organizing-Maps. GEMA is freely available under GNU General Public License at GitHub (https://github.com/ufvceiec/GEMA). The library has been evaluated in different a particular use case obtaining accurate results.			arxiv	['Alberto Nogales']	378.0
763	Applications of the Numerov method to simple quantum systems using Python	Francisco Caruso	2022-03-29 06:19:46	http://arxiv.org/abs/2203.15262v1	Numerov's numerical method is developed in a didactic way by using Python in its {\it Jupyter Notebook} version 6.0.3 for three different quantum physical systems: the hydrogen atom, a molecule governed by the Morse potential and for a quantum dot. After a brief introduction to the Numerov method, the complete code to calculate the eigenfunctions and eigenvalues of the hydrogen atom is presented. The necessary code changes to calculate the other two examples are also provided in the sequel.			arxiv	['Vitor Oguri', 'Felipe Silveira']	379.0
764	Continual Inference: A Library for Efficient Online Inference with Deep Neural Networks in PyTorch	Lukas Hedegaard	2022-04-07 13:03:09	http://arxiv.org/abs/2204.03418v1	We present Continual Inference, a Python library for implementing Continual Inference Networks (CINs) in PyTorch, a class of Neural Networks designed specifically for efficient inference in both online and batch processing scenarios. We offer a comprehensive introduction and guide to CINs and their implementation in practice, and provide best-practices and code examples for composing complex modules for modern Deep Learning. Continual Inference is readily downloadable via the Python Package Index and at \url{www.github.com/lukashedegaard/continual-inference}.			arxiv	['Alexandros Iosifidis']	380.0
765	DADApy: Distance-based Analysis of DAta-manifolds in Python	Aldo Glielmo	2022-05-04 08:41:59	http://arxiv.org/abs/2205.03373v2	DADApy is a python software package for analysing and characterising high-dimensional data manifolds. It provides methods for estimating the intrinsic dimension and the probability density, for performing density-based clustering and for comparing different distance metrics. We review the main functionalities of the package and exemplify its usage in toy cases and in a real-world application. DADApy is freely available under the open-source Apache 2.0 license.			arxiv	"['Iuri Macocco', 'Diego Doimo', 'Matteo Carli', 'Claudio Zeni', 'Romina Wild', ""Maria d'Errico"", 'Alex Rodriguez', 'Alessandro Laio']"	381.0
766	Towards understanding the central limit theorem by learning Python basics	Zolt√°n Kov√°cs	2022-05-17 11:08:24	http://arxiv.org/abs/2205.08233v1	We report on a first experiment about an email based course that connects learning Python basics and introductory probability theory. In the experiment 7 short sequences of homework were sent out to prospective mathematics teachers who did not have any programming background formerly, but already had some minor knowledge on probability theory. The experiment was about to decide if learning basics of programming can promote understanding main concepts of probability theory.			arxiv	['Alexander Thaller']	382.0
767	PyTSK: A Python Toolbox for TSK Fuzzy Systems	Yuqi Cui	2022-06-07 13:57:39	http://arxiv.org/abs/2206.03310v1	This paper presents PyTSK, a Python toolbox for developing Takagi-Sugeno-Kang (TSK) fuzzy systems. Based on scikit-learn and PyTorch, PyTSK allows users to optimize TSK fuzzy systems using fuzzy clustering or mini-batch gradient descent (MBGD) based algorithms. Several state-of-the-art MBGD-based optimization algorithms are implemented in the toolbox, which can improve the generalization performance of TSK fuzzy systems, especially for big data applications. PyTSK can also be easily extended and customized for more complicated algorithms, such as modifying the structure of TSK fuzzy systems, developing more sophisticated training algorithms, and combining TSK fuzzy systems with neural networks. The code of PyTSK can be found at https://github.com/YuqiCui/pytsk.			arxiv	['Dongrui Wu', 'Xue Jiang', 'Yifan Xu']	383.0
768	galmask: A Python package for unsupervised galaxy masking	Yash Gondhalekar	2022-06-12 00:02:20	http://arxiv.org/abs/2206.06787v1	Galaxy morphological classification is a fundamental aspect of galaxy formation and evolution studies. Various machine learning tools have been developed for automated pipeline analysis of large-scale surveys, enabling a fast search for objects of interest. However, crowded regions in the image may pose a challenge as they can lead to bias in the learning algorithm. In this Research Note, we present galmask, an open-source package for unsupervised galaxy masking to isolate the central object of interest in the image. galmask is written in Python and can be installed from PyPI via the pip command.			arxiv	['Rafael S. de Souza', 'Ana L. Chies-Santos']	384.0
769	EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless Machine Learning Integration	Moshe Sipper	2022-07-21 08:49:09	http://arxiv.org/abs/2207.10367v2	EC-KitY is a comprehensive Python library for doing evolutionary computation (EC), licensed under the BSD 3-Clause License, and compatible with scikit-learn. Designed with modern software engineering and machine learning integration in mind, EC-KitY can support all popular EC paradigms, including genetic algorithms, genetic programming, coevolution, evolutionary multi-objective optimization, and more. This paper provides an overview of the package, including the ease of setting up an EC experiment, the architecture, the main features, and a comparison with other libraries.			arxiv	['Tomer Halperin', 'Itai Tzruia', 'Achiya Elyasaf']	385.0
770	Frouros: A Python library for drift detection in machine learning systems	Jaime C√©spedes-Sisniega	2022-08-14 15:25:41	http://arxiv.org/abs/2208.06868v4	Frouros is an open-source Python library capable of detecting drift in machine learning systems. It provides a combination of classical and more recent algorithms for drift detection: both concept and data drift. We have designed it with the objective of making it compatible with any machine learning framework and easily adaptable to real-world use cases. The library is developed following a set of best development and continuous integration practices to ensure ease of maintenance and extensibility. The source code is available at https://github.com/IFCA/frouros.			arxiv	['√Ålvaro L√≥pez-Garc√≠a']	386.0
771	YALTAPy and YALTAPy_Online: Python toolboxes for the $H_\infty$-stability analysis of classical and fractional systems with commensurate delays	Hugo Cavalera	2022-09-11 13:14:44	http://arxiv.org/abs/2209.04857v1	The aim of this paper is to give a presentation of the Python toolbox YALTAPy dedicated to the stability study of standard and fractional delay systems as well as its online version YALTAPy_Online. Both toolboxes are derived from YALTA whose functionalities will be recalled here. Examples will be given to show how these toolboxes may be used.			arxiv	['Jayvir Raj', 'Guilherme Mazanti', 'Catherine Bonnet']	387.0
772	HiPart: Hierarchical Divisive Clustering Toolbox	Panagiotis Anagnostou	2022-09-18 23:48:43	http://arxiv.org/abs/2209.08680v1	This paper presents the HiPart package, an open-source native python library that provides efficient and interpret-able implementations of divisive hierarchical clustering algorithms. HiPart supports interactive visualizations for the manipulation of the execution steps allowing the direct intervention of the clustering outcome. This package is highly suited for Big Data applications as the focus has been given to the computational efficiency of the implemented clustering methodologies. The dependencies used are either Python build-in packages or highly maintained stable external packages. The software is provided under the MIT license. The package's source code and documentation can be found at https://github.com/panagiotisanagnostou/HiPart.			arxiv	['Sotiris Tasoulis', 'Vassilis Plagianakos', 'Dimitris Tasoulis']	388.0
773	BiFold: A Python code for the calculation of double folded (bifold) potentials with density-in/dependent nucleon-nucleon interactions	Mesut Karako√ß	2022-09-25 21:09:36	http://arxiv.org/abs/2209.13344v1	BiFold calculates the density-dependent (DDM3Y$n$, BDM3Y$n$, CDM3Y$n$) or independent double folded potentials between two colliding spherical nuclei. It is written in a Python package form to give the ability to use the potentials directly in a nuclear reaction/structure code. In addition to using Woods-Saxon/Fermi or Gaussian functions, the code also allows for the definition of nuclear matter densities using pre-calculated densities in a data file. The manuscript provides an overview of the double folding model and the use of the code.			arxiv	[]	389.0
774	Rieoptax: Riemannian Optimization in JAX	Saiteja Utpala	2022-10-10 16:55:32	http://arxiv.org/abs/2210.04840v1	We present Rieoptax, an open source Python library for Riemannian optimization in JAX. We show that many differential geometric primitives, such as Riemannian exponential and logarithm maps, are usually faster in Rieoptax than existing frameworks in Python, both on CPU and GPU. We support various range of basic and advanced stochastic optimization solvers like Riemannian stochastic gradient, stochastic variance reduction, and adaptive gradient methods. A distinguishing feature of the proposed toolbox is that we also support differentially private optimization on Riemannian manifolds.			arxiv	['Andi Han', 'Pratik Jawanpuria', 'Bamdev Mishra']	390.0
775	Pyrit: A Finite Element Based Field Simulation Software Written in Python	Jonas Bundschuh	2022-10-21 14:18:22	http://arxiv.org/abs/2210.11983v2	Pyrit is a field simulation software based on the finite element method written in Python to solve coupled systems of partial differential equations. It is designed as a modular software that is easily modifiable and extendable. The framework can, therefore, be adapted to various activities, i.e. research, education and industry collaboration.			arxiv	['M. Greta Ruppert', 'Yvonne Sp√§ck-Leigsnering']	391.0
776	The Interactive Modeling of a Binary Star System	Ayse Pelin Dedeler	2022-10-25 10:13:46	http://arxiv.org/abs/2210.14227v1	An interactive binary star system simulation was developed to be showcased on an educational platform. The main purpose of the project is to provide insight into the orbital mechanics of such star systems with the help of a three-body simulation. The initial simulation script was written in the Python programming language with the help of the VPython addition. The custom-made models were created on Blender and exported. For the final implementation of the simulation on the Godot game engine, the Python code was converted into GDScript and the Blender models were re-textured.			arxiv	['Canberk Soytekin']	392.0
777	Minimalist Data Wrangling with Python	Marek Gagolewski	2022-11-09 01:24:39	http://arxiv.org/abs/2211.04630v1	Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/.			arxiv	[]	393.0
778	Novelpy: A Python package to measure novelty and disruptiveness of bibliometric and patent data	Pierre Pelletier	2022-11-18 16:52:34	http://arxiv.org/abs/2211.10346v1	Novelpy (v1.2) is an open-source Python package designed to compute bibliometrics indicators. The package aims to provide a tool to the scientometrics community that centralizes different measures of novelty and disruptiveness, enables their comparison and fosters reproducibility. This paper offers a comprehensive review of the different indicators available in Novelpy by formally describing these measures (both mathematically and graphically) and presenting their benefits and limitations. We then compare the different measures on a random sample of 1.5M articles drawn from Pubmed Knowledge Graph to demonstrate the module's capabilities. We encourage anyone interested to participate in the development of future versions.			arxiv	['Kevin Wirtz']	394.0
779	QMKPy: A Python Testbed for the Quadratic Multiple Knapsack Problem	Karl-Ludwig Besser	2022-11-14 10:17:30	http://arxiv.org/abs/2211.17222v1	QMKPy provides a Python framework for modeling and solving the quadratic multiple knapsack problem (QMKP). It is primarily aimed at researchers who develop new solution algorithms for the QMKP. QMKPy therefore mostly functions as a testbed to quickly implement novel algorithms and compare their results with existing ones. However, the package also already includes implementations of established algorithms for those who only need to solve a QMKP as part of their research.			arxiv	['Eduard A. Jorswieck']	395.0
780	DeeProb-kit: a Python Library for Deep Probabilistic Modelling	Lorenzo Loconte	2022-12-08 17:02:16	http://arxiv.org/abs/2212.04403v1	DeeProb-kit is a unified library written in Python consisting of a collection of deep probabilistic models (DPMs) that are tractable and exact representations for the modelled probability distributions. The availability of a representative selection of DPMs in a single library makes it possible to combine them in a straightforward manner, a common practice in deep learning research nowadays. In addition, it includes efficiently implemented learning techniques, inference routines, statistical algorithms, and provides high-quality fully-documented APIs. The development of DeeProb-kit will help the community to accelerate research on DPMs as well as to standardise their evaluation and better understand how they are related based on their expressivity.			arxiv	['Gennaro Gala']	396.0
781	TextDescriptives: A Python package for calculating a large variety of metrics from text	Lasse Hansen	2023-01-05 13:19:17	http://arxiv.org/abs/2301.02057v3	TextDescriptives is a Python package for calculating a large variety of metrics from text. It is built on top of spaCy and can be easily integrated into existing workflows. The package has already been used for analysing the linguistic stability of clinical texts, creating features for predicting neuropsychiatric conditions, and analysing linguistic goals of primary school students. This paper describes the package and its features.			arxiv	['Ludvig Renbo Olsen', 'Kenneth Enevoldsen']	397.0
782	quTARANG: A python GPE solver to study turbulence in quantum systems	Shawan Kumar Jha	2023-01-19 19:26:08	http://arxiv.org/abs/2301.08275v2	quTARANG is a Python-based general-purpose Gross-Pitaevskii Equation (GPE) solver. It can solve GPE in 1D, 2D and 3D and has the ability to run on both CPU and GPU. It has been developed to study turbulence in quantum systems, specifically in atomic Bose-Einstein condensates, and can be used to study different quantities, such as the varied spectra associated with quantum turbulence.			arxiv	['Sachin Singh Rawat', 'Mahendra Kumar Verma', 'Pankaj Kumar Mishra']	398.0
783	naplib-python: Neural Acoustic Data Processing and Analysis Tools in Python	Gavin Mischler	2023-04-04 13:56:32	http://arxiv.org/abs/2304.01799v1	Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.			arxiv	['Vinay Raghavan', 'Menoua Keshishian', 'Nima Mesgarani']	399.0
784	pgmpy: A Python Toolkit for Bayesian Networks	Ankur Ankan	2023-04-17 22:17:53	http://arxiv.org/abs/2304.08639v1	Bayesian Networks (BNs) are used in various fields for modeling, prediction, and decision making. pgmpy is a python package that provides a collection of algorithms and tools to work with BNs and related models. It implements algorithms for structure learning, parameter estimation, approximate and exact inference, causal inference, and simulations. These implementations focus on modularity and easy extensibility to allow users to quickly modify/add to existing algorithms, or to implement new algorithms for different use cases. pgmpy is released under the MIT License; the source code is available at: https://github.com/pgmpy/pgmpy, and the documentation at: https://pgmpy.org.			arxiv	['Johannes Textor']	400.0
785	ganX -- generate artificially new XRF a python library to generate MA-XRF raw data out of RGB images	Alessandro Bombini	2023-04-27 10:40:06	http://arxiv.org/abs/2304.14078v1	In this paper we present the first version of ganX -- generate artificially new XRF, a Python library to generate X-ray fluorescence Macro maps (MA-XRF) from a coloured RGB image. To do that, a Monte Carlo method is used, where each MA-XRF pixel signal is sampled out of an XRF signal probability function. Such probability function is computed using a database of couples (pigment characteristic XRF signal, RGB), by a weighted sum of such pigment XRF signal by proximity of the image RGB to the pigment characteristic RGB. The library is released to PyPi and the code is available open source on GitHub.			arxiv	[]	401.0
786	Point in polygon calculation using vector geometric methods with application to geospatial data	Eyram Schwinger	2023-06-07 10:25:17	http://arxiv.org/abs/2306.04316v1	In this work, we designed algorithms for the point in polygon problem based on the ray casting algorithm using equations from vector geometry. The algorithms were implemented using the python programming language. We tested the algorithm against the point in polygon algorithms used by the shapely (and by extension geopandas) library and the OpenCV library using points from the google Open Buildings project. Our algorithm in pure python performed much better than the shapely implementation. It also performed better than the OpenCV implementation when combined with the Numba optimization library. We also performed simulations to verify that our algorithm performance was of the order O(n).			arxiv	['Ralph Twum', 'Thomas Katsekpor', 'Gladys Schwinger']	402.0
787	Badgers: generating data quality deficits with Python	Julien Siebert	2023-07-10 10:34:12	http://arxiv.org/abs/2307.04468v1	Generating context specific data quality deficits is necessary to experimentally assess data quality of data-driven (artificial intelligence (AI) or machine learning (ML)) applications. In this paper we present badgers, an extensible open-source Python library to generate data quality deficits (outliers, imbalanced data, drift, etc.) for different modalities (tabular data, time-series, text, etc.). The documentation is accessible at https://fraunhofer-iese.github.io/badgers/ and the source code at https://github.com/Fraunhofer-IESE/badgers			arxiv	['Daniel Seifert', 'Patricia Kelbert', 'Michael Kl√§s', 'Adam Trendowicz']	403.0
788	LINFA: a Python library for variational inference with normalizing flow and annealing	Yu Wang	2023-07-10 16:21:05	http://arxiv.org/abs/2307.04675v2	Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA.			arxiv	['Emma R. Cobian', 'Jubilee Lee', 'Fang Liu', 'Jonathan D. Hauenstein', 'Daniele E. Schiavazzi']	404.0
789	LCE: An Augmented Combination of Bagging and Boosting in Python	Kevin Fauvel	2023-08-14 16:34:47	http://arxiv.org/abs/2308.07250v2	lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.			arxiv	['√âlisa Fromont', 'V√©ronique Masson', 'Philippe Faverdin', 'Alexandre Termier']	405.0
790	TurboGP: A flexible and advanced python based GP library	Lino Rodriguez-Coayahuitl	2023-08-31 21:50:23	http://arxiv.org/abs/2309.00149v1	We introduce TurboGP, a Genetic Programming (GP) library fully written in Python and specifically designed for machine learning tasks. TurboGP implements modern features not available in other GP implementations, such as island and cellular population schemes, different types of genetic operations (migration, protected crossovers), online learning, among other features. TurboGP's most distinctive characteristic is its native support for different types of GP nodes to allow different abstraction levels, this makes TurboGP particularly useful for processing a wide variety of data sources.			arxiv	['Alicia Morales-Reyes', 'Hugo Jair Escalante']	406.0
791	GrassmannTN: a Python package for Grassmann tensor network computations	Atis Yosprakob	2023-09-14 09:39:57	http://arxiv.org/abs/2309.07557v3	We present GrassmannTN, a Python package for the computation of the Grassmann tensor network. The package is built to assist in the numerical computation without the need to input the fermionic sign factor manually. It prioritizes coding readability by designing every tensor manipulating function around the tensor subscripts. The computation of the Grassmann tensor renormalization group and Grassmann isometries using GrassmannTN are given as the use case examples.			arxiv	[]	407.0
792	A Framework and a python-package for Real-time NMPC parameters settings	Mazen Alamir	2023-09-29 13:41:52	http://arxiv.org/abs/2309.17238v1	This paper presents a framework that enables a systematic and rational choice of NMPC design components such as control updating period, down-sampling period for prediction, control parameterization, prediction horizon's length, the maximum number of iterations as well as penalties on the terminal cost and the soft constraints. The rationale that underlines the design choices is based on real-time implementability, convergence and constraints satisfaction for a given computational device and a specific optimization algorithm. Moreover, a freely available associated Python-based implementation is also described with a fully developed illustrative example implementing a nonlinear MPC controller for a Planar Vertical Take-Off and Landing (PVTOL) aircraft under control saturation and state constraints.			arxiv	[]	408.0
793	NoMoPy: Noise Modeling in Python	Dylan Albrecht	2023-10-31 18:52:05	http://arxiv.org/abs/2311.00084v1	NoMoPy is a code for fitting, analyzing, and generating noise modeled as a hidden Markov model (HMM) or, more generally, factorial hidden Markov model (FHMM). This code, written in Python, implements approximate and exact expectation maximization (EM) algorithms for performing the parameter estimation process, model selection procedures via cross-validation, and parameter confidence region estimation. Here, we describe in detail the functionality implemented in NoMoPy and provide examples of its use and performance on example problems.			arxiv	['N. Tobias Jacobson']	409.0
794	libepa -- a C++/Python library for calculations of cross sections of ultraperipheral collisions	E. V. Zhemchugov	2023-11-02 16:01:15	http://arxiv.org/abs/2311.01353v1	The library provides a set of C++/Python functions for computing cross sections of ultraperipheral collisions of high energy particles under the equivalent photons approximation. Cross sections are represented through multiple integrals over the phase space. The integrals are calculated through recurrent application of algorithms for one dimensional integration. The paper contains an introduction to the theory of ultraperipheral collisions, discusses the library approach and provides a few examples of calculations.			arxiv	['S. I. Godunov', 'E. K. Karkaryan', 'V. A. Novikov', 'A. N. Rozanov', 'M. I. Vysotsky']	410.0
795	sQUlearn $\unicode{x2013}$ A Python Library for Quantum Machine Learning	David A. Kreplin	2023-11-15 14:22:53	http://arxiv.org/abs/2311.08990v1	sQUlearn introduces a user-friendly, NISQ-ready Python library for quantum machine learning (QML), designed for seamless integration with classical machine learning tools like scikit-learn. The library's dual-layer architecture serves both QML researchers and practitioners, enabling efficient prototyping, experimentation, and pipelining. sQUlearn provides a comprehensive toolset that includes both quantum kernel methods and quantum neural networks, along with features like customizable data encoding strategies, automated execution handling, and specialized kernel regularization techniques. By focusing on NISQ-compatibility and end-to-end automation, sQUlearn aims to bridge the gap between current quantum computing capabilities and practical machine learning applications.			arxiv	['Moritz Willmann', 'Jan Schnabel', 'Frederic Rapp', 'Marco Roth']	411.0
796	HofstadterTools: A Python package for analyzing the Hofstadter model	Bartholomew Andrews	2023-11-30 17:23:33	http://arxiv.org/abs/2311.18726v1	The Hofstadter model successfully describes the behavior of non-interacting quantum particles hopping on a lattice coupled to a gauge field, and hence is ubiquitous in many fields of research, including condensed matter, optical, and atomic physics. Motivated by this, we introduce HofstadterTools (https://hofstadter.tools), a Python package that can be used to analyze the energy spectrum of a generalized Hofstadter model, with any combination of hoppings on any regular Euclidean lattice. The package can be applied to compute key properties of the band structure, such as quantum geometry and topology, as well as plot Hofstadter butterflies and Wannier diagrams that are colored according to their Chern numbers.			arxiv	[]	412.0
797	nerblackbox: A High-level Library for Named Entity Recognition in Python	Felix Stollenwerk	2023-12-07 14:04:15	http://arxiv.org/abs/2312.04306v1	We present nerblackbox, a python library to facilitate the use of state-of-the-art transformer-based models for named entity recognition. It provides simple-to-use yet powerful methods to access data and models from a wide range of sources, for fully automated model training and evaluation as well as versatile model inference. While many technical challenges are solved and hidden from the user by default, nerblackbox also offers fine-grained control and a rich set of customizable features. It is thus targeted both at application-oriented developers as well as machine learning experts and researchers.			arxiv	[]	413.0
798	Developing Elementary Federated Learning Algorithms Leveraging the ChatGPT	Miroslav Popovic	2023-12-07 16:34:47	http://arxiv.org/abs/2312.04412v1	The Python Testbed for Federated Learning Algorithms is a simple Python FL framework easy to use by ML&AI developers who do not need to be professional programmers, and this paper shows that it is also amenable to emerging AI tools. In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.			arxiv	['Marko Popovic', 'Ivan Kastelan', 'Miodrag Djukic', 'Ilija Basicevic']	414.0
799	Augmenty: A Python Library for Structured Text Augmentation	Kenneth Enevoldsen	2023-12-09 10:24:59	http://arxiv.org/abs/2312.05520v1	Augmnety is a Python library for structured text augmentation. It is built on top of spaCy and allows for augmentation of both the text and its annotations. Augmenty provides a wide range of augmenters which can be combined in a flexible manner to create complex augmentation pipelines. It also includes a set of primitives that can be used to create custom augmenters such as word replacement augmenters. This functionality allows for augmentations within a range of applications such as named entity recognition (NER), part-of-speech tagging, and dependency parsing.			arxiv	[]	415.0
800	Anisotropy in the Cosmic Microwave Background at Degree Angular Scales: Python V Results	K. Coble	1999-02-15 06:39:04	http://arxiv.org/abs/astro-ph/9902195v2	Observations of the microwave sky using the Python telescope in its fifth season of operation at the Amundsen-Scott South Pole Station in Antarctica are presented. The system consists of a 0.75 m off-axis telescope instrumented with a HEMT amplifier-based radiometer having continuum sensitivity from 37-45 GHz in two frequency bands. With a 0.91 deg x 1.02 deg beam the instrument fully sampled 598 deg^2 of sky, including fields measured during the previous four seasons of Python observations. Interpreting the observed fluctuations as anisotropy in the cosmic microwave background, we place constraints on the angular power spectrum of fluctuations in eight multipole bands up to l ~ 260. The observed spectrum is consistent with both the COBE experiment and previous Python results. There is no significant contamination from known foregrounds. The results show a discernible rise in the angular power spectrum from large (l ~ 40) to small (l ~ 200) angular scales. The shape of the observed power spectrum is not a simple linear rise but has a sharply increasing slope starting at l ~ 150.			arxiv	['M. Dragovan', 'J. Kovac', 'N. W. Halverson', 'W. L. Holzapfel', 'L. Knox', 'S. Dodelson', 'K. Ganga', 'D. Alvarez', 'J. B. Peterson', 'G. Griffin', 'M. Newcomb', 'K. Miller', 'S. R. Platt', 'G. Novak']	416.0
801	Multi Scale Investigation of Surface Topography of Ball Python (Python Regius) Shed Skin in Comparison to Human skin	H. A. Abdel-Aal	2010-01-27 04:35:43	http://arxiv.org/abs/1001.4850v1	Constructing a surface that is an integral part of the function of tribosystems (deterministic surface) is an intriguing goal. Inspirations for such surfaces come from studying natural systems and deducing design rules. The major attraction is that natural systems, while functionally complex, are, in general, of optimized shape and performance. It is further believed that functional complexity of natural systems is what affords natural species to morph continuously to adapt with the operating environment. One bio-species that is of interest is the Ball Python. This is because such a species continuously slides against various surfaces, many of which are deemed tribologically hostile, without sustaining much damage. Much of the success of that species in adapting to its environment is attributed to surface design features. In that respect, studying these features and how do they contribute to the control of friction and wear is very attractive. This paper is a step in this direction. In this work we apply a multi scale surface characterization approach to study surface design features of the Python Regius. The focus is on those features that are typically used to assess the performance of high quality lubricating surfaces. To this end, topographical features are studied by SEM and through White Light Interferrometery (WLI). We probe the roughness of the surface on multi scale and as a function of location within the body. In addition we draw a comparison of these features to those of human skin.			arxiv	['M. El Mansori', 'S. Mezghani']	417.0
802	Python Regius (Ball Python) shed skin: Biomimetic analogue for function-targeted design of tribo-surfaces	H. A. Abdel-Aal	2010-07-26 11:07:34	http://arxiv.org/abs/1007.4419v1	A major concern in designing tribo-systems is to minimize friction, save energy, and to reduce wear. Satisfying these requirements depends on the integrity of the rubbing surface and its suitability to sliding conditions. As such, designers currently focus on constructing surfaces that are an integral part of the function of the tribo-system. Inspirations for such constructs come from studying natural systems and from implementing natural design rules. One species that may serve as an analogue for design is the Ball python. This is because such a creature while depending on legless locomotion when sliding against various surfaces, many of which are deemed tribologically hostile, doesn't sustain much damage. Resistance to damage in this case originates from surface design features. As such, studying these features and how do they contribute to the control of friction and wear is very attractive for design purposes. In this work we apply a multi scale surface characterization approach to study surface design features of the Python regius that are beneficial to design high quality lubricating surfaces (such as those obtained through plateau honing). To this end, we studied topographical features by SEM and through White Light Interferrometery (WLI). We further probe the roughness of the surface on multi scale and as a function of location within the body. The results are used to draw a comparison to metrological features of commercial cylinder liners obtained by plateau honing.			arxiv	['M. El Mansori', 'I. C. Gebeshuber']	418.0
803	PyCOOL - a Cosmological Object-Oriented Lattice code written in Python	Jani Sainio	2012-01-24 16:09:52	http://arxiv.org/abs/1201.5029v3	There are a number of different phenomena in the early universe that have to be studied numerically with lattice simulations. This paper presents a graphics processing unit (GPU) accelerated Python program called PyCOOL that solves the evolution of scalar fields in a lattice with very precise symplectic integrators. The program has been written with the intention to hit a sweet spot of speed, accuracy and user friendliness. This has been achieved by using the Python language with the PyCUDA interface to make a program that is easy to adapt to different scalar field models. In this paper we derive the symplectic dynamics that govern the evolution of the system and then present the implementation of the program in Python and PyCUDA. The functionality of the program is tested in a chaotic inflation preheating model, a single field oscillon case and in a supersymmetric curvaton model which leads to Q-ball production. We have also compared the performance of a consumer graphics card to a professional Tesla compute card in these simulations. We find that the program is not only accurate but also very fast. To further increase the usefulness of the program we have equipped it with numerous post-processing functions that provide useful information about the cosmological model. These include various spectra and statistics of the fields. The program can be additionally used to calculate the generated curvature perturbation. The program is publicly available under GNU General Public License at https://github.com/jtksai/PyCOOL . Some additional information can be found from http://www.physics.utu.fi/tiedostot/theory/particlecosmology/pycool/ .			arxiv	[]	419.0
804	PyR@TE: Renormalization Group Equations for General Gauge Theories	Florian Lyonnet	2013-09-26 19:48:00	http://arxiv.org/abs/1309.7030v1	"Although the two-loop renormalization group equations for a general gauge field theory have been known for quite some time, deriving them for specific models has often been difficult in practice. This is mainly due to the fact that, albeit straightforward, the involved calculations are quite long, tedious and prone to error. The present work is an attempt to facilitate the practical use of the renormalization group equations in model building. To that end, we have developed two completely independent sets of programs written in Python and Mathematica, respectively. The Mathematica scripts will be part of an upcoming release of SARAH 4. The present article describes the collection of Python routines that we dubbed PyR@TE which is an acronym for ""Python Renormalization group equations At Two-loop for Everyone"". In PyR@TE, once the user specifies the gauge group and the particle content of the model, the routines automatically generate the full two-loop renormalization group equations for all (dimensionless and dimensionful) parameters. The results can optionally be exported to Latex and Mathematica, or stored in a Python data structure for further processing by other programs. For ease of use, we have implemented an interactive mode for PyR@TE in form of an IPython Notebook. As a first application, we have generated with PyR@TE the renormalization group equations for several non-supersymmetric extensions of the Standard Model and found some discrepancies with the existing literature."			arxiv	['Ingo Schienbein', 'Florian Staub', 'Akin Wingerter']	420.0
805	SClib, a hack for straightforward embedded C functions in Python	Esteban Fuentes	2014-12-19 15:51:21	http://arxiv.org/abs/1412.6395v1	"We present SClib, a simple hack that allows easy and straightforward evaluation of C functions within Python code, boosting flexibility for better trade-off between computation power and feature availability, such as visualization and existing computation routines in SciPy. We also present two cases were SClib has been used. In the first set of applications we use SClib to write a port to Python of a Schr\""odinger equation solver that has been extensively used the literature, the resulting script presents a speed-up of about 150x with respect to the original one. A review of the situations where the speeded-up script has been used is presented. We also describe the solution to the related problem of solving a set of coupled Schr\""odinger-like equations where SClib is used to implement the speed-critical parts of the code. We argue that when using SClib within IPython we can use NumPy and Matplotlib for the manipulation and visualization of the solutions in an interactive environment with no performance compromise. The second case is an engineering application. We use SClib to evaluate the control and system derivatives in a feedback control loop for electrical motors. With this and the integration routines available in SciPy, we can run simulations of the control loop a la Simulink. The use of C code not only boosts the speed of the simulations, but also enables to test the exact same code that we use in the test rig to get experimental results. Again, integration with IPython gives us the flexibility to analyze and visualize the data."			arxiv	['Hector E. Martinez']	421.0
806	pyFRET: A Python Library for Single Molecule Fluorescence Data Analysis	Rebecca R. Murphy	2014-12-19 16:00:31	http://arxiv.org/abs/1412.6402v1	"Single molecule F\""orster resonance energy transfer (smFRET) is a powerful experimental technique for studying the properties of individual biological molecules in solution. However, as adoption of smFRET techniques becomes more widespread, the lack of available software, whether open source or commercial, for data analysis, is becoming a significant issue. Here, we present pyFRET, an open source Python package for the analysis of data from single-molecule fluorescence experiments from freely diffusing biomolecules. The package provides methods for the complete analysis of a smFRET dataset, from burst selection and denoising, through data visualisation and model fitting. We provide support for both continuous excitation and alternating laser excitation (ALEX) data analysis. pyFRET is available as a package downloadable from the Python Package Index (PyPI) under the open source three-clause BSD licence, together with links to extensive documentation and tutorials, including example usage and test data. Additional documentation including tutorials is hosted independently on ReadTheDocs. The code is available from the free hosting site Bitbucket. Through distribution of this software, we hope to lower the barrier for the adoption of smFRET experiments by other research groups and we encourage others to contribute modules for specific analysis needs."			arxiv	['Sophie E. Jackson', 'David Klenerman']	422.0
807	Clone and graft: Testing scientific applications as they are built	Bruno Turcksin	2015-08-28 14:42:09	http://arxiv.org/abs/1508.07231v1	This article describes our experience developing and maintaining automated tests for scientific applications. The main idea evolves around building on already existing tests by cloning and grafting. The idea is demonstrated on a minimal model problem written in Python.			arxiv	['Timo Heister', 'Wolfgang Bangerth']	423.0
808	ExoData: A python package to handle large exoplanet catalogue data	Ryan Varley	2015-10-09 16:52:58	http://arxiv.org/abs/1510.02738v2	Exoplanet science often involves using the system parameters of real exoplanets for tasks such as simulations, fitting routines, and target selection for proposals. Several exoplanet catalogues are already well established but often lack a version history and code friendly interfaces. Software that bridges the barrier between the catalogues and code enables users to improve the specific repeatability of results by facilitating the retrieval of exact system parameters used in an articles results along with unifying the equations and software used. As exoplanet science moves towards large data, gone are the days where researchers can recall the current population from memory. An interface able to query the population now becomes invaluable for target selection and population analysis. ExoData is a Python interface and exploratory analysis tool for the Open Exoplanet Catalogue. It allows the loading of exoplanet systems into Python as objects (Planet, Star, Binary etc) from which common orbital and system equations can be calculated and measured parameters retrieved. This allows researchers to use tested code of the common equations they require (with units) and provides a large science input catalogue of planets for easy plotting and use in research. Advanced querying of targets are possible using the database and Python programming language. ExoData is also able to parse spectral types and fill in missing parameters according to programmable specifications and equations. Examples of use cases are integration of equations into data reduction pipelines, selecting planets for observing proposals and as an input catalogue to large scale simulation and analysis of planets.			arxiv	[]	424.0
809	Towards Turkish ASR: Anatomy of a rule-based Turkish g2p	Duygu Altinok	2016-01-15 00:09:52	http://arxiv.org/abs/1601.03783v1	This paper describes the architecture and implementation of a rule-based grapheme to phoneme converter for Turkish. The system accepts surface form as input, outputs SAMPA mapping of the all parallel pronounciations according to the morphological analysis together with stress positions. The system has been implemented in Python			arxiv	[]	425.0
810	On rigidity of unit-bar frameworks	Jozsef Solymosi	2018-08-12 21:53:54	http://arxiv.org/abs/1808.04005v1	We show the existence of infinitesimally rigid bipartite unit-bar frameworks in $\mathbb{R}^d$. We also construct unit-bar frameworks with girth up to 12 that are infinitesimally rigid in the plane. This answers problems proposed by Maehara.			arxiv	['Ethan White']	426.0
811	Asynchronous Execution of Python Code on Task Based Runtime Systems	R. Tohid	2018-10-17 14:50:42	http://arxiv.org/abs/1810.07591v2	Despite advancements in the areas of parallel and distributed computing, the complexity of programming on High Performance Computing (HPC) resources has deterred many domain experts, especially in the areas of machine learning and artificial intelligence (AI), from utilizing performance benefits of such systems. Researchers and scientists favor high-productivity languages to avoid the inconvenience of programming in low-level languages and costs of acquiring the necessary skills required for programming at this level. In recent years, Python, with the support of linear algebra libraries like NumPy, has gained popularity despite facing limitations which prevent this code from distributed runs. Here we present a solution which maintains both high level programming abstractions as well as parallel and distributed efficiency. Phylanx, is an asynchronous array processing toolkit which transforms Python and NumPy operations into code which can be executed in parallel on HPC resources by mapping Python and NumPy functions and variables into a dependency tree executed by HPX, a general purpose, parallel, task-based runtime system written in C++. Phylanx additionally provides introspection and visualization capabilities for debugging and performance analysis. We have tested the foundations of our approach by comparing our implementation of widely used machine learning algorithms to accepted NumPy standards.			arxiv	['Bibek Wagle', 'Shahrzad Shirzad', 'Patrick Diehl', 'Adrian Serio', 'Alireza Kheirkhahan', 'Parsa Amini', 'Katy Williams', 'Kate Isaacs', 'Kevin Huck', 'Steven Brandt', 'Hartmut Kaiser']	427.0
812	An Empirical Analysis of the Python Package Index (PyPI)	Ethan Bommarito	2019-07-25 14:11:32	http://arxiv.org/abs/1907.11073v2	In this research, we provide a comprehensive empirical summary of the Python Package Repository, PyPI, including both package metadata and source code covering 178,592 packages, 1,745,744 releases, 76,997 contributors, and 156,816,750 import statements. We provide counts and trends for packages, releases, dependencies, category classifications, licenses, and package imports, as well as authors, maintainers, and organizations. As one of the largest and oldest software repositories as of publication, PyPI provides insight not just into the Python ecosystem today, but also trends in software development and licensing more broadly over time. Within PyPI, we find that the growth of the repository has been robust under all measures, with a compound annual growth rate of 47% for active packages, 39% for new authors, and 61% for new import statements over the last 15 years. As with many similar social systems, we find a number of highly right-skewed distributions, including the distribution of releases per package, packages and releases per author, imports per package, and size per package and release. However, we also find that most packages are contributed by single individuals, not multiple individuals or organizations. The data, methods, and calculations herein provide an anchor for public discourse on PyPI and serve as a foundation for future research on the Python software ecosystem.			arxiv	['Michael Bommarito']	428.0
813	Simulation of Shive wave machines using GNU Octave, Python and C++ / Simulation von Wellenmaschinen mit GNU Octave, Python und C++	Tilman Kuepper	2017-10-17 13:35:47	http://arxiv.org/abs/1711.00717v1	"In ""Shive wave machines - Wave propagation, dispersion, reflection, simulation"" (arXiv:1503.02088) technical details of Shive wave machines are discussed. Wave propagation on these machines is simulated using the commercial numerical computing environment MATLAB. The MATLAB functions are compatible with GNU Octave, a free MATLAB alternative. The lower execution speed compared to MATLAB can be somewhat compensated by converting the underlying differential equations to matrix form. With Python or C++, these and similar simulations can also be performed easily. Suitable vector and matrix libraries, ODE solvers and plotting libraries for these programming languages are available free of charge on the internet. ----- In ""Die Wellenmaschine - Grundlagen der Wellenausbreitung, Dispersion, Reflexion, Simulation"" (arXiv:1503.02088) werden die in Schule und Hochschule verbreiteten Wellenmaschinen n\""aher betrachtet und die Wellenausbreitung auf solchen Maschinen mithilfe von MATLAB simuliert. Die kommerzielle Software MATLAB ist freilich nicht \""uberall verf\""ugbar, sodass sich die Frage nach freien Alternativen stellt. Der MATLAB-Quelltext im genannten Artikel kann zum Beispiel mit GNU Octave ausgef\""uhrt werden. Die im Vergleich zu MATLAB geringere Ausf\""uhrungsgeschwindigkeit l\""asst sich durch Umformung des zu l\""osenden Differentialgleichungssystems in Matrixschreibweise zumindest teilweise kompensieren. Aber auch mit klassischen Programmiersprachen wie Python oder C++ lassen sich diese und \""ahnliche Simulationen leicht durchf\""uhren. Passende Bibliotheken zur Arbeit mit Vektoren und Matrizen, zum L\""osen von Differentialgleichungen und zur animierten Darstellung der Simulationsergebnisse werden im Artikel vorgestellt."			arxiv	[]	429.0
814	An Open Source Power System Simulator in Python for Efficient Prototyping of WAMPAC Applications	Hallvar Haugdal	2021-01-08 10:03:07	http://arxiv.org/abs/2101.02937v1	An open source software package for performing dynamic RMS simulation of small to medium-sized power systems is presented, written entirely in the Python programming language. The main objective is to facilitate fast prototyping of new wide area monitoring, control and protection applications for the future power system by enabling seamless integration with other tools available for Python in the open source community, e.g. for signal processing, artificial intelligence, communication protocols etc. The focus is thus transparency and expandability rather than computational efficiency and performance. The main purpose of this paper, besides presenting the code and some results, is to share interesting experiences with the power system community, and thus stimulate wider use and further development. Two interesting conclusions at the current stage of development are as follows: First, the simulation code is fast enough to emulate real-time simulation for small and medium-size grids with a time step of 5 ms, and allows for interactive feedback from the user during the simulation. Second, the simulation code can be uploaded to an online Python interpreter, edited, run and shared with anyone with a compatible internet browser. Based on this, we believe that the presented simulation code could be a valuable tool, both for researchers in early stages of prototyping real-time applications, and in the educational setting, for students developing intuition for concepts and phenomena through real-time interaction with a running power system model.			arxiv	['Kjetil Uhlen']	430.0
815	Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python	Amir M. Mir	2021-01-12 13:32:53	http://arxiv.org/abs/2101.04470v3	Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing codebases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type inference based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present Type4Py, a deep similarity learning-based hierarchical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neighbor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that Type4Py achieves an MRR of 77.1%, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and TypeWriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Studio Code extension, which uses Type4Py to provide ML-based type auto-completion for Python.			arxiv	['Evaldas Latoskinas', 'Sebastian Proksch', 'Georgios Gousios']	431.0
816	pandapower - an Open Source Python Tool for Convenient Modeling, Analysis and Optimization of Electric Power Systems	Leon Thurner	2017-09-20 07:14:52	http://arxiv.org/abs/1709.06743v3	pandapower is a Python based, BSD-licensed power system analysis tool aimed at automation of static and quasi-static analysis and optimization of balanced power systems. It provides power flow, optimal power flow, state estimation, topological graph searches and short circuit calculations according to IEC 60909. pandapower includes a Newton-Raphson power flow solver formerly based on PYPOWER, which has been accelerated with just-in-time compilation. Additional enhancements to the solver include the capability to model constant current loads, grids with multiple reference nodes and a connectivity check. The pandapower network model is based on electric elements, such as lines, two and three-winding transformers or ideal switches. All elements can be defined with nameplate parameters and are internally processed with equivalent circuit models, which have been validated against industry standard software tools. The tabular data structure used to define networks is based on the Python library pandas, which allows comfortable handling of input and output parameters. The implementation in Python makes pandapower easy to use and allows comfortable extension with third-party libraries. pandapower has been successfully applied in several grid studies as well as for educational purposes. A comprehensive, publicly available case-study demonstrates a possible application of pandapower in an automated time series calculation.			arxiv	['Alexander Scheidler', 'Florian Sch√§fer', 'Jan-Hendrik Menke', 'Julian Dollichon', 'Friederike Meier', 'Steffen Meinecke', 'Martin Braun']	432.0
817	nbodykit: an open-source, massively parallel toolkit for large-scale structure	Nick Hand	2017-12-15 21:00:01	http://arxiv.org/abs/1712.05834v1	We present nbodykit, an open-source, massively parallel Python toolkit for analyzing large-scale structure (LSS) data. Using Python bindings of the Message Passing Interface (MPI), we provide parallel implementations of many commonly used algorithms in LSS. nbodykit is both an interactive and scalable piece of scientific software, performing well in a supercomputing environment while still taking advantage of the interactive tools provided by the Python ecosystem. Existing functionality includes estimators of the power spectrum, 2 and 3-point correlation functions, a Friends-of-Friends grouping algorithm, mock catalog creation via the halo occupation distribution technique, and approximate N-body simulations via the FastPM scheme. The package also provides a set of distributed data containers, insulated from the algorithms themselves, that enable nbodykit to provide a unified treatment of both simulation and observational data sets. nbodykit can be easily deployed in a high performance computing environment, overcoming some of the traditional difficulties of using Python on supercomputers. We provide performance benchmarks illustrating the scalability of the software. The modular, component-based approach of nbodykit allows researchers to easily build complex applications using its tools. The package is extensively documented at http://nbodykit.readthedocs.io, which also includes an interactive set of example recipes for new users to explore. As open-source software, we hope nbodykit provides a common framework for the community to use and develop in confronting the analysis challenges of future LSS surveys.			arxiv	['Yu Feng', 'Florian Beutler', 'Yin Li', 'Chirag Modi', 'Uros Seljak', 'Zachary Slepian']	433.0
818	SuSi: Supervised Self-Organizing Maps for Regression and Classification in Python	Felix M. Riese	2019-03-26 18:52:45	http://arxiv.org/abs/1903.11114v3	In many research fields, the sizes of the existing datasets vary widely. Hence, there is a need for machine learning techniques which are well-suited for these different datasets. One possible technique is the self-organizing map (SOM), a type of artificial neural network which is, so far, weakly represented in the field of machine learning. The SOM's unique characteristic is the neighborhood relationship of the output neurons. This relationship improves the ability of generalization on small datasets. SOMs are mostly applied in unsupervised learning and few studies focus on using SOMs as supervised learning approach. Furthermore, no appropriate SOM package is available with respect to machine learning standards and in the widely used programming language Python. In this paper, we introduce the freely available Supervised Self-organizing maps (SuSi) Python package which performs supervised regression and classification. The implementation of SuSi is described with respect to the underlying mathematics. Then, we present first evaluations of the SOM for regression and classification datasets from two different domains of geospatial image analysis. Despite the early stage of its development, the SuSi framework performs well and is characterized by only small performance differences between the training and the test datasets. A comparison of the SuSi framework with existing Python and R packages demonstrates the importance of the SuSi framework. In future work, the SuSi framework will be extended, optimized and upgraded e.g. with tools to better understand and visualize the input data as well as the handling of missing and incomplete data.			arxiv	['Sina Keller']	434.0
819	Parsl: Pervasive Parallel Programming in Python	Yadu Babuji	2019-05-06 17:19:10	http://arxiv.org/abs/1905.02158v2	High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore's law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250 000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.			arxiv	['Anna Woodard', 'Zhuozhao Li', 'Daniel S. Katz', 'Ben Clifford', 'Rohan Kumar', 'Lukasz Lacinski', 'Ryan Chard', 'Justin M. Wozniak', 'Ian Foster', 'Michael Wilde', 'Kyle Chard']	435.0
820	PyFLOSIC: Python-based Fermi-L√∂wdin orbital self-interaction correction	Sebastian Schwalbe	2019-05-07 15:01:31	http://arxiv.org/abs/1905.02631v4	"We present PyFLOSIC, an open-source, general-purpose Python implementation of the Fermi-L\""owdin orbital self-interaction correction (FLO-SIC), which is based on the Python simulation of chemistry frame-work (PySCF) electronic structure and quantum chemistry code. Thanks to PySCF, PyFLOSIC can be used with any kind of Gaussian-type basis set, various kinds of radial and angular quadrature grids, and all exchange-correlation functionals within the local density approximation (LDA), generalized-gradient approximation (GGA), and meta-GGA provided in the Libxc and XCFun libraries. A central aspect of FLO-SIC are Fermi-orbital descriptors, which are used to estimate the self-interaction correction. Importantly, they can be initialized automatically within PyFLOSIC and optimized with an interface to the atomic simulation environment, a Python library which provides a variety of powerful gradient-based algorithms for geometry optimization. Although PyFLOSIC has already facilitated applications of FLO-SIC to chemical studies, it offers an excellent starting point for further developments in FLO-SIC approaches, thanks to its use of a high-level programming language and pronounced modularity."			arxiv	['Lenz Fiedler', 'Jakob Kraus', 'Jens Kortus', 'Kai Trepte', 'Susi Lehtola']	436.0
821	Enhancing Python Compiler Error Messages via Stack Overflow	Emillie Thiselton	2019-06-27 06:41:04	http://arxiv.org/abs/1906.11456v1	Background: Compilers tend to produce cryptic and uninformative error messages, leaving programmers confused and requiring them to spend precious time to resolve the underlying error. To find help, programmers often take to online question-and-answer forums such as Stack Overflow to start discussion threads about the errors they encountered. Aims: We conjecture that information from Stack Overflow threads which discuss compiler errors can be automatically collected and repackaged to provide programmers with enhanced compiler error messages, thus saving programmers' time and energy. Method: We present Pycee, a plugin integrated with the popular Sublime Text IDE to provide enhanced compiler error messages for the Python programming language. Pycee automatically queries Stack Overflow to provide customised and summarised information within the IDE. We evaluated two Pycee variants through a think-aloud user study during which 16 programmers completed Python programming tasks while using Pycee. Results: The majority of participants agreed that Pycee was helpful while completing the study tasks. When compared to a baseline relying on the official Python documentation to enhance compiler error messages, participants generally preferred Pycee in terms of helpfulness, citing concrete suggestions for fixes and example code as major benefits. Conclusions: Our results confirm that data from online sources such as Stack Overflow can be successfully used to automatically enhance compiler error messages. Our work opens up venues for future work to further enhance compiler error messages as well as to automatically reuse content from Stack Overflow for other aspects of programming.			arxiv	['Christoph Treude']	437.0
822	PySPH: a Python-based framework for smoothed particle hydrodynamics	Prabhu Ramachandran	2019-09-10 14:11:27	http://arxiv.org/abs/1909.04504v3	PySPH is an open-source, Python-based, framework for particle methods in general and Smoothed Particle Hydrodynamics (SPH) in particular. PySPH allows a user to define a complete SPH simulation using pure Python. High-performance code is generated from this high-level Python code and executed on either multiple cores, or on GPUs, seamlessly. It also supports distributed execution using MPI. PySPH supports a wide variety of SPH schemes and formulations. These include, incompressible and compressible fluid flow, elastic dynamics, rigid body dynamics, shallow water equations, and other problems. PySPH supports a variety of boundary conditions including mirror, periodic, solid wall, and inlet/outlet boundary conditions. The package is written to facilitate reuse and reproducibility. This paper discusses the overall design of PySPH and demonstrates many of its features. Several example results are shown to demonstrate the range of features that PySPH provides.			arxiv	['Aditya Bhosale', 'Kunal Puri', 'Pawan Negi', 'Abhinav Muta', 'A Dinesh', 'Dileep Menon', 'Rahul Govind', 'Suraj Sanka', 'Amal S Sebastian', 'Ananyo Sen', 'Rohan Kaushik', 'Anshuman Kumar', 'Vikas Kurapati', 'Mrinalgouda Patil', 'Deep Tavker', 'Pankaj Pandey', 'Chandrashekhar Kaushik', 'Arkopal Dutt', 'Arpit Agarwal']	438.0
823	A survey on the topological entropy of cubic polynomials	Noah Cockram	2020-06-24 15:07:55	http://arxiv.org/abs/2006.13795v1	In this paper we discuss two different existing algorithms for computing topological entropy and we perform one of them in order to compute the isentropes for cubic polynomials.			arxiv	['Ana Rodrigues']	439.0
824	Development of a computational software in Python, used to study the materials resistance in beams	Julian A Rivera S	2020-09-20 14:56:07	http://arxiv.org/abs/2009.09448v2	In this research, we do a software writing in Python to calculate the efforts, bending moments and deformations in beams of different materials. This computational tool, that we developed, is of great help in area of computational physical, more exactly in resistance of materials, which serves as support for researchers and teachers especially in physics and civil engineering, of the part of statics, who wish to carry out the modeling of the functions, involved in the calculation of resistance in beams in a practical and simple way, using the software presented in this article. In order to carry out this software, we are going to use the following methods: the double-integration method and the conjugate-beam method, which will serve as the basis of calculation to find the mathematical expressions involved in the analysis of resistance in beams, then we will perform the implementation of the aforementioned methods, using Python as the programming language. As a final step in this project, the graphical interface of said calculation tool will be made using the Python 3.0 Tkinter library. In this work, we show the results of the graphs of the stress profiles, bending moments and deformations, for the case of different types of beams, load and force distributions applied to them. Where we were also able to conclude that a calculation software was successfully built, dedicated to the analysis of efforts and deformations in beams made of different materials.			arxiv	['Alex F Estupi√±√°n L']	440.0
825	Scipp: Scientific data handling with labeled multi-dimensional arrays for C++ and Python	Simon Heybrock	2020-10-01 08:59:03	http://arxiv.org/abs/2010.00257v1	Scipp is heavily inspired by the Python library xarray. It enriches raw NumPy-like multi-dimensional arrays of data by adding named dimensions and associated coordinates. Multiple arrays are combined into datasets. On top of this, scipp introduces (i) implicit handling of physical units, (ii) implicit propagation of uncertainties, (iii) support for histograms, i.e., bin-edge coordinate axes, which exceed the data's dimension extent by one, and (iv) support for event data. In conjunction these features enable a more natural and more concise user experience. The combination of named dimensions, coordinates, and units helps to drastically reduce the risk for programming errors. The core of scipp is written in C++ to open opportunities for performance improvements that a Python-based solution would not allow for. On top of the C++ core, scipp's Python components provide functionality for plotting and content representations, e.g., for use in Jupyter Notebooks. While none of scipp's concepts in isolation is novel per-se, we are not aware of any project combining all of these aspects in a single coherent software package.			arxiv	['Owen Arnold', 'Igor Gudich', 'Daniel Nixon', 'Neil Vaytet']	441.0
826	Pythonic Black-box Electronic Structure Tool (PyBEST). An open-source Python platform for electronic structure calculations at the interface between chemistry and physics	Katharina Boguslawski	2020-10-12 07:10:23	http://arxiv.org/abs/2010.05485v1	Pythonic Black-box Electronic Structure Tool (PyBEST) represents a fully-fledged modern electronic structure software package developed at Nicolaus Copernicus University in Toru\'n. The package provides an efficient and reliable platform for electronic structure calculations at the interface between chemistry and physics using unique electronic structure methods, analysis tools, and visualization. Examples are the (orbital-optimized) pCCD-based models for ground- and excited-states electronic structure calculations as well as the quantum entanglement analysis framework based on the single-orbital entropy and orbital-pair mutual information. PyBEST is written primarily in the Python3 programming language with additional parts written in C++, which are interfaced using Pybind11, a lightweight header-only library. By construction, PyBEST is easy to use, to code, and to interface with other software packages. Moreover, its modularity allows us to conveniently host additional Python packages and software libraries in future releases to enhance its performance. The electronic structure methods available in PyBEST are tested for the half-filled 1-D model Hamiltonian. The capability of PyBEST to perform large-scale electronic structure calculations is demonstrated for the model vitamin B12 compound. The investigated molecule is composed of 190 electrons and 777 orbitals for which an orbital optimization within pCCD and an orbital entanglement and correlation analysis are performed for the first time.			arxiv	['Aleksandra Leszczyk', 'Artur Nowak', 'Filip Brzƒôk', 'Piotr Szymon ≈ªuchowski', 'Dariusz Kƒôdziera', 'Pawe≈Ç Tecmer']	442.0
827	The NRGTEN Python package: an extensible toolkit for coarse-grained normal mode analysis of proteins, nucleic acids, small molecules and their complexes	Olivier Mailhot	2020-12-24 01:37:46	http://arxiv.org/abs/2012.13062v1	Summary: Coarse-grained normal mode analysis (NMA) is a fast computational technique to study the dynamics of biomolecules. Here we present the Najmanovich Research Group Toolkit for Elastic Networks (NRGTEN). NRGTEN is a Python toolkit that implements four different NMA models in addition to popular and novel metrics to benchmark and measure properties from these models. Furthermore, the toolkit is available as a public Python package and is easily extensible for the development or implementation of additional NMA models. The inclusion of the ENCoM model (Elastic Network Contact Model) developed in our group within NRGTEN is noteworthy, owing to its account for the specific chemical nature of atomic interactions. This makes possible some unique predictions of the effect of mutations, such as on stability (via changes in vibrational entropy differences), on the transition probability between different conformational states or on the flexibility profile of the whole macromolecule/complex (to study allostery and signalling). In addition, all NMA models can be used to generate conformational ensembles from a starting structure to aid in protein-protein, protein-ligand or other docking studies among applications. NRGTEN is freely available via a public Python package which can be easily installed on any modern machine and includes a detailed user guide hosted online. Availability and implementation: https://github.com/gregorpatof/nrgten_package/ Contact: rafael.najmanovich@umontreal.ca			arxiv	['Rafael Najmanovich']	443.0
828	DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python	Jinglin Peng	2021-04-02 01:24:10	http://arxiv.org/abs/2104.00841v2	Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.			arxiv	['Weiyuan Wu', 'Brandon Lockhart', 'Song Bian', 'Jing Nathan Yan', 'Linghao Xu', 'Zhixuan Chi', 'Jeffrey Rzeszotarski', 'Jiannan Wang']	444.0
829	A systematic review of Python packages for time series analysis	Julien Siebert	2021-04-15 12:09:54	http://arxiv.org/abs/2104.07406v2	This paper presents a systematic review of Python packages with a focus on time series analysis. The objective is to provide (1) an overview of the different time series analysis tasks and preprocessing methods implemented, and (2) an overview of the development characteristics of the packages (e.g., documentation, dependencies, and community size). This review is based on a search of literature databases as well as GitHub repositories. Following the filtering process, 40 packages were analyzed. We classified the packages according to the analysis tasks implemented, the methods related to data preparation, and the means for evaluating the results produced (methods and access to evaluation data). We also reviewed documentation aspects, the licenses, the size of the packages' community, and the dependencies used. Among other things, our results show that forecasting is by far the most frequently implemented task, that half of the packages provide access to real datasets or allow generating synthetic data, and that many packages depend on a few libraries (the most used ones being numpy, scipy and pandas). We hope that this review can help practitioners and researchers navigate the space of Python packages dedicated to time series analysis. We will provide an updated list of the reviewed packages online at https://siebert-julien.github.io/time-series-analysis-python/.			arxiv	['Janek Gro√ü', 'Christof Schroth']	445.0
830	Atomic Simulation Recipes -- a Python framework and library for automated workflows	Morten Gjerding	2021-04-27 19:03:29	http://arxiv.org/abs/2104.13431v1	The Atomic Simulation Recipes (ASR) is an open source Python framework for working with atomistic materials simulations in an efficient and sustainable way that is ideally suited for high-throughput projects. Central to ASR is the concept of a Recipe: a high-level Python script that performs a well defined simulation task robustly and accurately while keeping track of the data provenance. The ASR leverages the functionality of the Atomic Simulation Environment (ASE) to interface with external simulation codes and attain a high abstraction level. We provide a library of Recipes for common simulation tasks employing density functional theory and many-body perturbation schemes. These Recipes utilize the GPAW electronic structure code, but may be adapted to other simulation codes with an ASE interface. Being independent objects with automatic data provenance control, Recipes can be freely combined through Python scripting giving maximal freedom for users to build advanced workflows. ASR also implements a command line interface that can be used to run Recipes and inspect results. The ASR Migration module helps users maintain their data while the Database and App modules makes it possible to create local databases and present them as customized web pages.			arxiv	['Thorbj√∏rn Skovhus', 'Asbj√∏rn Rasmussen', 'Fabian Bertoldo', 'Ask Hjorth Larsen', 'Jens J√∏rgen Mortensen', 'Kristian Sommer Thygesen']	446.0
831	Scqubits: a Python package for superconducting qubits	Peter Groszkowski	2021-07-18 22:37:00	http://arxiv.org/abs/2107.08552v2	$\textbf{scqubits}$ is an open-source Python package for simulating and analyzing superconducting circuits. It provides convenient routines to obtain energy spectra of common superconducting qubits, such as the transmon, fluxonium, flux, cos(2$\phi$) and the 0-$\pi$ qubit. $\textbf{scqubits}$ also features a number of options for visualizing the computed spectral data, including plots of energy levels as a function of external parameters, display of matrix elements of various operators as well as means to easily plot qubit wavefunctions. Many of these tools are not limited to single qubits, but extend to composite Hilbert spaces consisting of coupled superconducting qubits and harmonic (or weakly anharmonic) modes. The library provides an extensive suite of methods for estimating qubit coherence times due to a variety of commonly considered noise channels. While all functionality of $\textbf{scqubits}$ can be accessed programatically, the package also implements GUI-like widgets that, with a few clicks can help users both create relevant Python objects, as well as explore their properties through various plots. When applicable, the library harnesses the computing power of multiple cores via multiprocessing. $\textbf{scqubits}$ further exposes a direct interface to the Quantum Toolbox in Python (QuTiP) package, allowing the user to efficiently leverage QuTiP's proven capabilities for simulating time evolution.			arxiv	['Jens Koch']	447.0
832	A Large-Scale Security-Oriented Static Analysis of Python Packages in PyPI	Jukka Ruohonen	2021-07-27 09:57:25	http://arxiv.org/abs/2107.12699v2	Different security issues are a common problem for open source packages archived to and delivered through software ecosystems. These often manifest themselves as software weaknesses that may lead to concrete software vulnerabilities. This paper examines various security issues in Python packages with static analysis. The dataset is based on a snapshot of all packages stored to the Python Package Index (PyPI). In total, over 197 thousand packages and over 749 thousand security issues are covered. Even under the constraints imposed by static analysis, (a) the results indicate prevalence of security issues; at least one issue is present for about 46% of the Python packages. In terms of the issue types, (b) exception handling and different code injections have been the most common issues. The subprocess module stands out in this regard. Reflecting the generally small size of the packages, (c) software size metrics do not predict well the amount of issues revealed through static analysis. With these results and the accompanying discussion, the paper contributes to the field of large-scale empirical studies for better understanding security problems in software ecosystems.			arxiv	['Kalle Hjerppe', 'Kalle Rindell']	448.0
833	The Python Sky Model 3 software	Andrea Zonca	2021-08-01 03:08:24	http://arxiv.org/abs/2108.01444v1	The Python Sky Model (PySM) is a Python package used by Cosmic Microwave Background (CMB) experiments to simulate maps, in HEALPix pixelization, of the various diffuse astrophysical components of Galactic emission relevant at CMB frequencies (i.e. dust, synchrotron, free-free and Anomalous Microwave Emission), as well as the CMB itself. These maps may be integrated over a given instrument bandpass and smoothed with a given instrument beam. PySM 2, released in 2016, has become the de-facto standard for simulating Galactic emission, for example it is used by CMB-S4, Simons Observatory, LiteBird, PICO, CLASS, POLARBEAR and other CMB experiments, as shown by the 80+ citations of the PySM 2 publication. As the resolution of upcoming experiments increases, the PySM 2 software has started to show some limitations, the solution to these issues was to reimplement PySM from scratch focusing on these features: reimplement all the models with the numba Just-In-Time compiler for Python to reduce memory overhead and optimize performance; use MPI through mpi4py to coordinate execution of PySM 3 across multiple nodes and rely on libsharp, for distributed spherical harmonic transforms; employ the data utilities infrastructure provided by astropy to download the input templates and cache them when requested. At this stage we strive to maintain full compatibility with PySM 2, therefore we implement the exact same astrophysical emission models with the same naming scheme. In the extensive test suite we compare the output of each PySM 3 model with the results obtained by PySM 2.			arxiv	['Ben Thorne', 'Nicoletta Krachmalnicoff', 'Julian Borrill']	449.0
834	A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python	Amirreza Bagheri	2021-08-04 13:14:25	http://arxiv.org/abs/2108.02044v1	In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, like text representation, to automatically derive the necessary features from the source code. However, the suitability and comparison of different text representation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three popular text representation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a data mining approach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all the text representation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy(93.8%) in predicting Python source code vulnerabilities.			arxiv	['P√©ter Heged≈±s']	450.0
835	CharmFL: A Fault Localization Tool for Python	Qusay Idrees Sarhan	2021-08-27 16:26:41	http://arxiv.org/abs/2108.12377v1	"Fault localization is one of the most time-consuming and error-prone parts of software debugging. There are several tools for helping developers in the fault localization process, however, they mostly target programs written in Java and C/C++ programming languages. While these tools are splendid on their own, we must not look over the fact that Python is a popular programming language, and still there are a lack of easy-to-use and handy fault localization tools for Python developers. In this paper, we present a tool called ""CharmFL"" for software fault localization as a plug-in for PyCharm IDE. The tool employs Spectrum-based fault localization (SBFL) to help Python developers automatically analyze their programs and generate useful data at run-time to be used, then to produce a ranked list of potentially faulty program elements (i.e., statements, functions, and classes). Thus, our proposed tool supports different code coverage types with the possibility to investigate these types in a hierarchical approach. The applicability of our tool has been presented by using a set of experimental use cases. The results show that our tool could help developers to efficiently find the locations of different types of faults in their programs."			arxiv	['Attila Szatmari', 'Rajmond Toth', 'Arpad Beszedes']	451.0
836	Improving a High Productivity Data Analytics Chapel Framework	Prashanth Pai	2021-11-19 17:26:50	http://arxiv.org/abs/2111.10333v1	Most state of the art exploratory data analysis frameworks fall into one of the two extremes: they either focus on the high-performance computational, or on the interactive and open-ended aspects of the analysis. Arkouda is a framework that attempts to integrate the interactive approach with the high performance computation by using a novel client-server architecture, with a Python interpreter on the client side for the interactions with the scientist and a Chapel server for performing the demanding high-performance computations. The Arkouda Python interpreter overloads the Python operators and transforms them into messages to the Chapel server that performs the actual computation. In this paper, we are proposing several client-side optimization techniques for the Arkouda framework that maintain the interactive nature of the Arkouda framework, but at the same time significantly improve the performance of the programs that perform operations running on the high-performance Chapel server. We do this by intercepting the Python operations in the interpreter, and delaying their execution until the user requires the data, or we fill out the instruction buffer. We implement caching and reuse of the Arkouda arrays on the Chapel server side (thus saving on the allocation, initialization and deallocation of the Chapel arrays), tracking and caching the results of function calls on the Arkouda arrays (thus avoiding repeated computation) and reusing the results of array operations by performing common subexpression elimination. We evaluate our approach on several Arkouda benchmarks and a large collection of real-world and synthetic data inputs and show significant performance improvements between 20% and 120% across the board, while fully maintaining the interactive nature of the Arkouda framework.			arxiv	['Andrej Jakovljeviƒá', 'Zoran Budimliƒá', 'Costin Iancu']	452.0
837	Terra: Imperative-Symbolic Co-Execution of Imperative Deep Learning Programs	Taebum Kim	2022-01-23 09:04:48	http://arxiv.org/abs/2201.09210v1	Imperative programming allows users to implement their deep neural networks (DNNs) easily and has become an essential part of recent deep learning (DL) frameworks. Recently, several systems have been proposed to combine the usability of imperative programming with the optimized performance of symbolic graph execution. Such systems convert imperative Python DL programs to optimized symbolic graphs and execute them. However, they cannot fully support the usability of imperative programming. For example, if an imperative DL program contains a Python feature with no corresponding symbolic representation (e.g., third-party library calls or unsupported dynamic control flows) they fail to execute the program. To overcome this limitation, we propose Terra, an imperative-symbolic co-execution system that can handle any imperative DL programs while achieving the optimized performance of symbolic graph execution. To achieve this, Terra builds a symbolic graph by decoupling DL operations from Python features. Then, Terra conducts the imperative execution to support all Python features, while delegating the decoupled operations to the symbolic execution. We evaluated the performance improvement and coverage of Terra with ten imperative DL programs for several DNN architectures. The results show that Terra can speed up the execution of all ten imperative DL programs, whereas AutoGraph, one of the state-of-the-art systems, fails to execute five of them.			arxiv	['Eunji Jeong', 'Geon-Woo Kim', 'Yunmo Koo', 'Sehoon Kim', 'Gyeong-In Yu', 'Byung-Gon Chun']	453.0
838	Automatic Parallelization of Python Programs for Distributed Heterogeneous Computing	Jun Shirako	2022-03-11 20:22:21	http://arxiv.org/abs/2203.06233v1	This paper introduces a novel approach to automatic ahead-of-time (AOT) parallelization and optimization of sequential Python programs for execution on distributed heterogeneous platforms. Our approach enables AOT source-to-source transformation of Python programs, driven by the inclusion of type hints for function parameters and return values. These hints can be supplied by the programmer or obtained by dynamic profiler tools; multi-version code generation guarantees the correctness of our AOT transformation in all cases. Our compilation framework performs automatic parallelization and sophisticated high-level code optimizations for the target distributed heterogeneous hardware platform. It includes extensions to the polyhedral framework that unify user-written loops and implicit loops present in matrix/tensor operators, as well as automated section of CPU vs. GPU code variants. Further, our polyhedral optimizations enable both intra-node and inter-node parallelism. Finally, the optimized output code is deployed using the Ray runtime for scheduling distributed tasks across multiple heterogeneous nodes in a cluster. Our empirical evaluation shows significant performance improvements relative to sequential Python in both single-node and multi-node experiments, with a performance improvement of over 20,000$\times$ when using 24 nodes and 144 GPUs in the OLCF Summit supercomputer for the Space-Time Adaptive Processing (STAP) radar application.			arxiv	['Akihiro Hayashi', 'Sri Raj Paul', 'Alexey Tumanov', 'Vivek Sarkar']	454.0
839	Transparent Serverless execution of Python multiprocessing applications	Aitor Arjona	2022-05-18 09:32:11	http://arxiv.org/abs/2205.08818v2	Access transparency means that both local and remote resources are accessed using identical operations. With transparency, unmodified single-machine applications could run over disaggregated compute, storage, and memory resources. Hiding the complexity of distributed systems through transparency would have great benefits, like scaling-out local-parallel scientific applications over flexible disaggregated resources in the Cloud. This paper presents a performance evaluation where we assess the feasibility of access transparency over state-of-the-art Cloud disaggregated resources for Python multiprocessing applications. We have interfaced the multiprocessing module with an implementation that transparently runs processes on serverless functions and uses an in-memory data store for shared state. To evaluate transparency, we run in the Cloud four unmodified applications: Uber Research's Evolution Strategies, Baselines-AI's Proximal Policy Optimization, Pandaral.lel's dataframe, and ScikitLearn's Hyperparameter tuning. We compare execution time and scalability of the same application running over disaggregated resources using our library, with the single-machine Python multiprocessing libraries in a large VM. For equal resources, applications efficiently using message-passing abstractions achieve comparable results despite the significant overheads of remote communication. Other shared-memory intensive applications do not perform due to high remote memory latency. The results show that Python's multiprocessing library design is an enabler towards transparency: legacy applications using efficient disaggregated abstractions can transparently scale beyond VM limited resources for increased parallelism without changing the underlying code or architecture.			arxiv	['Gerard Finol', 'Pedro Garcia-Lopez']	455.0
840	Plotly-Resampler: Effective Visual Analytics for Large Time Series	Jonas Van Der Donckt	2022-06-17 16:12:55	http://arxiv.org/abs/2206.08703v2	Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly's Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler's flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler's integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.			arxiv	['Jeroen Van Der Donckt', 'Emiel Deprost', 'Sofie Van Hoecke']	456.0
841	A Python-based Mixed Discrete-Continuous Simulation Framework for Digital Twins	Neha Karanjkar	2022-07-31 10:48:48	http://arxiv.org/abs/2208.01408v1	The use of Digital Twins is set to transform the manufacturing sector by aiding monitoring and real-time decision making. For several applications in this sector, the system to be modeled consists of a mix of discrete-event and continuous processes interacting with each other. Building simulation-based Digital Twins of such systems necessitates an open, flexible simulation framework which can support easy modeling and fast simulation of both continuous and discrete-event components, and their interactions. In this paper, we present an outline and key design aspects of a Python-based framework for performing mixed discrete-continuous simulations. The continuous processes in the system are assumed to be loosely coupled to other components via pre-defined events. For example, a continuous state variable crossing a threshold may trigger an external event. Similarly, external events may lead to a sudden change in the trajectory, state value or boundary conditions in a continuous process. We first present a systematic events-based interface using which such interactions can be modeled and simulated. We then discuss implementation details of the framework along with a detailed example. In our implementation, the advancement of time is controlled and performed using the event-stepped engine of SimPy (a popular discrete-event simulation library in Python). The continuous processes are modelled using existing frameworks with a Python wrapper providing the events interface. We discuss possible improvements to the time advancement scheme, a roadmap and use cases for the framework.			arxiv	['Subodh M. Joshi']	457.0
842	pPython for Parallel Python Programming	Chansup Byun	2022-08-31 15:08:39	http://arxiv.org/abs/2208.14908v1	pPython seeks to provide a parallel capability that provides good speed-up without sacrificing the ease of programming in Python by implementing partitioned global array semantics (PGAS) on top of a simple file-based messaging library (PythonMPI) in pure Python. The core data structure in pPython is a distributed numerical array whose distribution onto multiple processors is specified with a map construct. Communication operations between distributed arrays are abstracted away from the user and pPython transparently supports redistribution between any block-cyclic-overlapped distributions in up to four dimensions. pPython follows a SPMD (single program multiple data) model of computation. pPython runs on any combination of heterogeneous systems that support Python, including Windows, Linux, and MacOS operating systems. In addition to running transparently on single-node (e.g., a laptop), pPython provides a scheduler interface, so that pPython can be executed in a massively parallel computing environment. The initial implementation uses the Slurm scheduler. Performance of pPython on the HPC Challenge benchmark suite demonstrates both ease of programming and scalability.			arxiv	['William Arcand', 'David Bestor', 'Bill Bergeron', 'Vijay Gadepally', 'Michael Houle', 'Matthew Hubbell', 'Hayden Jananthan', 'Michael Jones', 'Kurt Keville', 'Anna Klein', 'Peter Michaleas', 'Lauren Milechin', 'Guillermo Morales', 'Julie Mullen', 'Andrew Prout', 'Albert Reuther', 'Antonio Rosa', 'Siddharth Samsi', 'Charles Yee', 'Jeremy Kepner']	458.0
843	TorchDIVA: An Extensible Computational Model of Speech Production built on an Open-Source Machine Learning Library	Sean Kinahan	2022-10-17 18:00:52	http://arxiv.org/abs/2210.09334v1	The DIVA model is a computational model of speech motor control that combines a simulation of the brain regions responsible for speech production with a model of the human vocal tract. The model is currently implemented in Matlab Simulink; however, this is less than ideal as most of the development in speech technology research is done in Python. This means there is a wealth of machine learning tools which are freely available in the Python ecosystem that cannot be easily integrated with DIVA. We present TorchDIVA, a full rebuild of DIVA in Python using PyTorch tensors. DIVA source code was directly translated from Matlab to Python, and built-in Simulink signal blocks were implemented from scratch. After implementation, the accuracy of each module was evaluated via systematic block-by-block validation. The TorchDIVA model is shown to produce outputs that closely match those of the original DIVA model, with a negligible difference between the two. We additionally present an example of the extensibility of TorchDIVA as a research platform. Speech quality enhancement in TorchDIVA is achieved through an integration with an existing PyTorch generative vocoder called DiffWave. A modified DiffWave mel-spectrum upsampler was trained on human speech waveforms and conditioned on the TorchDIVA speech production. The results indicate improved speech quality metrics in the DiffWave-enhanced output as compared to the baseline. This enhancement would have been difficult or impossible to accomplish in the original Matlab implementation. This proof-of-concept demonstrates the value TorchDIVA will bring to the research community. Researchers can download the new implementation at: https://github.com/skinahan/DIVA_PyTorch			arxiv	['Julie Liss', 'Visar Berisha']	459.0
844	Using the Uniqueness of Global Identifiers to Determine the Provenance of Python Software Source Code	Yiming Sun	2023-05-24 07:42:11	http://arxiv.org/abs/2305.14837v1	We consider the problem of identifying the provenance of free/open source software (FOSS) and specifically the need of identifying where reused source code has been copied from. We propose a lightweight approach to solve the problem based on software identifiers-such as the names of variables, classes, and functions chosen by programmers. The proposed approach is able to efficiently narrow down to a small set of candidate origin products, to be further analyzed with more expensive techniques to make a final provenance determination.By analyzing the PyPI (Python Packaging Index) open source ecosystem we find that globally defined identifiers are very distinct. Across PyPI's 244 K packages we found 11.2 M different global identifiers (classes and method/function names-with only 0.6% of identifiers shared among the two types of entities); 76% of identifiers were used only in one package, and 93% in at most 3. Randomly selecting 3 non-frequent global identifiers from an input product is enough to narrow down its origins to a maximum of 3 products within 89% of the cases.We validate the proposed approach by mapping Debian source packages implemented in Python to the corresponding PyPI packages; this approach uses at most five trials, where each trial uses three randomly chosen global identifiers from a randomly chosen python file of the subject software package, then ranks results using a popularity index and requires to inspect only the top result. In our experiments, this method is effective at finding the true origin of a project with a recall of 0.9 and precision of 0.77.			arxiv	['Daniel M. German', 'Stefano Zacchiroli']	460.0
845	PyBADS: Fast and robust black-box optimization in Python	Gurjeet Sangra Singh	2023-06-27 15:54:44	http://arxiv.org/abs/2306.15576v1	PyBADS is a Python implementation of the Bayesian Adaptive Direct Search (BADS) algorithm for fast and robust black-box optimization (Acerbi and Ma 2017). BADS is an optimization algorithm designed to efficiently solve difficult optimization problems where the objective function is rough (non-convex, non-smooth), mildly expensive (e.g., the function evaluation requires more than 0.1 seconds), possibly noisy, and gradient information is unavailable. With BADS, these issues are well addressed, making it an excellent choice for fitting computational models using methods such as maximum-likelihood estimation. The algorithm scales efficiently to black-box functions with up to $D \approx 20$ continuous input parameters and supports bounds or no constraints. PyBADS comes along with an easy-to-use Pythonic interface for running the algorithm and inspecting its results. PyBADS only requires the user to provide a Python function for evaluating the target function, and optionally other constraints. Extensive benchmarks on both artificial test problems and large real model-fitting problems models drawn from cognitive, behavioral and computational neuroscience, show that BADS performs on par with or better than many other common and state-of-the-art optimizers (Acerbi and Ma 2017), making it a general model-fitting tool which provides fast and robust solutions.			arxiv	['Luigi Acerbi']	461.0
846	TurboGenius: Python suite for high-throughput calculations of ab initio quantum Monte Carlo methods	Kousuke Nakano	2023-10-04 06:02:17	http://arxiv.org/abs/2310.02597v1	TurboGenius is an open-source Python package designed to fully control ab initio quantum Monte Carlo (QMC) jobs using a Python script, which allows one to perform high-throughput calculations combined with TurboRVB [K. Nakano et al. J. Phys. Chem. 152, 204121 (2020)]. This paper provides an overview of the TurboGenius package and showcases several results obtained in a high-throughput mode. For the purpose of performing high-throughput calculations with TurboGenius, we implemented another open-source Python package, TurboWorkflows, that enables one to construct simple workflows using TurboGenius. We demonstrate its effectiveness by performing (1) validations of density functional theory (DFT) and QMC drivers as implemented in the TurboRVB package and (2) benchmarks of Diffusion Monte Carlo (DMC) calculations for several data sets. For (1), we checked inter-package consistencies between TurboRVB and other established quantum chemistry packages. By doing so, we confirmed that DFT energies obtained by PySCF are consistent with those obtained by TurboRVB within the local density approximation (LDA), and that Hartree-Fock (HF) energies obtained by PySCF and Quantum Package are consistent with variational Monte Carlo energies obtained by TurboRVB with the HF wavefunctions. These validation tests constitute a further reliability check of the TurboRVB package. For (2), we benchmarked atomization energies of the Gaussian-2 set, binding energies of the S22, A24, and SCAI sets, and equilibrium lattice parameters of 12 cubic crystals using DMC calculations. We found that, for all compounds analyzed here, the DMC calculations with the LDA nodal surface give satisfactory results, i.e., consistent either with high-level computational or with experimental reference values.			arxiv	['Oto Kohul√°k', 'Abhishek Raghav', 'Michele Casula', 'Sandro Sorella']	462.0
847	SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation	Jialing Pan	2023-10-24 06:04:28	http://arxiv.org/abs/2310.15539v2	With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance training efficiency in terms of time, we adopt curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert takes only 6 hours to train on one single 80Gb A100 HBM. With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is release here: https://github.com/sade-adrien/SteloCoder.			arxiv	['Adrien Sad√©', 'Jin Kim', 'Eric Soriano', 'Guillem Sole', 'Sylvain Flamant']	463.0
848	Galactic Foreground Constraints from the Python V Cosmic Microwave Background Anisotropy Data	Pia Mukherjee	2003-01-29 17:43:39	http://arxiv.org/abs/astro-ph/0301581v2	We constrain Galactic foreground contamination of the Python V cosmic microwave background anisotropy data by cross correlating it with foreground contaminant emission templates. To model foreground emission we use 100 and 12 $\mu$m dust emission templates and two point source templates based on the PMN survey. The analysis takes account of inter-modulation correlations in 8 modulations of the data that are sensitive to a large range of angular scales and also densely sample a large area of sky. As a consequence the analysis here is highly constraining. We find little evidence for foreground contamination in an analysis of the whole data set. However, there is indication that foregrounds are present in the data from the larger-angular-scale modulations of those Python V fields that overlap the region scanned earlier by the UCSB South Pole 1994 experiment. This is an independent consistency cross-check of findings from the South Pole 1994 data.			arxiv	['Kim Coble', 'Mark Dragovan', 'Ken Ganga', 'John Kovac', 'Bharat Ratra', 'Tarun Souradeep']	464.0
849	Users' participation to the design process in an Open Source Software online community	Flore Barcellini	2006-12-01 11:42:44	http://arxiv.org/abs/cs/0612009v1	The objective of this research is to analyse the ways members of open-source software communities participate in design. In particular we focus on how users of an Open Source (OS) programming language (Python) participate in adding new functionalities to the language. Indeed, in the OS communities, users are highly skilled in computer sciences; they do not correspond to the common representation of end-users and can potentially participate to the design process. Our study characterizes the Python galaxy and analyses a formal process to introduce new functionalities to the language called Python Enhancement Proposal (PEP) from the idea of language evolution to the PEP implementation. The analysis of a particular pushed-by-users PEP from one application domain community (financial), shows: that the design process is distributed and specialized between online and physical interactions spaces; and there are some cross participants between users and developers communities which may reveal boundary spanners roles.			arxiv	['Fran√ßoise D√©tienne', 'Jean-Marie Burkhardt']	465.0
850	Python - All a Scientist Needs	Julius B. Lucks	2008-03-12 20:08:07	http://arxiv.org/abs/0803.1838v1	Any cutting-edge scientific research project requires a myriad of computational tools for data generation, management, analysis and visualization. Python is a flexible and extensible scientific programming platform that offered the perfect solution in our recent comparative genomics investigation (J. B. Lucks, D. R. Nelson, G. Kudla, J. B. Plotkin. Genome landscapes and bacteriophage codon usage, PLoS Computational Biology, 4, 1000001, 2008). In this paper, we discuss the challenges of this project, and how the combined power of Biopython, Matplotlib and SWIG were utilized for the required computational tasks. We finish by discussing how python goes beyond being a convenient programming language, and promotes good scientific practice by enabling clean code, integration with professional programming techniques such as unit testing, and strong data provenance.			arxiv	[]	466.0
851	High-Performance Astrophysical Simulations and Analysis with Python	Matthew J. Turk	2011-12-19 21:00:05	http://arxiv.org/abs/1112.4482v1	The usage of the high-level scripting language Python has enabled new mechanisms for data interrogation, discovery and visualization of scientific data. We present yt, an open source, community-developed astrophysical analysis and visualization toolkit for data generated by high-performance computing (HPC) simulations of astrophysical phenomena. Through a separation of responsibilities in the underlying Python code, yt allows data generated by incompatible, and sometimes even directly competing, astrophysical simulation platforms to be analyzed in a consistent manner, focusing on physically relevant quantities rather than quantities native to astrophysical simulation codes. We present on its mechanisms for data access, capabilities for MPI-parallel analysis, and its implementation as an in situ analysis and visualization tool.			arxiv	['Britton D. Smith']	467.0
852	Technical report: CSVM dictionaries	Fr√©d√©ric Rodriguez	2012-08-08 19:48:40	http://arxiv.org/abs/1208.1934v1	CSVM (CSV with Metadata) is a simple file format for tabular data. The possible application domain is the same as typical spreadsheets files, but CSVM is well suited for long term storage and the inter-conversion of RAW data. CSVM embeds different levels for data, metadata and annotations in human readable format and flat ASCII files. As a proof of concept, Perl and Python toolkits were designed in order to handle CSVM data and objects in workflows. These parsers can process CSVM files independently of data types, so it is possible to use same data format and parser for a lot of scientific purposes. CSVM-1 is the first version of CSVM specification, an extension of CSVM-1 for implementing a translation system between CSVM files is presented in this paper. The necessary data used to make the translation are also coded in another CSVM file. This particular kind of CSVM is called a CSVM dictionary, it is also readable by the current CSVM parser and it is fully supported by the Python toolkit. This report presents a proposal for CSVM dictionaries, a working example in chemistry, and some elements of Python toolkit usable to handle these files.			arxiv	[]	468.0
853	Towards Python-based Domain-specific Languages for Self-reconfigurable Modular Robotics Research	Mikael Moghadam	2013-02-22 09:01:24	http://arxiv.org/abs/1302.5521v1	This paper explores the role of operating system and high-level languages in the development of software and domain-specific languages (DSLs) for self-reconfigurable robotics. We review some of the current trends in self-reconfigurable robotics and describe the development of a software system for ATRON II which utilizes Linux and Python to significantly improve software abstraction and portability while providing some basic features which could prove useful when using Python, either stand-alone or via a DSL, on a self-reconfigurable robot system. These features include transparent socket communication, module identification, easy software transfer and reliable module-to-module communication. The end result is a software platform for modular robots that where appropriate builds on existing work in operating systems, virtual machines, middleware and high-level languages.			arxiv	['David Johan Christensen', 'David Brandt', 'Ulrik Pagh Schultz']	469.0
854	Simulating X-ray Observations with Python	John A. ZuHone	2014-07-07 17:41:05	http://arxiv.org/abs/1407.1783v2	X-ray astronomy is an important tool in the astrophysicist's toolkit to investigate high-energy astrophysical phenomena. Theoretical numerical simulations of astrophysical sources are fully three-dimensional representations of physical quantities such as density, temperature, and pressure, whereas astronomical observations are two-dimensional projections of the emission generated via mechanisms dependent on these quantities. To bridge the gap between simulations and observations, algorithms for generating synthetic observations of simulated data have been developed. We present an implementation of such an algorithm in the yt analysis software package. We describe the underlying model for generating the X-ray photons, the important role that yt and other Python packages play in its implementation, and present a detailed workable example of the creation of simulated X-ray observations.			arxiv	['Veronica Biffi', 'Eric J. Hallman', 'Scott W. Randall', 'Adam R. Foster', 'Christian Schmid']	470.0
855	PyCraters: A Python framework for crater function analysis	Scott A. Norris	2014-10-28 21:13:02	http://arxiv.org/abs/1410.8489v1	We introduce a Python framework designed to automate the most common tasks associated with the extraction and upscaling of the statistics of single-impact crater functions to inform coefficients of continuum equations describing surface morphology evolution. Designed with ease-of-use in mind, the framework allows users to extract meaningful statistical estimates with very short Python programs. Wrappers to interface with specific simulation packages, routines for statistical extraction of output, and fitting and differentiation libraries are all hidden behind simple, high-level user-facing functions. In addition, the framework is extensible, allowing advanced users to specify the collection of specialized statistics or the creation of customized plots. The framework is hosted on the BitBucket service under an open-source license, with the aim of helping non-specialists easily extract preliminary estimates of relevant crater function results associated with a particular experimental system.			arxiv	[]	471.0
856	Numerical simulation of liver perfusion: from CT scans to FE model	Vladim√≠r Luke≈°	2014-12-19 16:12:02	http://arxiv.org/abs/1412.6412v1	We use a collection of Python programs for numerical simulation of liver perfusion. We have an application for semi-automatic generation of a finite element mesh of the human liver from computed tomography scans and for reconstruction of the liver vascular structure. When the real vascular trees can not be obtained from the CT data we generate artificial trees using the constructive optimization method. The generated FE mesh and vascular trees are imported into SfePy (Simple Finite Elements in Python) and numerical simulations are performed in order to get the pressure distribution and perfusion flows in the liver tissue. In the post-processing steps we calculate transport of a contrast fluid through the liver parenchyma.			arxiv	['Miroslav Ji≈ô√≠k', 'Alena Jon√°≈°ov√°', 'Eduard Rohan', 'Ond≈ôej Bubl√≠k', 'Robert Cimrman']	472.0
857	Liouville theory with a central charge less than one	Sylvain Ribault	2015-03-06 20:57:01	http://arxiv.org/abs/1503.02067v3	We determine the spectrum and correlation functions of Liouville theory with a central charge less than (or equal) one. This completes the definition of Liouville theory for all complex values of the central charge. The spectrum is always spacelike, and there is no consistent timelike Liouville theory. We also study the non-analytic conformal field theories that exist at rational values of the central charge. Our claims are supported by numerical checks of crossing symmetry. We provide Python code for computing Virasoro conformal blocks, and correlation functions in Liouville theory and (generalized) minimal models.			arxiv	['Raoul Santachiara']	473.0
858	Oasis: a high-level/high-performance open source Navier-Stokes solver	Mikael Mortensen	2016-02-11 08:56:43	http://arxiv.org/abs/1602.03643v1	Oasis is a high-level/high-performance finite element Navier-Stokes solver written from scratch in Python using building blocks from the FEniCS project (fenicsproject.org). The solver is unstructured and targets large-scale applications in complex geometries on massively parallel clusters. Oasis utilizes MPI and interfaces, through FEniCS, to the linear algebra backend PETSc. Oasis advocates a high-level, programmable user interface through the creation of highly flexible Python modules for new problems. Through the high-level Python interface the user is placed in complete control of every aspect of the solver. A version of the solver, that is using piecewise linear elements for both velocity and pressure, is shown reproduce very well the classical, spectral, turbulent channel simulations of Moser, Kim and Mansour at $Re_{\tau}=180$ [Phys. Fluids, vol 11(4), p. 964]. The computational speed is strongly dominated by the iterative solvers provided by the linear algebra backend, which is arguably the best performance any similar implicit solver using PETSc may hope for. Higher order accuracy is also demonstrated and new solvers may be easily added within the same framework.			arxiv	['Kristian Valen-Sendstad']	474.0
859	Cygrid: A fast Cython-powered convolution-based gridding module for Python	B. Winkel	2016-04-22 14:07:06	http://arxiv.org/abs/1604.06667v1	Data gridding is a common task in astronomy and many other science disciplines. It refers to the resampling of irregularly sampled data to a regular grid. We present cygrid, a library module for the general purpose programming language Python. Cygrid can be used to resample data to any collection of target coordinates, although its typical application involves FITS maps or data cubes. The FITS world coordinate system standard is supported. The regridding algorithm is based on the convolution of the original samples with a kernel of arbitrary shape. We introduce a lookup table scheme that allows us to parallelize the gridding and combine it with the HEALPix tessellation of the sphere for fast neighbor searches. We show that for $n$ input data points, cygrids runtime scales between O(n) and O(n log n) and analyze the performance gain that is achieved using multiple CPU cores. We also compare the gridding speed with other techniques, such as nearest-neighbor, and linear and cubic spline interpolation. Cygrid is a very fast and versatile gridding library that significantly outperforms other third-party Python modules, such as the linear and cubic spline interpolation provided by SciPy.			arxiv	['D. Lenz', 'L. Fl√∂er']	475.0
860	Mocking the Weak Lensing universe: the LensTools python computing package	Andrea Petri	2016-06-06 20:00:10	http://arxiv.org/abs/1606.01903v2	We present a newly developed software package which implements a wide range of routines frequently used in Weak Gravitational Lensing (WL). With the continuously increasing size of the WL scientific community we feel that easy to use Application Program Interfaces (APIs) for common calculations are a necessity to ensure efficiency and coordination across different working groups. Coupled with existing open source codes, such as CAMB and Gadget2, LensTools brings together a cosmic shear simulation pipeline which, complemented with a variety of WL feature measurement tools and parameter sampling routines, provides easy access to the numerics for theoretical studies of WL as well as for experiment forecasts. Being implemented in python, LensTools takes full advantage of a range of state--of--the art techniques developed by the large and growing open--source software community (scipy,pandas,astropy,scikit-learn,emcee). We made the LensTools code available on the Python Package Index and published its documentation on http://lenstools.readthedocs.io			arxiv	[]	476.0
861	D2O - a distributed data object for parallel high-performance computing in Python	T. Steininger	2016-06-16 23:19:58	http://arxiv.org/abs/1606.05385v2	We introduce D2O, a Python module for cluster-distributed multi-dimensional numerical arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achieve usability without losing numerical performance and scalability. D2O's global interface is similar to the one of a numpy.ndarray, whereas the cluster node's local data is directly accessible for use in customized high-performance modules. D2O is written in pure Python which makes it portable and easy to use and modify. Expensive operations are carried out by dedicated external libraries like numpy and mpi4py. The performance of D2O is on a par with numpy for serial applications and scales well when moving to an MPI cluster. D2O is open-source software available under the GNU General Public License v3 (GPL-3) at https://gitlab.mpcdf.mpg.de/ift/D2O			arxiv	['M. Greiner', 'F. Beaujean', 'T. En√ülin']	477.0
862	Quintuple: a Python 5-qubit quantum computer simulator to facilitate cloud quantum computing	Christine Corbett Moran	2016-06-29 19:12:21	http://arxiv.org/abs/1606.09225v1	"In May 2016 IBM released access to its 5-qubit quantum computer to the scientific community, its ""IBM Quantum Experience"" since acquiring over 25,000 users from students, educators and researchers around the globe. In the short time since the ""IBM Quantum Experience"" became available, a flurry of research results on 5-qubit systems have been published derived from the platform hardware. Quintuple is an open-source object-oriented Python module implementing the simulation of the ""IBM Quantum Experience"" hardware. Quintuple quantum algorithms can be programmed and run via a custom language fully compatible with the ""IBM Quantum Experience"" or in pure Python. Over 40 example programs are provided with expected results, including Grover's Algorithm and the Deutsch-Jozsa algorithm. Quintuple contributes to the study of 5-qubit systems and the development and debugging of quantum algorithms for deployment on the ""IBM Quantum Experience"" hardware."			arxiv	[]	478.0
863	User interfaces for computational science: a domain specific language for OOMMF embedded in Python	Marijan Beg	2016-09-23 17:07:32	http://arxiv.org/abs/1609.07432v2	Computer simulations are used widely across the engineering and science disciplines, including in the research and development of magnetic devices using computational micromagnetics. In this work, we identify and review different approaches to configuring simulation runs: (i) the re-compilation of source code, (ii) the use of configuration files, (iii) the graphical user interface, and (iv) embedding the simulation specification in an existing programming language to express the computational problem. We identify the advantages and disadvantages of different approaches and discuss their implications on effectiveness and reproducibility of computational studies and results. Following on from this, we design and describe a domain specific language for micromagnetics that is embedded in the Python language, and allows users to define the micromagnetic simulations they want to carry out in a flexible way. We have implemented this micromagnetic simulation description language together with a computational backend that executes the simulation task using the Object Oriented MicroMagnetic Framework (OOMMF). We illustrate the use of this Python interface for OOMMF by solving the micromagnetic standard problem 4. All the code is publicly available and is open source.			arxiv	['Ryan A. Pepper', 'Hans Fangohr']	479.0
864	Big Data analytics. Three use cases with R, Python and Spark	Philippe Besse	2016-09-30 07:35:49	http://arxiv.org/abs/1609.09619v1	Management and analysis of big data are systematically associated with a data distributed architecture in the Hadoop and now Spark frameworks. This article offers an introduction for statisticians to these technologies by comparing the performance obtained by the direct use of three reference environments: R, Python Scikit-learn, Spark MLlib on three public use cases: character recognition, recommending films, categorizing products. As main result, it appears that, if Spark is very efficient for data munging and recommendation by collaborative filtering (non-negative factorization), current implementations of conventional learning methods (logistic regression, random forests) in MLlib or SparkML do not ou poorly compete habitual use of these methods (R, Python Scikit-learn) in an integrated or undistributed architecture			arxiv	['Brendan Guillouet', 'Jean-Michel Loubes']	480.0
865	QuSpin: a Python Package for Dynamics and Exact Diagonalisation of Quantum Many Body Systems part I: spin chains	Phillip Weinberg	2016-10-10 19:46:58	http://arxiv.org/abs/1610.03042v4	We present a new open-source Python package for exact diagonalization and quantum dynamics of spin(-photon) chains, called QuSpin, supporting the use of various symmetries in 1-dimension and (imaginary) time evolution for chains up to 32 sites in length. The package is well-suited to study, among others, quantum quenches at finite and infinite times, the Eigenstate Thermalisation hypothesis, many-body localisation and other dynamical phase transitions, periodically-driven (Floquet) systems, adiabatic and counter-diabatic ramps, and spin-photon interactions. Moreover, QuSpin's user-friendly interface can easily be used in combination with other Python packages which makes it amenable to a high-level customisation. We explain how to use QuSpin using four detailed examples: (i) Standard exact diagonalisation of XXZ chain (ii) adiabatic ramping of parameters in the many-body localised XXZ model, (iii) heating in the periodically-driven transverse-field Ising model in a parallel field, and (iv) quantised light-atom interactions: recovering the periodically-driven atom in the semi-classical limit of a static Hamiltonian.			arxiv	['Marin Bukov']	481.0
866	TensorLy: Tensor Learning in Python	Jean Kossaifi	2016-10-29 18:32:27	http://arxiv.org/abs/1610.09555v2	Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed \emph{TensorLy}, a high-level API for tensor methods and deep tensorized neural networks in Python. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and seamlessly integrates with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows users to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly			arxiv	['Yannis Panagakis', 'Anima Anandkumar', 'Maja Pantic']	482.0
867	The Python-based Simulations of Chemistry Framework (PySCF)	Qiming Sun	2017-01-27 23:57:43	http://arxiv.org/abs/1701.08223v2	PySCF is a general-purpose electronic structure platform designed from the ground up to emphasize code simplicity, both to aid new method development, as well as for flexibility in computational workflow. The package provides a wide range of tools to support simulations of finite size systems, extended systems with periodic boundary conditions, low dimensional periodic systems, and custom Hamiltonians, using mean-field and post-mean-field methods with standard Gaussian basis functions. To ensure easy of extensibility, PySCF uses the Python language to implement almost all its features, while computationally critical paths are implemented with heavily optimized C routines. Using this combined Python/C implementation, the package is as efficient as the best existing C or Fortran based quantum chemistry programs. In this paper we document the capabilities and design philosophy of the current version of the PySCF package.			arxiv	['Timothy C. Berkelbach', 'Nick S. Blunt', 'George H. Booth', 'Sheng Guo', 'Zhendong Li', 'Junzi Liu', 'James McClain', 'Elvira R. Sayfutyarova', 'Sandeep Sharma', 'Sebastian Wouters', 'Garnet Kin-Lic Chan']	483.0
868	PonyGE2: Grammatical Evolution in Python	Michael Fenton	2017-03-24 17:50:28	http://arxiv.org/abs/1703.08535v2	Grammatical Evolution (GE) is a population-based evolutionary algorithm, where a formal grammar is used in the genotype to phenotype mapping process. PonyGE2 is an open source implementation of GE in Python, developed at UCD's Natural Computing Research and Applications group. It is intended as an advertisement and a starting-point for those new to GE, a reference for students and researchers, a rapid-prototyping medium for our own experiments, and a Python workout. As well as providing the characteristic genotype to phenotype mapping of GE, a search algorithm engine is also provided. A number of sample problems and tutorials on how to use and adapt PonyGE2 have been developed.			arxiv	"['James McDermott', 'David Fagan', 'Stefan Forstenlechner', ""Michael O'Neill"", 'Erik Hemberg']"	484.0
869	Magpy: A C++ accelerated Python package for simulating magnetic nanoparticle stochastic dynamics	Oliver Laslett	2018-01-18 14:55:18	http://arxiv.org/abs/1801.06073v2	Magpy is a C++ accelerated Python package for modelling and simulating the magnetic dynamics of nano-sized particles. Nanoparticles are modelled as a system of three-dimensional macrospins and simulated with a set of coupled stochastic differential equations (the Landau-Lifshitz-Gilbert equation), which are solved numerically using explicit or implicit methods. The results of the simulations may be used to compute equilibrium states, the dynamic response to external magnetic fields, and heat dissipation. Magpy is built on a C++ library, which is optimised for serial execution, and exposed through a Python interface utilising an embarrassingly parallel strategy. Magpy is free, open-source, and available on github under the 3-Clause BSD License.			arxiv	['Jonathon Waters', 'Hans Fangohr', 'Ondrej Hovorka']	485.0
870	ChromaStarPy: A stellar atmosphere and spectrum modeling and visualization lab in python	C. Ian Short	2018-01-22 17:19:45	http://arxiv.org/abs/1801.07208v1	"We announce ChromaStarPy, an integrated general stellar atmospheric modeling and spectrum synthesis code written entirely in python V. 3. ChromaStarPy is a direct port of the ChromaStarServer (CSServ) Java modeling code described in earlier papers in this series, and many of the associated JavaScript (JS) post-processing procedures have been ported and incorporated into CSPy so that students have access to ready-made ""data products"". A python integrated development environment (IDE) allows a student in a more advanced course to experiment with the code and to graphically visualize intermediate and final results, ad hoc, as they are running it. CSPy allows students and researchers to compare modeled to observed spectra in the same IDE in which they are processing observational data, while having complete control over the stellar parameters affecting the synthetic spectra. We also take the opportunity to describe improvements that have been made to the related codes, ChromaStar (CS), CSServ and ChromaStarDB (CSDB) that, where relevant, have also been incorporated into CSPy. The application may be found at the home page of the OpenStars project: http://www.ap.smu.ca/~ishort/OpenStars/ ."			arxiv	['Jason H. T. Bayer', 'Lindsey M. Burns']	486.0
871	pyGDM -- A python toolkit for full-field electro-dynamical simulations and evolutionary optimization of nanostructures	Peter R. Wiecha	2018-02-12 14:34:48	http://arxiv.org/abs/1802.04071v2	pyGDM is a python toolkit for electro-dynamical simulations in nano-optics based on the Green Dyadic Method (GDM). In contrast to most other coupled-dipole codes, pyGDM uses a generalized propagator, which allows to cost-efficiently solve large monochromatic problems such as polarization-resolved calculations or raster-scan simulations with a focused beam or a quantum-emitter probe. A further peculiarity of this software is the possibility to very easily solve 3D problems including a dielectric or metallic substrate. Furthermore, pyGDM includes tools to easily derive several physical quantities such as far-field patterns, extinction and scattering cross-section, the electric and magnetic near-field in the vicinity of the structure, the decay rate of quantum emitters and the LDOS or the heat deposited inside a nanoparticle. Finally, pyGDM provides a toolkit for efficient evolutionary optimization of nanoparticle geometries in order to maximize (or minimize) optical properties such as a scattering at selected resonance wavelengths.			arxiv	[]	487.0
872	DESlib: A Dynamic ensemble selection library in Python	Rafael M. O. Cruz	2018-02-14 06:30:47	http://arxiv.org/abs/1802.04967v3	DESlib is an open-source python library providing the implementation of several dynamic selection techniques. The library is divided into three modules: (i) \emph{dcs}, containing the implementation of dynamic classifier selection methods (DCS); (ii) \emph{des}, containing the implementation of dynamic ensemble selection methods (DES); (iii) \emph{static}, with the implementation of static ensemble techniques. The library is fully documented (documentation available online on Read the Docs), has a high test coverage (codecov.io) and is part of the scikit-learn-contrib supported projects. Documentation, code and examples can be found on its GitHub page: https://github.com/scikit-learn-contrib/DESlib.			arxiv	['Luiz G. Hafemann', 'Robert Sabourin', 'George D. C. Cavalcanti']	488.0
873	VoigtFit: A Python package for Voigt profile fitting	Jens-Kristian Krogager	2018-03-03 15:44:50	http://arxiv.org/abs/1803.01187v1	I present a Python package developed for fitting Voigt profiles to absorption lines. The software fits multiple components for various atomic lines simultaneously allowing parameters to be tied and fixed. Moreover, the code is able to automatically fit a polynomial continuum model together with the line profiles. Lastly, a physical model can readily be used to constrain thermal and turbulent broadening of absorption lines as well as implementing molecular excitation models. The code can be run with interactive features such as manual continuum placement locally around each line, manual masking of undesired fitting regions, and interactive definition of velocity components for various elements. This greatly improves the ease by which the initial guesses can be estimated. Since the code is written in pure Python, it can easily be scripted and modified to fit the user's needs. The code uses a $\chi^2$ minimization approach to find the best solution. The code and a set of test-data together with the full documentation is available on GitHub.			arxiv	[]	489.0
874	PySEAL: A Python wrapper implementation of the SEAL homomorphic encryption library	Alexander J. Titus	2018-03-05 19:23:37	http://arxiv.org/abs/1803.01891v1	Motivation: The ability to perform operations on encrypted data has a growing number of applications in bioinformatics, with implications for data privacy in health care and biosecurity. The SEAL library is a popular implementation of fully homomorphic encryption developed in C++ by Microsoft Research. Despite the advantages of C++, Python is a flexible and dominant programming language that enables rapid prototyping of bioinformatics pipelines. Results: In an effort to make homomorphic encryption accessible to a broader range of bioinformatics scientists and applications, we present a Python binding implementation of the popular homomorphic encryption library, SEAL, using pybind11. The software contains a Docker image to facilitate easy installation and execution of the SEAL build process. Availability: All code is publicly available at https://github.com/Lab41/PySEAL Contact: lab41@iqt.org Supplementary information: Supplementary information is available on the Lab41 GitHub.			arxiv	['Shashwat Kishore', 'Todd Stavish', 'Stephanie M. Rogers', 'Karl Ni']	490.0
875	Generating Periodic Grain Boundary Structures: Algorithm and Open-Source Python Library	Jianli Cheng	2018-07-03 23:11:09	http://arxiv.org/abs/1807.01393v1	An algorithm implemented in an open-source python library was developed for building periodic coincidence site lattice (CSL) grain boundary models in a universal fashion. The software framework aims to generate tilt and twist grain boundaries from cubic and tetragonal crystals for ab-initio and classical atomistic simulation. This framework has two useful features: i) it can calculate all the CSL matrices for generating CSL from a given Sigma ({\Sigma}) value and rotation axis, allowing the users to build the specific CSL and grain boundary models; ii) it provides a convenient command line tool to enable high-throughput generation of tilt and twist grain boundaries by assigning an input crystal structure, {\Sigma} value, rotation axis, and grain boundary plane. The developed algorithm in the open-source python library is expected to facilitate studies of grain boundary in materials science. The software framework is available on the website: aimsgb.org.			arxiv	['Jian Luo', 'Kesong Yang']	491.0
876	Physical-type correctness in scientific Python	Marcus Foster	2018-07-17 13:12:32	http://arxiv.org/abs/1807.07643v3	The representation of units and dimensions in informatics systems is barely codified and often ignored. For instance, the major languages used in scientific computing (Fortran, C and Python), have no type for dimension or unit, and so physical quantities are represented in a program by variables of type real, resulting in the possibility of unit or dimensional errors. In view of this danger, many authors have proposed language schemes for unit-checking and conversion. However, since many physical quantities have the same units, it is possible for a block of code to be unit-compatible, but still physically meaningless. We demonstrate the limitations of three Python unit-libraries and present a justification and method for checking kind-of-quantity.			arxiv	['Sean Tregeagle']	492.0
877	Sourcerer's Apprentice and the study of code snippet migration	Stephen Romansky	2018-07-31 23:12:45	http://arxiv.org/abs/1808.00106v1	On the worldwide web, not only are webpages connected but source code is too. Software development is becoming more accessible to everyone and the licensing for software remains complicated. We need to know if software licenses are being maintained properly throughout their reuse and evolution. This motivated the development of the Sourcerer's Apprentice, a webservice that helps track clone relicensing, because software typically employ software licenses to describe how their software may be used and adapted. But most developers do not have the legal expertise to sort out license conflicts. In this paper we put the Apprentice to work on empirical studies that demonstrate there is much sharing between StackOverflow code and Python modules and Python documentation that violates the licensing of the original Python modules and documentation: software snippets shared through StackOverflow are often being relicensed improperly to CC-BY-SA 3.0 without maintaining the appropriate attribution. We show that many snippets on StackOverflow are inappropriately relicensed by StackOverflow users, jeopardizing the status of the software built by companies and developers who reuse StackOverflow snippets.			arxiv	['Cheng Chen', 'Baljeet Malhotra', 'Abram Hindle']	493.0
878	An Empirical Analysis of Vulnerabilities in Python Packages for Web Applications	Jukka Ruohonen	2018-10-31 14:41:12	http://arxiv.org/abs/1810.13310v2	This paper examines software vulnerabilities in common Python packages used particularly for web development. The empirical dataset is based on the PyPI package repository and the so-called Safety DB used to track vulnerabilities in selected packages within the repository. The methodological approach builds on a release-based time series analysis of the conditional probabilities for the releases of the packages to be vulnerable. According to the results, many of the Python vulnerabilities observed seem to be only modestly severe; input validation and cross-site scripting have been the most typical vulnerabilities. In terms of the time series analysis based on the release histories, only the recent past is observed to be relevant for statistical predictions; the classical Markov property holds.			arxiv	[]	494.0
879	xSLHA: an Les Houches Accord reader for Python and Mathematica	Florian Staub	2018-12-11 19:16:38	http://arxiv.org/abs/1812.04655v1	The format defined by the SUSY Les Houches Accord (SLHA) is widely used in high energy physics to store and exchange information. It is no longer applied only to a few supersymmetric models, but the general structure is adapted to all kind of models. Therefore, it is helpful to have parsers at hand which can import files in the SLHA format into high-level languages as Python and Mathematica in order to further process the data. The focus of the xSLHA package, which exists now for Python and Mathematica, was on a fast read-in of large data samples. Moreover, also some blocks used by different tools, as HiggsBounds for instance, deviate from the standard conventions. These are also supported by xSLHA.			arxiv	[]	495.0
880	Snakes on a Spaceship - An Overview of Python in Heliophysics	A. G. Burrell	2019-01-01 12:19:59	http://arxiv.org/abs/1901.00143v1	Computational analysis has become ubiquitous within the heliophysics community. However, community standards for peer-review of codes and analysis have lagged behind these developments. This absence has contributed to the reproducibility crisis, where inadequate analysis descriptions and loss of scientific data have made scientific studies difficult or impossible to replicate. The heliophysics community has responded to this challenge by expressing a desire for a more open, collaborative set of analysis tools. This article summarizes the current state of these efforts and presents an overview of many of the existing Python heliophysics tools. It also outlines the challenges facing community members who are working towards the goal of an open, collaborative, Python heliophysics toolkit and presents guidelines that can ease the transition from individualistic data analysis practices to an accountable, communalistic environment.			arxiv	['A. Halford', 'J. Klenzing', 'R. A. Stoneback', 'S. K. Morley', 'A. M. Annex', 'K. M. Laundal', 'A. C. Kellerman', 'D. Stansby', 'J. Ma']	496.0
881	GraSPy: Graph Statistics in Python	Jaewon Chung	2019-03-29 18:58:31	http://arxiv.org/abs/1904.05329v3	We introduce GraSPy, a Python library devoted to statistical inference, machine learning, and visualization of random graphs and graph populations. This package provides flexible and easy-to-use algorithms for analyzing and understanding graphs with a scikit-learn compliant API. GraSPy can be downloaded from Python Package Index (PyPi), and is released under the Apache 2.0 open-source license. The documentation and all releases are available at https://neurodata.io/graspy.			arxiv	['Benjamin D. Pedigo', 'Eric W. Bridgeford', 'Bijan K. Varjavand', 'Hayden S. Helm', 'Joshua T. Vogelstein']	497.0
882	PyLops -- A Linear-Operator Python Library for large scale optimization	Matteo Ravasi	2019-07-29 11:53:46	http://arxiv.org/abs/1907.12349v1	Linear operators and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing software packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector products and manipulation of matrices) with syntax that closely represents their corresponding analytical forms. However, many real application, large-scale operators do not lend themselves to explicit matrix representations, usually forcing practitioners to forego of the convenient linear-algebra syntax available for their explicit-matrix counterparts. PyLops is an open-source Python library providing a flexible and scalable framework for the creation and combination of so-called linear operators, class-based entities that represent matrices and inherit their associated syntax convenience, but do not rely on the creation of explicit matrices. We show that PyLops operators can dramatically reduce the memory load and CPU computations compared to explicit-matrix calculations, while still allowing users to seamlessly use their existing knowledge of compact matrix-based syntax that scales to any problem size because no explicit matrices are required.			arxiv	['Ivan Vasconcelos']	498.0
883	scikit-hubness: Hubness Reduction and Approximate Neighbor Search	Roman Feldbauer	2019-12-02 12:04:32	http://arxiv.org/abs/1912.00706v1	"This paper introduces scikit-hubness, a Python package for efficient nearest neighbor search in high-dimensional spaces. Hubness is an aspect of the curse of dimensionality, and is known to impair various learning tasks, including classification, clustering, and visualization. scikit-hubness provides algorithms for hubness analysis (""Is my data affected by hubness?""), hubness reduction (""How can we improve neighbor retrieval in high dimensions?""), and approximate neighbor search (""Does it work for large data sets?""). It is integrated into the scikit-learn environment, enabling rapid adoption by Python-based machine learning researchers and practitioners. Users will find all functionality of the scikit-learn neighbors package, plus additional support for transparent hubness reduction and approximate nearest neighbor search. scikit-hubness is developed using several quality assessment tools and principles, such as PEP8 compliance, unit tests with high code coverage, continuous integration on all major platforms (Linux, MacOS, Windows), and additional checks by LGTM. The source code is available at https://github.com/VarIr/scikit-hubness under the BSD 3-clause license. Install from the Python package index with $ pip install scikit-hubness."			arxiv	['Thomas Rattei', 'Arthur Flexer']	499.0
884	GPU Computing with Python: Performance, Energy Efficiency and Usability	H√•vard H. Holm	2019-12-05 14:46:06	http://arxiv.org/abs/1912.02607v1	In this work, we examine the performance, energy efficiency and usability when using Python for developing HPC codes running on the GPU. We investigate the portability of performance and energy efficiency between CUDA and OpenCL; between GPU generations; and between low-end, mid-range and high-end GPUs. Our findings show that the impact of using Python is negligible for our applications, and furthermore, CUDA and OpenCL applications tuned to an equivalent level can in many cases obtain the same computational performance. Our experiments show that performance in general varies more between different GPUs than between using CUDA and OpenCL. We also show that tuning for performance is a good way of tuning for energy efficiency, but that specific tuning is needed to obtain optimal energy efficiency.			arxiv	['Andr√© R. Brodtkorb', 'Martin L. S√¶tra']	500.0
885	Customers Churn Prediction in Financial Institution Using Artificial Neural Network	Kamorudeen A. Amuda	2019-12-23 08:24:29	http://arxiv.org/abs/1912.11346v1	In this study, a predictive model using Multi-layer Perceptron of Artificial Neural Network architecture was developed to predict customer churn in a financial institution. Previous researches have used supervised machine learning classifiers such as Logistic Regression, Decision Tree, Support Vector Machine, K-Nearest Neighbors, and Random Forest. These classifiers require human effort to perform feature engineering which leads to over-specified and incomplete feature selection. Therefore, this research developed a model to eliminate manual feature engineering in data preprocessing stage. Fifty thousand customers? data were extracted from the database of one of the leading financial institution in Nigeria for the study. The multi-layer perceptron model was built with python programming language and used two overfitting techniques (Dropout and L2 regularization). The implementation done in python was compared with another model in Neuro solution infinity software. The results showed that the Artificial Neural Network software development (Python) had comparable performance with that obtained from the Neuro Solution Infinity software. The accuracy rates are 97.53% and 97.4% while ROC (Receiver Operating Characteristic) curve graphs are 0.89 and 0.85 respectively.			arxiv	['Adesesan B. Adeyemo']	501.0
886	Enrico : a Python package to simplify Fermi-LAT analysis	D. A. Sanchez	2013-07-17 08:33:17	http://arxiv.org/abs/1307.4534v1	With the advent of the Large Array Telescope (LAT) on board the Fermi satellite, a new window on the Universe has been opened. Publicly available, the Fermi-LAT data come together with an analysis software named ScienceTools (ST, http://fermi.gsfc.nasa.gov/ssc/data/analysis/software/) which can be run through a Python interface. Nevertheless, for the user, the ST can be hard to run and imply several steps. Users already contributed with scripts for a specific task but no tool allowing a complete analysis is currently available. We present a Python package called {\tt Enrico}, designed to facilitate the data analysis. Using only configuration files and front end tools from the command line, the user can easily perform/reproduce an entire Fermi analysis and make plots for publications. It also include new features like debug plots, pipeline execution on one or several CPUs, downloading of the Fermi data or the generation of a sky model from the Fermi catalogue. {\tt Enrico} is an open-source project currently available for download at \url{https://github.com/gammapy/enrico}			arxiv	['C. Deil']	502.0
887	Krotov: A Python implementation of Krotov's method for quantum optimal control	Michael H. Goerz	2019-02-28 18:39:25	http://arxiv.org/abs/1902.11284v6	We present a new open-source Python package, krotov, implementing the quantum optimal control method of that name. It allows to determine time-dependent external fields for a wide range of quantum control problems, including state-to-state transfer, quantum gate implementation and optimization towards an arbitrary perfect entangler. Krotov's method compares to other gradient-based optimization methods such as gradient-ascent and guarantees monotonic convergence for approximately time-continuous control fields. The user-friendly interface allows for combination with other Python packages, and thus high-level customization. The package is being developed at https://github.com/qucontrol/krotov			arxiv	['Daniel Basilewitsch', 'Fernando Gago-Encinas', 'Matthias G. Krauss', 'Karl P. Horn', 'Daniel M. Reich', 'Christiane P. Koch']	503.0
888	OPFython: A Python-Inspired Optimum-Path Forest Classifier	Gustavo Henrique de Rosa	2020-01-28 15:46:19	http://arxiv.org/abs/2001.10420v3	Machine learning techniques have been paramount throughout the last years, being applied in a wide range of tasks, such as classification, object recognition, person identification, and image segmentation. Nevertheless, conventional classification algorithms, e.g., Logistic Regression, Decision Trees, and Bayesian classifiers, might lack complexity and diversity, not suitable when dealing with real-world data. A recent graph-inspired classifier, known as the Optimum-Path Forest, has proven to be a state-of-the-art technique, comparable to Support Vector Machines and even surpassing it in some tasks. This paper proposes a Python-based Optimum-Path Forest framework, denoted as OPFython, where all of its functions and classes are based upon the original C language implementation. Additionally, as OPFython is a Python-based library, it provides a more friendly environment and a faster prototyping workspace than the C language.			arxiv	['Jo√£o Paulo Papa', 'Alexandre Xavier Falc√£o']	504.0
889	Basic Data Analysis and More - A Guided Tour Using Python	O. Melchert	2012-07-25 14:20:08	http://arxiv.org/abs/1207.6002v1	In these lecture notes, a selection of frequently required statistical tools will be introduced and illustrated. They allow to post-process data that stem from, e.g., large-scale numerical simulations (aka sequence of random experiments). From a point of view of data analysis, the concepts and techniques introduced here are of general interest and are, at best, employed by computational aid. Consequently, an exemplary implementation of the presented techniques using the Python programming language is provided. The contents of these lecture notes is rather selective and represents a computational experimentalist's view on the subject of basic data analysis, ranging from the simple computation of moments for distributions of random variables to more involved topics such as hierarchical cluster analysis and the parallelization of Python code.			arxiv	[]	505.0
890	Mahotas: Open source software for scriptable computer vision	Luis Pedro Coelho	2012-11-21 00:51:10	http://arxiv.org/abs/1211.4907v2	Mahotas is a computer vision library for Python. It contains traditional image processing functionality such as filtering and morphological operations as well as more modern computer vision functions for feature computation, including interest point detection and local descriptors. The interface is in Python, a dynamic programming language, which is very appropriate for fast development, but the algorithms are implemented in C++ and are tuned for speed. The library is designed to fit in with the scientific software ecosystem in this language and can leverage the existing infrastructure developed in that language. Mahotas is released under a liberal open source license (MIT License) and is available from (http://github.com/luispedro/mahotas) and from the Python Package Index (http://pypi.python.org/pypi/mahotas).			arxiv	[]	506.0
891	QuTiP 2: A Python framework for the dynamics of open quantum systems	J. R. Johansson	2012-11-28 04:48:22	http://arxiv.org/abs/1211.6518v1	We present version 2 of QuTiP, the Quantum Toolbox in Python. Compared to the preceding version [Comput. Phys. Comm. 183 (2012) 1760], we have introduced numerous new features, enhanced performance, made changes in the Application Programming Interface (API) for improved functionality and consistency within the package, as well as increased compatibility with existing conventions used in other scientific software packages for Python. The most significant new features include efficient solvers for arbitrary time-dependent Hamiltonians and collapse operators, support for the Floquet formalism, and new solvers for Bloch-Redfield and Floquet-Markov master equations. Here we introduce these new features, demonstrate their use, and give a summary of the important backward-incompatible API changes introduced in this version.			arxiv	['P. D. Nation', 'Franco Nori']	507.0
892	Fermipy: An open-source Python package for analysis of Fermi-LAT Data	Matthew Wood	2017-07-29 19:34:22	http://arxiv.org/abs/1707.09551v1	Fermipy is an open-source python framework that facilitates analysis of data collected by the Fermi Large Area Telescope (LAT). Fermipy is built on the Fermi Science Tools, the publicly available software suite provided by NASA for the LAT mission. Fermipy provides a high-level interface for analyzing LAT data in a simple and reproducible way. The current feature set includes methods for extracting spectral energy distributions and lightcurves, generating test statistic maps, finding new source candidates, and fitting source position and extension. Fermipy leverages functionality from other scientific python packages including NumPy, SciPy, Matplotlib, and Astropy and is organized as a community-developed package following an open-source development model. We review the current functionality of Fermipy and plans for future development.			arxiv	['Regina Caputo', 'Eric Charles', 'Mattia Di Mauro', 'Jeffrey Magill', 'Jeremy Perkins']	508.0
893	pschitt! - A Python package for the modelling of atmoSpheric Showers and CHerenkov Imaging Terrestrial Telescopes	Thomas Vuillaume	2017-09-14 09:12:12	http://arxiv.org/abs/1709.04675v1	The simulation of atmospheric showers through Monte-Carlo processes as well as their projection into Imaging Atmospheric Cherenkov Telescopes (IACT) is long and very computing intensive. As these simulations are the most advanced ones from a physics point of view, they are not suited for simple tests. Here we present a Python package developed in order to model atmospheric showers using different profiles and to image them with an array of IACT. This allows for first order studies of the influence of the primary photon energy and angular direction on the stereoscopic images. Its simplicity also makes it convenient for public dissemination and outreach as well as for teaching purposes. This package has been developed to make the most out of the simplicity of Python but has also been optimised for fast calculations. It is developed in the framework of the ASTERICS H2020 project and as such is available as an open-source software.			arxiv	['Florian Gat√©', 'Gilles Maurin', 'Jean Jacquemier', 'Giovanni Lamanna']	509.0
894	Python Framework for HP Adaptive Discontinuous Galerkin Method for Two Phase Flow in Porous Media	Andreas Dedner	2018-05-01 12:30:23	http://arxiv.org/abs/1805.00290v1	In this paper we present a framework for solving two phase flow problems in porous media. The discretization is based on a Discontinuous Galerkin method and includes local grid adaptivity and local choice of polynomial degree. The method is implemented using the new Python frontend Dune-FemPy to the open source framework Dune. The code used for the simulations is made available as Jupyter notebook and can be used through a Docker container. We present a number of time stepping approaches ranging from a classical IMPES method to fully coupled implicit scheme. The implementation of the discretization is very flexible allowing for test different formulations of the two phase flow model and adaptation strategies.			arxiv	['Birane Kane', 'Robert Kl√∂fkorn', 'Martin Nolte']	510.0
895	Spectrum management and compatibility studies with Python	B. Winkel	2018-05-29 13:27:42	http://arxiv.org/abs/1805.11434v1	We developed the pycraf Python package, which provides functions and procedures for various tasks related to spectrum-management compatibility studies. This includes an implementation of ITU-R Rec. P.452, which allows to calculate the path attenuation arising from the distance and terrain properties between an interferer and the victim service. A typical example would be the calculation of interference levels at a radio telescope produced from a radio broadcasting tower. Furthermore, pycraf provides functionality to calculate atmospheric attenuation as proposed in ITU-R Rec. P.676. Using the rich ecosystem of scientific Python libraries and our pycraf package, we performed a large number of compatibility studies. Here, we will highlight a recent case study, where we analysed the potential harm that the next-generation cell-phone standard 5G could bring to observations at a radio observatory. For this we implemented a Monte-Carlo simulation to deal with the quasi-statistical spatial distribution of base stations and user devices around the radio astronomy station.			arxiv	['A. Jessner']	511.0
896	TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning	Akshay Agrawal	2019-02-27 03:08:20	http://arxiv.org/abs/1903.01855v1	TensorFlow Eager is a multi-stage, Python-embedded domain-specific language for hardware-accelerated machine learning, suitable for both interactive research and production. TensorFlow, which TensorFlow Eager extends, requires users to represent computations as dataflow graphs; this permits compiler optimizations and simplifies deployment but hinders rapid prototyping and run-time dynamism. TensorFlow Eager eliminates these usability costs without sacrificing the benefits furnished by graphs: It provides an imperative front-end to TensorFlow that executes operations immediately and a JIT tracer that translates Python functions composed of TensorFlow operations into executable dataflow graphs. TensorFlow Eager thus offers a multi-stage programming model that makes it easy to interpolate between imperative and staged execution in a single package.			arxiv	['Akshay Naresh Modi', 'Alexandre Passos', 'Allen Lavoie', 'Ashish Agarwal', 'Asim Shankar', 'Igor Ganichev', 'Josh Levenberg', 'Mingsheng Hong', 'Rajat Monga', 'Shanqing Cai']	512.0
897	AlphaTwirl: A Python library for summarizing event data into multivariate categorical data	Tai Sakuma	2019-05-16 09:13:38	http://arxiv.org/abs/1905.06609v1	AlphaTwirl is a Python library that summarizes large event data into multivariate categorical data, which can be regarded as generalizations of histograms. The output can be imported as data frames in R and pandas. With their rich set of data wrangling tools, users can develop flexible and configurable analysis code. The multivariate categorical data loaded as data frames are readily visualized by graphic tools available in R and Python. AlphaTwirl can process event data concurrently with multiple cores or batch systems. Users can extend and customize nearly any functionality of AlphaTwirl with reusable code. AlphaTwirl is released under the BSD license.			arxiv	[]	513.0
898	PymePix: A python library for SPIDR readout of Timepix3	Ahmed Al-Refaie	2019-05-20 11:18:58	http://arxiv.org/abs/1905.07999v3	PymePix is a new Python 3 library that provides control and acquisition for the Timepix3-SPIDR hardware. The rich set of data-structures and intuitive routines reduces time and coding effort to quickly configure, acquire, and visualize data from Timepix3. The highly extensible high-performance data-pipeline allows for alteration of the Timepix3 datastream into a form that is convinient for the user. This library is intended to be easily inserted into a standard scientific software stack as well as to allow for more direct interaction of Timepix3 with interactive flavors of Python. Included with the library are two example programs using PymePix: pymepix-acq is a command line control and acquisition program that can capture UDP packets and decode them into pixels and triggers. The second is PymePix-Viewer, an online control and data-acquisition program for general use, but with features geared toward mass-spectroscopy and ion imaging.			arxiv	['Melby Johny', 'Jonathan Correa', 'David Pennicard', 'Peter Svihra', 'Andrei Nomerotski', 'Sebastian Trippel', 'Jochen K√ºpper']	514.0
899	Generation of Pseudo Code from the Python Source Code using Rule-Based Machine Translation	Sawan Rai	2019-06-14 10:35:56	http://arxiv.org/abs/1906.06117v3	Pseudo code is one of the valuable artifacts to comprehending the complex program codes. Most of the source code still has no equivalent pseudo code, due to the time-consuming process of writing pseudo codes. In this work, we have developed an approach to generate pseudo code from the python source code. In the first step, we convert python code into XML code for better code information extraction. Next, Important information extracted from the XML code, which later on used to generate actual pseudo code with the help of pseudo code templates. Initial performance results have been discussed in this paper.			arxiv	['Atul Gupta']	515.0
900	miniSAM: A Flexible Factor Graph Non-linear Least Squares Optimization Framework	Jing Dong	2019-09-03 00:51:29	http://arxiv.org/abs/1909.00903v1	Many problems in computer vision and robotics can be phrased as non-linear least squares optimization problems represented by factor graphs, for example, simultaneous localization and mapping (SLAM), structure from motion (SfM), motion planning, and control. We have developed an open-source C++/Python framework miniSAM, for solving such factor graph based least squares problems. Compared to most existing frameworks for least squares solvers, miniSAM has (1) full Python/NumPy API, which enables more agile development and easy binding with existing Python projects, and (2) a wide list of sparse linear solvers, including CUDA enabled sparse linear solvers. Our benchmarking results shows miniSAM offers comparable performances on various types of problems, with more flexible and smoother development experience.			arxiv	['Zhaoyang Lv']	516.0
901	Midi Miner -- A Python library for tonal tension and track classification	Rui Guo	2019-10-03 08:09:55	http://arxiv.org/abs/1910.02049v2	We present a Python library, called Midi Miner, that can calculate tonal tension and classify different tracks. MIDI (Music Instrument Digital Interface) is a hardware and software standard for communicating musical events between digital music devices. It is often used for tasks such as music representation, communication between devices, and even music generation [5]. Tension is an essential element of the music listening experience, which can come from a number of musical features including timbre, loudness and harmony [3]. Midi Miner provides a Python implementation for the tonal tension model based on the spiral array [1] as presented by Herremans and Chew [4]. Midi Miner also performs key estimation and includes a track classifier that can disentangle melody, bass, and harmony tracks. Even though tracks are often separated in MIDI files, the musical function of each track is not always clear. The track classifier keeps the identified tracks and discards messy tracks, which can enable further analysis and training tasks.			arxiv	['Dorien Herremans', 'Thor Magnusson']	517.0
902	TorchBeast: A PyTorch Platform for Distributed RL	Heinrich K√ºttler	2019-10-08 17:24:28	http://arxiv.org/abs/1910.03552v1	"TorchBeast is a platform for reinforcement learning (RL) research in PyTorch. It implements a version of the popular IMPALA algorithm for fast, asynchronous, parallel training of RL agents. Additionally, TorchBeast has simplicity as an explicit design goal: We provide both a pure-Python implementation (""MonoBeast"") as well as a multi-machine high-performance version (""PolyBeast""). In the latter, parts of the implementation are written in C++, but all parts pertaining to machine learning are kept in simple Python using PyTorch, with the environments provided using the OpenAI Gym interface. This enables researchers to conduct scalable RL research using TorchBeast without any programming knowledge beyond Python and PyTorch. In this paper, we describe the TorchBeast design principles and implementation and demonstrate that it performs on-par with IMPALA on Atari. TorchBeast is released as an open-source package under the Apache 2.0 license and is available at \url{https://github.com/facebookresearch/torchbeast}."			arxiv	['Nantas Nardelli', 'Thibaut Lavril', 'Marco Selvatici', 'Viswanath Sivakumar', 'Tim Rockt√§schel', 'Edward Grefenstette']	518.0
903	AOtools -- a Python package for adaptive optics modelling and analysis	M. J. Townson	2019-10-10 07:52:19	http://arxiv.org/abs/1910.04414v1	AOtools is a Python package which is open-source and aimed at providing tools for adaptive optics users and researchers. We present version 1.0 which contains tools for adaptive optics processing, including analysing data in the pupil plane, images and point spread functions in the focal plane, wavefront sensors, modelling of atmospheric turbulence, physical optical propagation of wavefronts, and conversion between frequently used adaptive optics and astronomical units. The main drivers behind AOtools is that it should be easy to install and use. To achieve this the project features extensive documentation, automated unit testing and is registered on the Python Package Index. AOtools is under continuous active development to expand the features available and we encourage everyone involved in adaptive optics to become involved and contribute to the project.			arxiv	['O. J. D. Farley', 'G. Orban de Xivry', 'J. Osborn', 'A. P. Reeves']	519.0
904	PySAP: Python Sparse Data Analysis Package for Multidisciplinary Image Processing	S. Farrens	2019-10-18 15:20:32	http://arxiv.org/abs/1910.08465v2	We present the open-source image processing software package PySAP (Python Sparse data Analysis Package) developed for the COmpressed Sensing for Magnetic resonance Imaging and Cosmology (COSMIC) project. This package provides a set of flexible tools that can be applied to a variety of compressed sensing and image reconstruction problems in various research domains. In particular, PySAP offers fast wavelet transforms and a range of integrated optimisation algorithms. In this paper we present the features available in PySAP and provide practical demonstrations on astrophysical and magnetic resonance imaging data.			arxiv	['A. Grigis', 'L. El Gueddari', 'Z. Ramzi', 'Chaithya G. R.', 'S. Starck', 'B. Sarthou', 'H. Cherkaoui', 'P. Ciuciu', 'J. -L. Starck']	520.0
905	XDeep: An Interpretation Tool for Deep Neural Networks	Fan Yang	2019-11-04 01:59:41	http://arxiv.org/abs/1911.01005v1	XDeep is an open-source Python package developed to interpret deep models for both practitioners and researchers. Overall, XDeep takes a trained deep neural network (DNN) as the input, and generates relevant interpretations as the output with the post-hoc manner. From the functionality perspective, XDeep integrates a wide range of interpretation algorithms from the state-of-the-arts, covering different types of methodologies, and is capable of providing both local explanation and global explanation for DNN when interpreting model behaviours. With the well-documented API designed in XDeep, end-users can easily obtain the interpretations for their deep models at hand with several lines of codes, and compare the results among different algorithms. XDeep is generally compatible with Python 3, and can be installed through Python Package Index (PyPI). The source codes are available at: https://github.com/datamllab/xdeep.			arxiv	['Zijian Zhang', 'Haofan Wang', 'Yuening Li', 'Xia Hu']	521.0
906	zksk: A Library for Composable Zero-Knowledge Proofs	Wouter Lueks	2019-11-06 16:25:45	http://arxiv.org/abs/1911.02459v2	Zero-knowledge proofs are an essential building block in many privacy-preserving systems. However, implementing these proofs is tedious and error-prone. In this paper, we present zksk, a well-documented Python library for defining and computing sigma protocols: the most popular class of zero-knowledge proofs. In zksk, proofs compose: programmers can convert smaller proofs into building blocks that then can be combined into bigger proofs. zksk features a modern Python-based domain-specific language. This makes possible to define proofs without learning a new custom language, and to benefit from the rich Python syntax and ecosystem. The library is available at https://github.com/spring-epfl/zksk			arxiv	['Bogdan Kulynych', 'Jules Fasquelle', 'Simon Le Bail-Collet', 'Carmela Troncoso']	522.0
907	BciPy: Brain-Computer Interface Software in Python	Tab Memmott	2020-02-16 18:36:43	http://arxiv.org/abs/2002.06642v1	There are high technological and software demands associated with conducting brain-computer interface (BCI) research. In order to accelerate the development and accessibility of BCI, it is worthwhile to focus on open-source and desired tooling. Python, a prominent computer language, has emerged as a language of choice for many research and engineering purposes. In this manuscript, we present BciPy, an open-source, Python-based software for conducting BCI research. It was developed with a focus on restoring communication using event-related potential (ERP) spelling interfaces, however, it may be used for other non-spelling and non-ERP BCI paradigms. Major modules in this system include support for data acquisition, data queries, stimuli presentation, signal processing, signal viewing and modeling, language modeling, task building, and a simple Graphical User Interface (GUI).			arxiv	['Aziz Ko√ßanaoƒüullarƒ±', 'Matthew Lawhead', 'Daniel Klee', 'Shiran Dudy', 'Melanie Fried-Oken', 'Barry Oken']	523.0
908	Who is the Centre of the Movie Universe? Using Python and NetworkX to Analyse the Social Network of Movie Stars	Rhyd Lewis	2020-02-26 09:56:23	http://arxiv.org/abs/2002.11103v1	"This paper provides the technical details of an article originally published in The Conversation in February 2020. The purpose is to use centrality measures to analyse the social network of movie stars and thereby identify the most ""important"" actors in the movie business. The analysis is presented in a step-by-step, tutorial-like fashion and makes use of the Python programming language together with the NetworkX library. It reveals that the most central actors in the network are those with lengthy acting careers, such as Christopher Lee, Nassar, Sukumari, Michael Caine, Om Puri, Jackie Chan, and Robert De Niro. We also present similar results for the movie releases of each decade. These indicate that the most central actors since the turn of the millennium include people like Angelina Jolie, Brahmanandam, Samuel L. Jackson, Nassar, and Ben Kingsley."			arxiv	[]	524.0
909	Stanza: A Python Natural Language Processing Toolkit for Many Human Languages	Peng Qi	2020-03-16 09:05:53	http://arxiv.org/abs/2003.07082v2	We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza.			arxiv	['Yuhao Zhang', 'Yuhui Zhang', 'Jason Bolton', 'Christopher D. Manning']	525.0
910	QRMine: A python package for triangulation in Grounded Theory	Bell Raj Eapen	2020-03-30 14:45:51	http://arxiv.org/abs/2003.13519v1	Grounded theory (GT) is a qualitative research method for building theory grounded in data. GT uses textual and numeric data and follows various stages of coding or tagging data for sense-making, such as open coding and selective coding. Machine Learning (ML) techniques, including natural language processing (NLP), can assist the researchers in the coding process. Triangulation is the process of combining various types of data. ML can facilitate deriving insights from numerical data for corroborating findings from the textual interview transcripts. We present an open-source python package (QRMine) that encapsulates various ML and NLP libraries to support coding and triangulation in GT. QRMine enables researchers to use these methods on their data with minimal effort. Researchers can install QRMine from the python package index (PyPI) and can contribute to its development. We believe that the concept of computational triangulation will make GT relevant in the realm of big data.			arxiv	['Norm Archer', 'Kamran Sartipi']	526.0
911	QSystem: bitwise representation for quantum circuit simulations	Evandro Chagas Ribeiro da Rosa	2020-04-07 17:35:15	http://arxiv.org/abs/2004.03560v1	We present QSystem, an open-source platform for the simulation of quantum circuits focused on bitwise operations on a Hashmap data structure storing quantum states and gates. QSystem is implemented in C++ and delivered as a Python module, taking advantage of the C++ performance and the Python dynamism. The simulators API is designed to be simple and intuitive, thus streamlining the simulation of a quantum circuit in Python. The current release has three distinct ways to represent the quantum state: vector, matrix, and the proposed bitwise. The latter constitutes our main results and is a new way to store and manipulate both states and operations which shows an exponential advantage with the amount of superposition in the systems state. We benchmark the bitwise representation against other simulators, namely Qiskit, Forest SDK QVM, and Cirq.			arxiv	['Bruno G. Taketani']	527.0
912	Digging Into MUD With Python: mudpy, bdata, and bfit	Derek Fujimoto	2020-04-22 05:04:04	http://arxiv.org/abs/2004.10395v1	Used to store the results of $\mu$SR measurements at TRIUMF, the Muon Data (MUD) file format serves as a useful and flexible scheme that is both lightweight and self-describing. The application programming interface (API) for these files is written in C and FORTRAN, languages not known for their ease of use. In contrast, Python is a language which emphasizes rapid prototyping and readability. This work describes three Python 3 packages to interface with MUD files and analyze their contents: mudpy, bdata, and bfit. The first enables easy access to the contents of any MUD file. The latter two are implemented specifically for the implanted-ion $\beta$-detected NMR ($\beta$-NMR) experiment at TRIUMF. These tools provide both an API and graphical user interface (GUI) to help users extract and fit $\beta$-NMR data.			arxiv	[]	528.0
913	mvlearn: Multiview Machine Learning in Python	Ronan Perry	2020-05-25 02:35:35	http://arxiv.org/abs/2005.11890v4	As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have ballooned in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license. The documentation, detailed examples, and all releases are available at https://mvlearn.github.io/.			arxiv	['Gavin Mischler', 'Richard Guo', 'Theodore Lee', 'Alexander Chang', 'Arman Koul', 'Cameron Franz', 'Hugo Richard', 'Iain Carmichael', 'Pierre Ablin', 'Alexandre Gramfort', 'Joshua T. Vogelstein']	529.0
914	Kernel methods library for pattern analysis and machine learning in python	Pradeep Reddy Raamana	2020-05-27 16:44:42	http://arxiv.org/abs/2005.13483v1	Kernel methods have proven to be powerful techniques for pattern analysis and machine learning (ML) in a variety of domains. However, many of their original or advanced implementations remain in Matlab. With the incredible rise and adoption of Python in the ML and data science world, there is a clear need for a well-defined library that enables not only the use of popular kernels, but also allows easy definition of customized kernels to fine-tune them for diverse applications. The kernelmethods library fills that important void in the python ML ecosystem in a domain-agnostic fashion, allowing the sample data type to be anything from numerical, categorical, graphs or a combination of them. In addition, this library provides a number of well-defined classes to make various kernel-based operations efficient (for large scale datasets), modular (for ease of domain adaptation), and inter-operable (across different ecosystems). The library is available at https://github.com/raamana/kernelmethods.			arxiv	[]	530.0
915	DyPy: A Python Library for Simulating Matrix-Form Games	Anjalika Nande	2020-07-27 19:04:39	http://arxiv.org/abs/2007.13815v1	Evolutionary Game Theory (EGT) simulations are used to model populations undergoing biological and cultural evolution in a range of fields, from biology to economics to linguistics. In this paper we present DyPy, an open source Python package that can perform evolutionary simulations for any matrix form game for three common evolutionary dynamics: Moran, Wright-Fisher and Replicator. We discuss the basic components of this package and illustrate how it can be used to run a variety of simulations. Our package allows a user to run such simulations fairly easily without much prior Python knowledge. We hope that this will be a great asset to researchers in a number of different fields.			arxiv	['Andrew Ferdowsian', 'Eric Lubin', 'Erez Yoeli', 'Martin Nowak']	531.0
916	Biomedical and Clinical English Model Packages in the Stanza Python NLP Library	Yuhao Zhang	2020-07-29 07:27:41	http://arxiv.org/abs/2007.14640v1	We introduce biomedical and clinical English model packages for the Stanza Python NLP library. These packages offer accurate syntactic analysis and named entity recognition capabilities for biomedical and clinical text, by combining Stanza's fully neural architecture with a wide variety of open datasets as well as large-scale unsupervised biomedical and clinical text data. We show via extensive experiments that our packages achieve syntactic analysis and named entity recognition performance that is on par with or surpasses state-of-the-art results. We further show that these models do not compromise speed compared to existing toolkits when GPU acceleration is available, and are made easy to download and use with Stanza's Python interface. A demonstration of our packages is available at: http://stanza.run/bio.			arxiv	['Yuhui Zhang', 'Peng Qi', 'Christopher D. Manning', 'Curtis P. Langlotz']	532.0
917	Scikit-network: Graph Analysis in Python	Thomas Bonald	2020-09-14 12:03:01	http://arxiv.org/abs/2009.07660v1	Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online.			arxiv	['Nathan de Lara', 'Quentin Lutz', 'Bertrand Charpentier']	533.0
918	munuSSM: A python package for the $Œº$-from-$ŒΩ$ Supersymmetric Standard Model	Thomas Biek√∂tter	2020-09-27 16:07:36	http://arxiv.org/abs/2009.12887v2	We present the public python package munuSSM that can be used for phenomenological studies in the context of the $\mu$-from-$\nu$ Supersymmetric Standard Model ($\mu\nu$SSM). The code incorporates the radiative corrections to the neutral scalar potential at the full one-loop level. Sizable higher-order corrections, required for an accurate prediction of the SM-like Higgs-boson mass, can be consistently included via an automated link to the public code FeynHiggs. In addition, a calculation of effective couplings and branching ratios of the neutral and charged Higgs bosons is implemented. This provides the required ingredients to check a benchmark point against collider constraints from searches for additional Higgs bosons via an interface to the public code HiggsBounds. At the same time, the signal rates of the SM-like Higgs boson can be tested applying the experimental results implemented in the public code HiggsSignals. The python package is constructed in a flexible and modular way, such that it provides a simple framework that can be extended by the user with further calculations of observables and constraints on the model parameters.			arxiv	[]	534.0
919	DIETERpy: a Python framework for The Dispatch and Investment Evaluation Tool with Endogenous Renewables	Carlos Gaete-Morales	2020-10-02 09:27:33	http://arxiv.org/abs/2010.00883v2	DIETER is an open-source power sector model designed to analyze future settings with very high shares of variable renewable energy sources. It minimizes overall system costs, including fixed and variable costs of various generation, flexibility and sector coupling options. Here we introduce DIETERpy that builds on the existing model version, written in the General Algebraic Modeling System (GAMS), and enhances it with a Python framework. This combines the flexibility of Python regarding pre- and post-processing of data with a straightforward algebraic formulation in GAMS and the use of efficient solvers. DIETERpy also offers a browser-based graphical user interface. The new framework is designed to be easily accessible as it enables users to run the model, alter its configuration, and define numerous scenarios without a deeper knowledge of GAMS. Code, data, and manuals are available in public repositories under permissive licenses for transparency and reproducibility.			arxiv	['Martin Kittel', 'Alexander Roth', 'Wolf-Peter Schill']	535.0
920	TextAttack: Lessons learned in designing Python frameworks for NLP	John X. Morris	2020-10-05 00:23:00	http://arxiv.org/abs/2010.01724v1	TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.			arxiv	['Jin Yong Yoo', 'Yanjun Qi']	536.0
921	PySBD: Pragmatic Sentence Boundary Disambiguation	Nipun Sadvilkar	2020-10-19 16:56:03	http://arxiv.org/abs/2010.09657v1	In this paper, we present a rule-based sentence boundary disambiguation Python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the Golden Rules Set (a language-specific set of sentence boundary exemplars) originally implemented as a ruby gem - pragmatic_segmenter - which we ported to Python with additional improvements and functionality. PySBD passes 97.92% of the Golden Rule Set exemplars for English, an improvement of 25% over the next best open-source Python tool.			arxiv	['Mark Neumann']	537.0
922	MFST: A Python OpenFST Wrapper With Support for Custom Semirings and Jupyter Notebooks	Matthew Francis-Landau	2020-12-07 03:36:54	http://arxiv.org/abs/2012.03437v1	This paper introduces mFST, a new Python library for working with Finite-State Machines based on OpenFST. mFST is a thin wrapper for OpenFST and exposes all of OpenFST's methods for manipulating FSTs. Additionally, mFST is the only Python wrapper for OpenFST that exposes OpenFST's ability to define a custom semirings. This makes mFST ideal for developing models that involve learning the weights on a FST or creating neuralized FSTs. mFST has been designed to be easy to get started with and has been previously used in homework assignments for a NLP class as well in projects for integrating FSTs and neural networks. In this paper, we exhibit mFST API and how to use mFST to build a simple neuralized FST with PyTorch.			arxiv	[]	538.0
923	SWIGLAL: Python and Octave interfaces to the LALSuite gravitational-wave data analysis libraries	Karl Wette	2020-12-17 12:50:35	http://arxiv.org/abs/2012.09552v1	The LALSuite data analysis libraries, written in C, implement key routines critical to the successful detection of gravitational waves, such as the template waveforms describing the merger of two black holes or two neutron stars. SWIGLAL is a component of LALSuite which provides interfaces for Python and Octave, making LALSuite routines accessible directly from scripts written in those languages. It has enabled modern gravitational-wave data analysis software, used in the first detection of gravitational waves, to be written in Python, thereby benefiting from its ease of development and rich feature set, while still having access to the computational speed and scientific trustworthiness of the routines provided by LALSuite.			arxiv	[]	539.0
924	Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations	Jimmy Lin	2021-02-19 18:12:44	http://arxiv.org/abs/2102.10073v1	Pyserini is an easy-to-use Python toolkit that supports replicable IR research by providing effective first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. We also describe how our group has built a culture of replicability through shared norms and tools that enable rigorous automated testing.			arxiv	['Xueguang Ma', 'Sheng-Chieh Lin', 'Jheng-Hong Yang', 'Ronak Pradeep', 'Rodrigo Nogueira']	540.0
925	A Python toolbox for unbiased statistical analysis of fluorescence intermittency of multi-level emitters	Isabelle M. Palstra	2021-02-18 13:19:18	http://arxiv.org/abs/2102.11044v3	We report on a Python-toolbox for unbiased statistical analysis of fluorescence intermittency properties of single emitters. Intermittency, i.e., step-wise temporal variations in the instantaneous emission intensity and fluorescence decay rate properties are common to organic fluorophores, II-VI quantum dots and perovskite quantum dots alike. Unbiased statistical analysis of intermittency switching time distributions, involved levels and lifetimes is important to avoid interpretation artefacts. This work provides an implementation of Bayesian changepoint analysis and level clustering applicable to time-tagged single-photon detection data of single emitters that can be applied to real experimental data and as tool to verify the ramifications of hypothesized mechanistic intermittency models. We provide a detailed Monte Carlo analysis to illustrate these statistics tools, and to benchmark the extent to which conclusions can be drawn on the photophysics of highly complex systems, such as perovskite quantum dots that switch between a plethora of states instead of just two.			arxiv	['A. Femius Koenderink']	541.0
926	PHIDL: Python CAD layout and geometry creation for nanolithography	A. N. McCaughan	2021-03-01 17:44:03	http://arxiv.org/abs/2103.01152v1	Computer-aided design (CAD) has become a critical element in the creation of nanopatterned structures and devices. In particular, with the increased adoption of easy-to-learn programming languages like Python there has been a significant rise in the amount of lithographic geometries generated through scripting and programming. However, there are currently unaddressed gaps in usability for open-source CAD tools -- especially those in the GDSII design space -- that prevent wider adoption by scientists and students who might otherwise benefit from scripted design. For example, constructing relations between adjacent geometries is often much more difficult than necessary -- spacing a resonator structure a few micrometers from a readout structure often requires manually-coding the placement arithmetic. While inconveniences like this can be overcome by writing custom functions, they are often significant barriers to entry for new users or those less familiar with programming. To help streamline the design process and reduce barrier to entry for scripting designs, we have developed PHIDL, an open-source GDSII-based CAD tool for Python 2 and 3.			arxiv	['A. M. Tait', 'S. M. Buckley', 'D. M. Oh', 'J. T. Chiles', 'J. M. Shainline', 'S. W. Nam']	542.0
927	DoubleML -- An Object-Oriented Implementation of Double Machine Learning in Python	Philipp Bach	2021-04-07 16:16:39	http://arxiv.org/abs/2104.03220v2	DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.			arxiv	['Victor Chernozhukov', 'Malte S. Kurz', 'Martin Spindler']	543.0
928	A python package for ultrashort optical pulse propagation in terms of forward models for the analytic signal	O. Melchert	2021-04-23 14:58:26	http://arxiv.org/abs/2104.11649v1	We present a flexible, open-source Python package for the accurate simulation of the $z$-propagation dynamics of ultrashort optical pulses in nonlinear waveguides, especially valid for few-cycle pulses and their interaction. The simulation approach is based on unidirectional propagation equations for the analytic signal. The provided software allows to account for dispersion, attenuation, four-wave mixing processes including, e.g., third-harmonic generation, and features various models for the Raman response. The propagation equations are solved on a periodic temporal domain. For $z$-propagation, a selection of pseudospectral methods is available. Propagation scenarios for a custom propagation constant and initial field pulses can either be specified in terms of a HDF5 based input file format or by direct implementation using a python script. We demonstrate the functionality for a test-case for which an exact solution is available, by reproducing exemplary results documented in the scientific literature, and a complex propagation scenario involving multiple pulses. The py-fmas code, its reference manual, an extended user guide, and further usage examples are available online at https://github.com/omelchert/py-fmas.			arxiv	['A. Demircan']	544.0
929	PyMatching: A Python package for decoding quantum codes with minimum-weight perfect matching	Oscar Higgott	2021-05-27 12:10:37	http://arxiv.org/abs/2105.13082v2	This paper introduces PyMatching, a fast open-source Python package for decoding quantum error-correcting codes with the minimum-weight perfect matching (MWPM) algorithm. PyMatching includes the standard MWPM decoder as well as a variant, which we call local matching, that restricts each syndrome defect to be matched to another defect within a local neighbourhood. The decoding performance of local matching is almost identical to that of the standard MWPM decoder in practice, while reducing the computational complexity approximately quadratically. We benchmark the performance of PyMatching, showing that local matching is several orders of magnitude faster than implementations of the full MWPM algorithm using NetworkX or Blossom V for problem sizes typically considered in error correction simulations. PyMatching and its dependencies are open-source, and it can be used to decode any quantum code for which syndrome defects come in pairs using a simple Python interface. PyMatching supports the use of weighted edges, hook errors, boundaries and measurement errors, enabling fast decoding and simulation of fault-tolerant quantum computing.			arxiv	[]	545.0
930	Sarkas: A Fast Pure-Python Molecular Dynamics Suite for Plasma Physics	Luciano G. Silvestri	2021-05-31 13:41:03	http://arxiv.org/abs/2105.14955v1	We present an open-source, performant, pure-python molecular dynamics (MD) suite for non-ideal plasmas. The code, Sarkas, aims to accelerate the research process by providing an MD code but also pre- and post-processing tools. Sarkas offers the ease of use of Python while employing the Numba library to obtain execution speeds comparable to that of compiled languages. The available tools in Sarkas include graphical displays of the equilibration process through a Jupyter interface and the ability to compute quantities such as, radial distribution functions, autocorrelation functions and Green-Kubo relations. Many force laws used to simulate plasmas are included in Sarkas, namely, pure Coulomb, Yukawa, and Moli\`ere pair-potentials. Sarkas also contains quantum statistical potentials and fast Ewald methods are included where necessary. An object-oriented approach allows for easy modification of Sarkas, such as adding new time integrators, boundary conditions and force laws.			arxiv	['Lucas J. Stanek', 'Gautham Dharuman', 'Yongjun Choi', 'Michael S. Murillo']	546.0
931	pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks	Juan Manuel P√©rez	2021-06-17 13:15:07	http://arxiv.org/abs/2106.09462v2	In recent years, the extraction of opinions and information from user-generated text has attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinion mining and other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.			arxiv	['Mariela Rajngewerc', 'Juan Carlos Giudici', 'Dami√°n A. Furman', 'Franco Luque', 'Laura Alonso Alemany', 'Mar√≠a Vanina Mart√≠nez']	547.0
932	pyWATTS: Python Workflow Automation Tool for Time Series	Benedikt Heidrich	2021-06-18 14:50:11	http://arxiv.org/abs/2106.10157v1	Time series data are fundamental for a variety of applications, ranging from financial markets to energy systems. Due to their importance, the number and complexity of tools and methods used for time series analysis is constantly increasing. However, due to unclear APIs and a lack of documentation, researchers struggle to integrate them into their research projects and replicate results. Additionally, in time series analysis there exist many repetitive tasks, which are often re-implemented for each project, unnecessarily costing time. To solve these problems we present \texttt{pyWATTS}, an open-source Python-based package that is a non-sequential workflow automation tool for the analysis of time series data. pyWATTS includes modules with clearly defined interfaces to enable seamless integration of new or existing methods, subpipelining to easily reproduce repetitive tasks, load and save functionality to simply replicate results, and native support for key Python machine learning libraries such as scikit-learn, PyTorch, and Keras.			arxiv	['Andreas Bartschat', 'Marian Turowski', 'Oliver Neumann', 'Kaleb Phipps', 'Stefan Meisenbacher', 'Kai Schmieder', 'Nicole Ludwig', 'Ralf Mikut', 'Veit Hagenmeyer']	548.0
933	Speeding Up OPFython with Numba	Gustavo H. de Rosa	2021-06-22 14:39:32	http://arxiv.org/abs/2106.11828v1	"A graph-inspired classifier, known as Optimum-Path Forest (OPF), has proven to be a state-of-the-art algorithm comparable to Logistic Regressors, Support Vector Machines in a wide variety of tasks. Recently, its Python-based version, denoted as OPFython, has been proposed to provide a more friendly framework and a faster prototyping environment. Nevertheless, Python-based algorithms are slower than their counterpart C-based algorithms, impacting their performance when confronted with large amounts of data. Therefore, this paper proposed a simple yet highly efficient speed up using the Numba package, which accelerates Numpy-based calculations and attempts to increase the algorithm's overall performance. Experimental results showed that the proposed approach achieved better results than the na\""ive Python-based OPF and speeded up its distance measurement calculation."			arxiv	['Jo√£o Paulo Papa']	549.0
934	Pytearcat: PYthon TEnsor AlgebRa calCulATor	Marco San Mart√≠n	2021-06-28 22:58:22	http://arxiv.org/abs/2106.15016v3	This paper introduces the first release of Pytearcat, a Python package developed to compute tensor algebra operations in the context of theoretical physics, for instance, in general relativity. Given that working with tensors can become a complex task, people often rely on computational tools to perform tensor calculations. We aim to build a tensor calculator based on Python, which benefits from being free and easy to use. Pytearcat syntax resembles the usual physics notation for tensor calculus, such as the Einstein notation for index contraction. This version allows the user to perform many tensor operations, including derivatives and series expansions, along with routines to obtain the typical General Relativity tensors. A particular concern was put in the execution times, leading to incorporate an alternative core for the symbolic calculations, enabling to reach much faster execution times. The syntax and the versatility of Pytearcat are the most important features of this package, where the latter can be used to extend Pytearcat to other areas of theoretical physics.			arxiv	['Joaquin Sureda']	550.0
935	Algorithmic Causal Effect Identification with causaleffect	Mart√≠ Pedemonte	2021-07-09 19:00:33	http://arxiv.org/abs/2107.04632v1	Our evolution as a species made a huge step forward when we understood the relationships between causes and effects. These associations may be trivial for some events, but they are not in complex scenarios. To rigorously prove that some occurrences are caused by others, causal theory and causal inference were formalized, introducing the $do$-operator and its associated rules. The main goal of this report is to review and implement in Python some algorithms to compute conditional and non-conditional causal queries from observational data. To this end, we first present some basic background knowledge on probability and graph theory, before introducing important results on causal theory, used in the construction of the algorithms. We then thoroughly study the identification algorithms presented by Shpitser and Pearl in 2006, explaining our implementation in Python alongside. The main identification algorithm can be seen as a repeated application of the rules of $do$-calculus, and it eventually either returns an expression for the causal query from experimental probabilities or fails to identify the causal effect, in which case the effect is non-identifiable. We introduce our newly developed Python library and give some usage examples.			arxiv	['Jordi Vitri√†', '√Ålvaro Parafita']	551.0
936	PyCCE: A Python Package for Cluster Correlation Expansion Simulations of Spin Qubit Dynamic	Mykyta Onizhuk	2021-07-13 04:49:49	http://arxiv.org/abs/2107.05843v1	We present PyCCE, an open-source Python library to simulate the dynamics of spin qubits in a spin bath, using the cluster-correlation expansion (CCE) method. PyCCE includes modules to generate realistic spin baths, employing coupling parameters computed from first principles with electronic structure codes, and enables the user to run simulations with either the conventional or generalized CCE method. We illustrate three use cases of the Python library: the calculation of the Hahn-echo coherence time of the nitrogen vacancy in diamond; the calculation of the coherence time of the basal divacancy in silicon carbide at avoided crossings; and the magnetic field orientation-dependent dynamics of a shallow donor in silicon. The complete documentation and installation instructions are available at https://pycce.readthedocs.io/en/latest/.			arxiv	['Giulia Galli']	552.0
937	LUCI: A Python package for SITELLE spectral analysis	Carter Lee Rhea	2021-08-27 14:03:39	http://arxiv.org/abs/2108.12428v1	High-resolution optical integral field units (IFUs) are rapidly expanding our knowledge of extragalactic emission nebulae in galaxies and galaxy clusters. By studying the spectra of these objects -- which include classic HII regions, supernova remnants, planetary nebulae, and cluster filaments -- we are able to constrain their kinematics (velocity and velocity dispersion). In conjunction with additional tools, such as the BPT diagram, we can further classify emission regions based on strong emission-line flux ratios. LUCI is a simple-to-use python module intended to facilitate the rapid analysis of IFU spectra. LUCI does this by integrating well-developed pre-existing python tools such as astropy and scipy with new machine learning tools for spectral analysis (Rhea et al. 2020). Furthermore, LUCI provides several easy-to-use tools to access and fit SITELLE data cubes.			arxiv	['Julie Hlavacek-Larrondo', 'Laurie Rousseau-Nepton', 'Benjamin Vigneron', 'Louis-Simon Guit√©']	553.0
938	PTRAIL -- A python package for parallel trajectory data preprocessing	Salman Haidri	2021-08-26 20:14:07	http://arxiv.org/abs/2108.13202v1	Trajectory data represent a trace of an object that changes its position in space over time. This kind of data is complex to handle and analyze, since it is generally produced in huge quantities, often prone to errors generated by the geolocation device, human mishandling, or area coverage limitation. Therefore, there is a need for software specifically tailored to preprocess trajectory data. In this work we propose PTRAIL, a python package offering several trajectory preprocessing steps, including filtering, feature extraction, and interpolation. PTRAIL uses parallel computation and vectorization, being suitable for large datasets and fast compared to other python libraries.			arxiv	['Yaksh J. Haranwala', 'Vania Bogorny', 'Chiara Renso', 'Vinicius Prado da Fonseca', 'Amilcar Soares']	554.0
939	Scikit-dimension: a Python package for intrinsic dimension estimation	Jonathan Bac	2021-09-06 16:46:38	http://arxiv.org/abs/2109.02596v1	Dealing with uncertainty in applications of machine learning to real-life data critically depends on the knowledge of intrinsic dimensionality (ID). A number of methods have been suggested for the purpose of estimating ID, but no standard package to easily apply them one by one or all at once has been implemented in Python. This technical note introduces \texttt{scikit-dimension}, an open-source Python package for intrinsic dimension estimation. \texttt{scikit-dimension} package provides a uniform implementation of most of the known ID estimators based on scikit-learn application programming interface to evaluate global and local intrinsic dimension, as well as generators of synthetic toy and benchmark datasets widespread in the literature. The package is developed with tools assessing the code quality, coverage, unit testing and continuous integration. We briefly describe the package and demonstrate its use in a large-scale (more than 500 datasets) benchmarking of methods for ID estimation in real-life and synthetic data. The source code is available from https://github.com/j-bac/scikit-dimension , the documentation is available from https://scikit-dimension.readthedocs.io .			arxiv	['Evgeny M. Mirkes', 'Alexander N. Gorban', 'Ivan Tyukin', 'Andrei Zinovyev']	555.0
940	gnlse-python: Open Source Software to Simulate Nonlinear Light Propagation In Optical Fibers	Pawe≈Ç Redman	2021-10-01 10:06:48	http://arxiv.org/abs/2110.00298v1	The propagation of pulses in optical fibers is described by the generalized nonlinear Schrodinger equation (GNLSE), which takes into account the fiber losses, nonlinear effects, and higher-order chromatic dispersion. The GNLSE is a partial differential equation, whose order depends on the accounted nonlinear and dispersion effects. We present gnlse-python, a nonlinear optics modeling toolbox that contains a rich set of components and modules to solve the GNLSE using the split-step Fourier transform method (SSFM). The numerical solver is freely available, implemented in Python language, and includes a number of optical fiber analysis tools. Code and data are available at https://github.com/WUST-FOG/gnlse-python.			arxiv	['Magdalena Zatorska', 'Adam Paw≈Çowski', 'Daniel Szulc', 'Sylwia Majchrowska', 'Karol Tarnowski']	556.0
941	PyTorrent: A Python Library Corpus for Large-scale Language Models	Mehdi Bahrami	2021-10-04 20:48:31	http://arxiv.org/abs/2110.01710v1	A large scale collection of both semantic and natural language resources is essential to leverage active Software Engineering research areas such as code reuse and code comprehensibility. Existing machine learning models ingest data from Open Source repositories (like GitHub projects) and forum discussions (like Stackoverflow.com), whereas, in this showcase, we took a step backward to orchestrate a corpus titled PyTorrent that contains 218,814 Python package libraries from PyPI and Anaconda environment. This is because earlier studies have shown that much of the code is redundant and Python packages from these environments are better in quality and are well-documented. PyTorrent enables users (such as data scientists, students, etc.) to build off the shelf machine learning models directly without spending months of effort on large infrastructure. The dataset, schema and a pretrained language model is available at: https://github.com/fla-sil/PyTorrent			arxiv	['N. C. Shrikanth', 'Shade Ruangwan', 'Lei Liu', 'Yuji Mizobuchi', 'Masahiro Fukuyori', 'Wei-Peng Chen', 'Kazuki Munakata', 'Tim Menzies']	557.0
942	abess: A Fast Best Subset Selection Library in Python and R	Jin Zhu	2021-10-19 02:34:55	http://arxiv.org/abs/2110.09697v2	We introduce a new library named abess that implements a unified framework of best-subset selection for solving diverse machine learning problems, e.g., linear regression, classification, and principal component analysis. Particularly, the abess certifiably gets the optimal solution within polynomial times with high probability under the linear model. Our efficient implementation allows abess to attain the solution of best-subset selection problems as fast as or even 20x faster than existing competing variable (model) selection toolboxes. Furthermore, it supports common variants like best group subset selection and $\ell_2$ regularized best-subset selection. The core of the library is programmed in C++. For ease of use, a Python library is designed for conveniently integrating with scikit-learn, and it can be installed from the Python library Index. In addition, a user-friendly R library is available at the Comprehensive R Archive Network. The source code is available at: https://github.com/abess-team/abess.			arxiv	['Xueqin Wang', 'Liyuan Hu', 'Junhao Huang', 'Kangkang Jiang', 'Yanhang Zhang', 'Shiyun Lin', 'Junxian Zhu']	558.0
943	Distill: Domain-Specific Compilation for Cognitive Models	Jan Vesely	2021-10-28 20:27:22	http://arxiv.org/abs/2110.15425v2	This paper discusses our proposal and implementation of Distill, a domain-specific compilation tool based on LLVM to accelerate cognitive models. Cognitive models explain the process of cognitive function and offer a path to human-like artificial intelligence. However, cognitive modeling is laborious, requiring composition of many types of computational tasks, and suffers from poor performance as it relies on high-level languages like Python. In order to continue enjoying the flexibility of Python while achieving high performance, Distill uses domain-specific knowledge to compile Python-based cognitive models into LLVM IR, carefully stripping away features like dynamic typing and memory management that add overheads to the actual model. As we show, this permits significantly faster model execution. We also show that the code so generated enables using classical compiler data flow analysis passes to reveal properties about data flow in cognitive models that are useful to cognitive scientists. Distill is publicly available, is being used by researchers in cognitive science, and has led to patches that are currently being evaluated for integration into mainline LLVM.			arxiv	['Raghavendra Pradyumna Pothukuchi', 'Ketaki Joshi', 'Samyak Gupta', 'Jonathan D. Cohen', 'Abhishek Bhattacharjee']	559.0
944	IMBENS: Ensemble Class-imbalanced Learning in Python	Zhining Liu	2021-11-24 20:14:20	http://arxiv.org/abs/2111.12776v2	imbalanced-ensemble, abbreviated as imbens, is an open-source Python toolbox for leveraging the power of ensemble learning to address the class imbalance problem. It provides standard implementations of popular ensemble imbalanced learning (EIL) methods with extended features and utility functions. These ensemble methods include resampling-based, e.g., under/over-sampling, and reweighting-based, e.g., cost-sensitive learning. Beyond the implementation, we empower EIL algorithms with new functionalities like customizable resampling scheduler and verbose logging, thus enabling more flexible training and evaluating strategies. The package was developed under a simple, well-documented API design that follows scikit-learn for increased ease of use. imbens is released under the MIT open-source license and can be installed from Python Package Index (PyPI) or https://github.com/ZhiningLiu1998/imbalanced-ensemble.			arxiv	['Jian Kang', 'Hanghang Tong', 'Yi Chang']	560.0
945	Shennong: a Python toolbox for audio speech features extraction	Mathieu Bernard	2021-12-10 14:08:52	http://arxiv.org/abs/2112.05555v1	We introduce Shennong, a Python toolbox and command-line utility for speech features extraction. It implements a wide range of well-established state of art algorithms including spectro-temporal filters such as Mel-Frequency Cepstral Filterbanks or Predictive Linear Filters, pre-trained neural networks, pitch estimators as well as speaker normalization methods and post-processing algorithms. Shennong is an open source, easy-to-use, reliable and extensible framework. The use of Python makes the integration to others speech modeling and machine learning tools easy. It aims to replace or complement several heterogeneous software, such as Kaldi or Praat. After describing the Shennong software architecture, its core components and implemented algorithms, this paper illustrates its use on three applications: a comparison of speech features performances on a phones discrimination task, an analysis of a Vocal Tract Length Normalization model as a function of the speech duration used for training and a comparison of pitch estimation algorithms under various noise conditions.			arxiv	['Maxime Poli', 'Julien Karadayi', 'Emmanuel Dupoux']	561.0
946	Torch.fx: Practical Program Capture and Transformation for Deep Learning in Python	James K. Reed	2021-12-15 19:16:29	http://arxiv.org/abs/2112.08429v2	Modern deep learning frameworks provide imperative, eager execution programming interfaces embedded in Python to provide a productive development experience. However, deep learning practitioners sometimes need to capture and transform program structure for performance optimization, visualization, analysis, and hardware integration. We study the different designs for program capture and transformation used in deep learning. By designing for typical deep learning use cases rather than long tail ones, it is possible to create a simpler framework for program capture and transformation. We apply this principle in torch.fx, a program capture and transformation library for PyTorch written entirely in Python and optimized for high developer productivity by ML practitioners. We present case studies showing how torch.fx enables workflows previously inaccessible in the PyTorch ecosystem.			arxiv	['Zachary DeVito', 'Horace He', 'Ansley Ussery', 'Jason Ansel']	562.0
947	SimPlot++: a Python application for representing sequence similarity and detecting recombination	St√©phane Samson	2021-12-17 20:34:00	http://arxiv.org/abs/2112.09755v2	Motivation: Accurate detection of sequence similarity and homologous recombination are essential parts of many evolutionary analyses. Results: We have developed SimPlot++, an open-source multiplatform application implemented in Python, which can be used to produce publication quality sequence similarity plots using 63 nucleotide and 20 amino acid distance models, to detect intergenic and intragenic recombination events using Phi, Max-X2, NSS or proportion tests, and to generate and analyze interactive sequence similarity networks. SimPlot++ supports multicore data processing and provides useful distance calculability diagnostics. Availability: SimPlot++ is freely available on GitHub at: https://github.com/Stephane-S/Simplot_PlusPlus, as both an executable file (for Windows) and Python scripts (for Windows/Linux/MacOS).			arxiv	['√âtienne Lord', 'Vladimir Makarenkov']	563.0
948	AmPyfier: Test Amplification in Python	Ebert Schoofs	2021-12-21 12:50:18	http://arxiv.org/abs/2112.11155v1	Test Amplification is a method to extend handwritten tests into a more rigorous test suite covering corner cases in the system under test. Unfortunately, the current state-of-the-art for test amplification heavily relies on program analysis techniques which benefit a lot from explicit type declarations present in statically typed languages like Java and C++. In dynamically typed languages, such type declarations are not available and as a consequence test amplification has yet to find its way to programming languages like Python, Ruby and Javascript. In this paper, we present AmPyfier, a proof-of-concept tool, which brings test amplification to the dynamically typed, interpreted language Python. We evaluated this approach on 7 open-source projects, and found that AmPyfier could successfully strengthen 7 out of 10 test classes (70%). As such we demonstrate that test amplification is feasible for one of the most popular programming languages in use today.			arxiv	['Mehrdad Abdi', 'Serge Demeyer']	564.0
949	PyCIL: A Python Toolbox for Class-Incremental Learning	Da-Wei Zhou	2021-12-23 13:41:24	http://arxiv.org/abs/2112.12533v2	Traditional machine learning systems are deployed under the closed-world setting, which requires the entire training data before the offline training process. However, real-world applications often face the incoming new classes, and a model should incorporate them continually. The learning paradigm is called Class-Incremental Learning (CIL). We propose a Python toolbox that implements several key algorithms for class-incremental learning to ease the burden of researchers in the machine learning community. The toolbox contains implementations of a number of founding works of CIL such as EWC and iCaRL, but also provides current state-of-the-art algorithms that can be used for conducting novel fundamental research. This toolbox, named PyCIL for Python Class-Incremental Learning, is available at https://github.com/G-U-N/PyCIL			arxiv	['Fu-Yun Wang', 'Han-Jia Ye', 'De-Chuan Zhan']	565.0
950	Lyncs-API: a Python API for Lattice QCD applications	Simone Bacchio	2022-01-11 10:31:34	http://arxiv.org/abs/2201.03873v1	We present Lyncs-API, a Python API for Lattice QCD applications currently under development. Lyncs aims to bring several widely used libraries for Lattice QCD under a common framework. Lyncs flexibly links to libraries for CPUs and GPUs in a way that can accommodate additional computing architectures as these arise, achieving performance-portability for the calculations while maintaining the same high-level workflow. Lyncs distributes calculations using Dask and mpi4py, with bindings to the libraries automatically generated by cppyy. While Lyncs is designed to allow linking to multiple libraries, we focus on a set of targeted packages that include DDalphaAMG, tmLQCD, QUDA and c-lime. More libraries will be added in the future. We also develop generic-purpose tools for facilitating the usage of Python in Lattice QCD and HPC in general. The project is open-source, community-oriented and available on Github.			arxiv	['Jacob Finkenrath', 'Christodoulos Stylianou']	566.0
951	${\tt simwave}$ -- A Finite Difference Simulator for Acoustic Waves Propagation	Jaime Freire de Souza	2022-01-14 02:21:49	http://arxiv.org/abs/2201.05278v1	${\tt simwave}$ is an open-source Python package to perform wave simulations in 2D or 3D domains. It solves the constant and variable density acoustic wave equation with the finite difference method and has support for domain truncation techniques, several boundary conditions, and the modeling of sources and receivers given a user-defined acquisition geometry. The architecture of ${\tt simwave}$ is designed for applications with geophysical exploration in mind. Its Python front-end enables straightforward integration with many existing Python scientific libraries for the composition of more complex workflows and applications (e.g., migration and inversion problems). The back-end is implemented in C enabling performance portability across a range of computing hardware and compilers including both CPUs and GPUs.			arxiv	['Jo√£o Baptista Dias Moreira', 'Keith Jared Roberts', 'Roussian di Ramos Alves Gaioso', 'Edson Satoshi Gomi', 'Em√≠lio Carlos Nelli Silva', 'Hermes Senger']	567.0
952	PyGOD: A Python Library for Graph Outlier Detection	Kay Liu	2022-04-26 06:15:21	http://arxiv.org/abs/2204.12095v1	PyGOD is an open-source Python library for detecting outliers on graph data. As the first comprehensive library of its kind, PyGOD supports a wide array of leading graph-based methods for node-, edge-, subgraph-, and graph-level outlier detection, under a unified, well-documented API designed for use by both researchers and practitioners. To overcome the scalability issue in large graphs, we provide advanced functionalities for selected models, including mini-batch and sampling. PyGOD is equipped with best practices to foster code reliability and maintainability, including unit testing, continuous integration, and code coverage. To foster accessibility, PyGOD is released under a permissive BSD-license at https://github.com/pygod-team/pygod/ and the Python Package Index (PyPI).			arxiv	['Yingtong Dou', 'Yue Zhao', 'Xueying Ding', 'Xiyang Hu', 'Ruitong Zhang', 'Kaize Ding', 'Canyu Chen', 'Hao Peng', 'Kai Shu', 'George H. Chen', 'Zhihao Jia', 'Philip S. Yu']	568.0
953	Extended Abstract: Productive Parallel Programming with Parsl	Kyle Chard	2022-05-03 14:29:42	http://arxiv.org/abs/2205.01527v2	Parsl is a parallel programming library for Python that aims to make it easy to specify parallelism in programs and to realize that parallelism on arbitrary parallel and distributed computing systems. Parsl relies on developers annotating Python functions-wrapping either Python or external applications-to indicate that these functions may be executed concurrently. Developers can then link together functions via the exchange of data. Parsl establishes a dynamic dependency graph and sends tasks for execution on connected resources when dependencies are resolved. Parsl's runtime system enables different compute resources to be used, from laptops to supercomputers, without modification to the Parsl program.			arxiv	['Yadu Babuji', 'Anna Woodard', 'Ben Clifford', 'Zhuozhao Li', 'Mihael Hategan', 'Ian Foster', 'Mike Wilde', 'Daniel S. Katz']	569.0
954	PyEOC: a Python Code for Determining Electro-Optic Coefficients of Thin-Film Materials	Sidi Ould Saad Hamady	2022-05-10 20:43:12	http://arxiv.org/abs/2205.05157v1	PyEOC is an open-source Python code for determining electro-optic (EO) coefficients of thin-film materials from the static and dynamic reflectivity measurements. It uses the experimental results, the transfer-matrix method and implements a robust fitting procedure to precisely calculate the EO coefficients. The developed code is applied to a Pt/SBN/Pt/MgO structure and can be easily adapted to any multilayer planar structure. The values of the EO coefficients determined using PyEOC are in excellent agreement with those obtained in the literature and this code will make it possible to explore EO properties of other thin-film materials, in particular III-V and III-N semiconductors. PyEOC is released under the permissive open-source MIT license. It is made available at https://github.com/sidihamady/PyEOC and depends only on standard Python packages (NumPy, SciPy and Matplotlib).			arxiv	[]	570.0
955	celmech: A Python package for celestial mechanics	Sam Hadden	2022-05-20 18:00:09	http://arxiv.org/abs/2205.10385v1	We present celmech, an open-source Python package designed to facilitate a wide variety of celestial mechanics calculations. The package allows users to formulate and integrate equations of motion incorporating user-specified terms from the classical disturbing function expansion of the interaction potential between pairs of planets. The code can be applied, for example, to isolate the contribution of particular resonances to a system's dynamical evolution and develop simple analytical models with the minimum number of terms required to capture a particular dynamical phenomenon. Equations and expressions can be easily manipulated by leveraging the extensive symbolic mathematics capabilities of the sympy Python package. The celmech package is designed to interface seamlessly with the popular $N$-body code REBOUND to facilitate comparisons between calculation results and direct $N$-body integrations. The code is extensively documented and numerous example Jupyter notebooks illustrating its use are available online.			arxiv	['Daniel Tamayo']	571.0
956	A basic time series forecasting course with Python	Alain Zemkoho	2022-05-22 22:23:34	http://arxiv.org/abs/2205.10941v1	The aim of this paper is to present a set of Python-based tools to develop forecasts using time series data sets. The material is based on a four week course that the author has taught for seven years to students on operations research, management science, analytics, and statistics one-year MSc programmes. However, it can easily be adapted to various other audiences, including executive management or some undergraduate programmes. No particular knowledge of Python is required to use this material. Nevertheless, we assume a good level of familiarity with standard statistical forecasting methods such as exponential smoothing, AutoRegressive Integrated Moving Average (ARIMA), and regression-based techniques, which is required to deliver such a course. Access to relevant data, codes, and lecture notes, which serve as based for this material are made available (see github.com/abzemkoho/forecasting) for anyone interested in teaching such a course or developing some familiarity with the mathematical background of relevant methods and tools.			arxiv	[]	572.0
957	A generalized nonlinear Schr√∂dinger Python module implementing different models of input pulse quantum noise	O. Melchert	2022-06-13 16:18:10	http://arxiv.org/abs/2206.07526v1	"We provide Python tools enabling numerical simulation and analysis of the propagation dynamics of ultrashort laser pulses in nonlinear waveguides. The modeling approach is based on the widely used generalized nonlinear Schr\""odinger equation for the pulse envelope. The presented software implements the effects of linear dispersion, pulse self-steepening, and the Raman effect. The focus lies on the implementation of input pulse shot noise, i.e. classical background fields that mimick quantum noise, which are often not thoroughly presented in the scientific literature. We discuss and implement commonly adopted quantum noise models based on pure spectral phase noise, as well as Gaussian noise. Coherence properties of the resulting spectra can be calculated. We demonstrate the functionality of the software by reproducing results for a supercontinuum generation process in a photonic crystal fiber, documented in the scientific literature. The presented Python tools are are open-source and released under the MIT license in a publicly available software repository."			arxiv	['A. Demircan']	573.0
958	Cait: analysis toolkit for cryogenic particle detectors in Python	Felix Wagner	2022-07-05 17:40:07	http://arxiv.org/abs/2207.02187v2	Cryogenic solid state detectors are widely used in dark matter and neutrino experiments, and require a sensible raw data analysis. For this purpose, we present Cait, an open source Python package with all essential methods for the analysis of detector modules fully integrable with the Python ecosystem for scientific computing and machine learning. It comes with methods for triggering of events from continuously sampled streams, identification of particle recoils and artifacts in a low signal-to-noise ratio environment, the reconstruction of deposited energies, and the simulation of a variety of typical event types. Furthermore, by connecting Cait with existing machine learning frameworks we introduce novel methods, for better automation in data cleaning and background rejection.			arxiv	['Daniel Bartolot', 'Damir Rizvanovic', 'Florian Reindl', 'Jochen Schieck', 'Wolfgang Waltenberger']	574.0
959	The DynaSig-ML Python package: automated learning of biomolecular dynamics-function relationships	Olivier Mailhot	2022-07-07 13:03:05	http://arxiv.org/abs/2207.03276v1	Summary: The DynaSig-ML (Dynamical Signatures - Machine Learning) Python package allows the efficient, user-friendly exploration of 3D dynamics-function relationships in biomolecules, using datasets of experimental measures from large numbers of sequence variants. The DynaSig-ML package is built around the Elastic Network Contact Model (ENCoM), the first and only sequence-sensitive coarse-grained NMA model, which is used to generate the input Dynamical Signatures. Starting from in silico mutated structures, the whole pipeline can be run with just a few lines of Python and modest computational resources. The compute-intensive steps can also easily be parallelized in the case of either large biomolecules or vast amounts of sequence variants. As an example application, we use the DynaSig-ML package to predict the evolutionary fitness of the bacterial enzyme VIM-2 lactamase from deep mutational scan data. Availability and implementation: DynaSig-ML is open source software available at https: //github.com/gregorpatof/dynasigml package Contact: rafael.najmanovich@umontreal.ca			arxiv	['Francois Major', 'Rafael Najmanovich']	575.0
960	pocoMC: A Python package for accelerated Bayesian inference in astronomy and cosmology	Minas Karamanis	2022-07-12 16:42:32	http://arxiv.org/abs/2207.05660v1	pocoMC is a Python package for accelerated Bayesian inference in astronomy and cosmology. The code is designed to sample efficiently from posterior distributions with non-trivial geometry, including strong multimodality and non-linearity. To this end, pocoMC relies on the Preconditioned Monte Carlo algorithm which utilises a Normalising Flow in order to decorrelate the parameters of the posterior. It facilitates both tasks of parameter estimation and model comparison, focusing especially on computationally expensive applications. It allows fitting arbitrary models defined as a log-likelihood function and a log-prior probability density function in Python. Compared to popular alternatives (e.g. nested sampling) pocoMC can speed up the sampling procedure by orders of magnitude, cutting down the computational cost substantially. Finally, parallelisation to computing clusters manifests linear scaling.			arxiv	['David Nabergoj', 'Florian Beutler', 'John A. Peacock', 'Uros Seljak']	576.0
961	RobotIO: A Python Library for Robot Manipulation Experiments	Lukas Hermann	2022-07-27 15:46:13	http://arxiv.org/abs/2207.13591v2	Setting up robot environments to quickly test newly developed algorithms is still a difficult and time consuming process. This presents a significant hurdle to researchers interested in performing real-world robotic experiments. RobotIO is a python library designed to solve this problem. It focuses on providing common, simple, and well structured python interfaces for robots, grippers, and cameras, etc. These are provided with implementations of these interfaces for common hardware. This enables code using RobotIO to be portable across different robot setups. In terms of architecture, RobotIO is designed to be compatible with OpenAI gym environments, as well as ROS; examples of both of these are provided. The library comes together with a number of helpful tools, such as camera calibration scripts and episode recording functionality that further support algorithm development.			arxiv	['Max Argus', 'Adrian Roefer', 'Abhinav Valada', 'Thomas Brox']	577.0
962	A Library for Representing Python Programs as Graphs for Machine Learning	David Bieber	2022-08-15 22:36:17	http://arxiv.org/abs/2208.07461v1	Graph representations of programs are commonly a central element of machine learning for code research. We introduce an open source Python library python_graphs that applies static analysis to construct graph representations of Python programs suitable for training machine learning models. Our library admits the construction of control-flow graphs, data-flow graphs, and composite ``program graphs'' that combine control-flow, data-flow, syntactic, and lexical information about a program. We present the capabilities and limitations of the library, perform a case study applying the library to millions of competitive programming submissions, and showcase the library's utility for machine learning research.			arxiv	['Kensen Shi', 'Petros Maniatis', 'Charles Sutton', 'Vincent Hellendoorn', 'Daniel Johnson', 'Daniel Tarlow']	578.0
963	pyBumpHunter: A model independent bump hunting tool in Python for High Energy Physics analyses	Louis Vaslin	2022-08-31 10:28:28	http://arxiv.org/abs/2208.14760v4	The BumpHunter algorithm is widely used in the search for new particles in High Energy Physics analysis. This algorithm offers the advantage of evaluating the local and global p-values of a localized deviation in the observed data without making any hypothesis on the supposed signal. The increasing popularity of the Python programming language motivated the development of a new public implementation of this algorithm in Python, called pyBumpHunter, together with several improvements and additional features. It is the first public implementation of the BumpHunter algorithm to be added to Scikit-HEP. This paper presents in detail the BumpHunter algorithm as well as all the features proposed in this implementation. All these features have been tested in order to demonstrate their behaviour and performance.			arxiv	['Samuel Calvet', 'Vincent Barra', 'Julien Donini']	579.0
964	Python Implementation of the Dynamic Distributed Dimensional Data Model	Hayden Jananthan	2022-09-01 17:17:07	http://arxiv.org/abs/2209.00602v2	Python has become a standard scientific computing language with fast-growing support of machine learning and data analysis modules, as well as an increasing usage of big data. The Dynamic Distributed Dimensional Data Model (D4M) offers a highly composable, unified data model with strong performance built to handle big data fast and efficiently. In this work we present an implementation of D4M in Python. $D4M.py$ implements all foundational functionality of D4M and includes Accumulo and SQL database support via Graphulo. We describe the mathematical background and motivation, an explanation of the approaches made for its fundamental functions and building blocks, and performance results which compare $D4M.py$'s performance to D4M-MATLAB and D4M.jl.			arxiv	['Lauren Milechin', 'Michael Jones', 'William Arcand', 'William Bergeron', 'David Bestor', 'Chansup Byun', 'Michael Houle', 'Matthew Hubbell', 'Vijay Gadepally', 'Anna Klein', 'Peter Michaleas', 'Guillermo Morales', 'Julie Mullen', 'Andrew Prout', 'Albert Reuther', 'Antonio Rosa', 'Siddharth Samsi', 'Charles Yee', 'Jeremy Kepner']	580.0
965	pyerrors: a python framework for error analysis of Monte Carlo data	Fabian Joswig	2022-09-28 19:01:13	http://arxiv.org/abs/2209.14371v2	We present the pyerrors python package for statistical error analysis of Monte Carlo data. Linear error propagation using automatic differentiation in an object oriented framework is combined with the $\Gamma$-method for a reliable estimation of autocorrelation times. Data from different sources can easily be combined, keeping the information on the origin of error components intact throughout the analysis. pyerrors can be smoothly integrated into the existing scientific python ecosystem which allows for efficient and compact analyses.			arxiv	['Simon Kuberski', 'Justus T. Kuhlmann', 'Jan Neuendorf']	581.0
966	Modeling the cooldown of cryocooler conduction-cooled devices	Ram C. Dhuley	2022-10-14 15:01:36	http://arxiv.org/abs/2210.13218v1	Cryocooler conduction cooled devices can experience significant cooldown time due to lower available cooling capacity compares to convection cooled devices. Therefore, the cooldown time is an important design parameter for conduction cooled devices. This article introduces a framework developed in Python for calculating the cooldown profiles and cooldown time of cryocooler conduction-cooled devices such as superconducting magnets and accelerator cavities. The cooldown time estimation problem is essentially a system of ordinary first-order differential equations comprising the material properties (temperature dependent thermal conductivity and specific heat capacity) of the components intertwined with the prevailing heat transfer channels (conduction, radiation, and heat flow across pressed contacts) and the cryocooler capacity. The formulation of this ODE system is first presented. This ODE system is then solved using the in-built Python library odeint. A case study is presented comprising a small cryocooler conduction-cooled copper stabilized niobium-titanium magnet. The case study is supplemented with the Python script enabling the reader to simply tweak the device design parameters and optimize the design from the point of view of slow/fast cooldown.			arxiv	[]	582.0
967	scikit-fda: A Python Package for Functional Data Analysis	Carlos Ramos-Carre√±o	2022-11-04 16:34:03	http://arxiv.org/abs/2211.02566v2	The library scikit-fda is a Python package for Functional Data Analysis (FDA). It provides a comprehensive set of tools for representation, preprocessing, and exploratory analysis of functional data. The library is built upon and integrated in Python's scientific ecosystem. In particular, it conforms to the scikit-learn application programming interface so as to take advantage of the functionality for machine learning provided by this package: pipelines, model selection, and hyperparameter tuning, among others. The scikit-fda package has been released as free and open-source software under a 3-Clause BSD license and is open to contributions from the FDA community. The library's extensive documentation includes step-by-step tutorials and detailed examples of use.			arxiv	['Jos√© Luis Torrecilla', 'Miguel Carbajo-Berrocal', 'Pablo Marcos', 'Alberto Su√°rez']	583.0
968	Augmenting data-driven models for energy systems through feature engineering: A Python framework for feature engineering	Sandra Wilfling	2023-01-04 17:37:15	http://arxiv.org/abs/2301.01720v1	Data-driven modeling is an approach in energy systems modeling that has been gaining popularity. In data-driven modeling, machine learning methods such as linear regression, neural networks or decision-tree based methods are being applied. While these methods do not require domain knowledge, they are sensitive to data quality. Therefore, improving data quality in a dataset is beneficial for creating machine learning-based models. The improvement of data quality can be implemented through preprocessing methods. A selected type of preprocessing is feature engineering, which focuses on evaluating and improving the quality of certain features inside the dataset. Feature engineering methods include methods such as feature creation, feature expansion, or feature selection. In this work, a Python framework containing different feature engineering methods is presented. This framework contains different methods for feature creation, expansion and selection; in addition, methods for transforming or filtering data are implemented. The implementation of the framework is based on the Python library scikit-learn. The framework is demonstrated on a case study of a use case from energy demand prediction. A data-driven model is created including selected feature engineering methods. The results show an improvement in prediction accuracy through the engineered features.			arxiv	[]	584.0
969	BackdoorBox: A Python Toolbox for Backdoor Learning	Yiming Li	2023-02-01 09:45:42	http://arxiv.org/abs/2302.01762v1	Third-party resources ($e.g.$, samples, backbones, and pre-trained models) are usually involved in the training of deep neural networks (DNNs), which brings backdoor attacks as a new training-phase threat. In general, backdoor attackers intend to implant hidden backdoor in DNNs, so that the attacked DNNs behave normally on benign samples whereas their predictions will be maliciously changed to a pre-defined target label if hidden backdoors are activated by attacker-specified trigger patterns. To facilitate the research and development of more secure training schemes and defenses, we design an open-sourced Python toolbox that implements representative and advanced backdoor attacks and defenses under a unified and flexible framework. Our toolbox has four important and promising characteristics, including consistency, simplicity, flexibility, and co-development. It allows researchers and developers to easily implement and compare different methods on benchmark or their local datasets. This Python toolbox, namely \texttt{BackdoorBox}, is available at \url{https://github.com/THUYimingLi/BackdoorBox}.			arxiv	['Mengxi Ya', 'Yang Bai', 'Yong Jiang', 'Shu-Tao Xia']	585.0
970	Scaffolding Progress: How Structured Editors Shape Novice Errors When Transitioning from Blocks to Text	Majeed Kazemitabaar	2023-02-11 14:46:44	http://arxiv.org/abs/2302.05708v1	Transitioning from block-based programming to text-based programming environments can be challenging as it requires students to learn new programming language concepts. In this paper, we identify and classify the issues encountered when transitioning from block-based to text-based programming. In particular, we investigate differences that emerge in learners when using a structured editor compared to an unstructured editor. We followed 26 high school students (ages 12-16; M=14 years) as they transitioned from Scratch to Python in three phases: (i) learning Scratch, (ii) transitioning from Scratch to Python using either a structured or unstructured editor, and (iii) evaluating Python coding skills using an unstructured editor. We identify 27 distinct types of issues and show that learners who used a structured editor during the transition phase had 4.6x less syntax issues and 1.9x less data-type issues compared to those who did not. When these learners switched to an unstructured editor for evaluation, they kept a lower rate on data-type issues but faced 4x more syntax errors.			arxiv	['Viktar Chyhir', 'David Weintrop', 'Tovi Grossman']	586.0
971	PyExaFMM: an exercise in designing high-performance software with Python and Numba	Srinath Kailasa	2023-03-15 06:51:42	http://arxiv.org/abs/2303.08394v3	Numba is a game-changing compiler for high-performance computing with Python. It produces machine code that runs outside of the single-threaded Python interpreter and that fully utilizes the resources of modern CPUs. This means support for parallel multithreading and auto vectorization if available, as with compiled languages such as C++ or Fortran. In this article we document our experience developing PyExaFMM, a multithreaded Numba implementation of the Fast Multipole Method, an algorithm with a non-linear data structure and a large amount of data organization. We find that designing performant Numba code for complex algorithms can be as challenging as writing in a compiled language.			arxiv	['Tingyu Wang', 'Lorena A. Barba', 'Timo Betcke']	587.0
972	IMPACT: A Toolchain for Nonlinear Model Predictive Control Specification, Prototyping, and Deployment	Alvaro Florez	2023-03-15 18:10:23	http://arxiv.org/abs/2303.08850v1	We present IMPACT, a flexible toolchain for nonlinear model predictive control (NMPC) specification with automatic code generation capabilities. The toolchain reduces the engineering complexity of NMPC implementations by providing the user with an easy-to-use application programming interface, and with the flexibility of using multiple state-of-the-art tools and numerical optimization solvers for rapid prototyping of NMPC solutions. IMPACT is written in Python, users can call it from Python and MATLAB, and the generated NMPC solvers can be directly executed from C, Python, MATLAB and Simulink. An application example is presented involving problem specification and deployment on embedded hardware using Simulink, showing the effectiveness and applicability of IMPACT for NMPC-based solutions.			arxiv	['Alejandro Astudillo', 'Wilm Decr√©', 'Jan Swevers', 'Joris Gillis']	588.0
973	GMP-Featurizer: A parallelized Python package for efficiently computing the Gaussian Multipole features of atomic systems	Xiangyun Lei	2023-03-23 01:13:54	http://arxiv.org/abs/2303.12980v1	GMP-Featurizer is a lightweight, accurate, efficient, and scalable software package for calculating the Gaussian Multipole (GMP) features \cite{GMP} for a variety of atomic systems with elements across the periodic table. Starting from the GMP feature computation module from AmpTorch \cite{amptorch}, the capability of GMP-Featurizer has since been greatly improved, including its accuracy and efficiency, as well as the ability to parallelize on different cores, even machines. Moreover, this python package only has very few dependencies that are all standard python libraries, plus cffi for C++ code interfacing and Ray \cite{Ray} for parallelization, making it lightweight and robust. A set of unit tests are designed to ensure the reliability of its outputs. A set of extensive examples and tutorials, as well as two sets of pseudopotential files (needed for specifying the GMP feature set), are also included in this package for its users. Overall, this package is designed to serve as a standard implementation for chemical and material scientists who are interested in developing models based on GMP features. The source code for this package is freely available to the public under the Apache 2.0 license.			arxiv	['Joseph Montoya']	589.0
974	Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python	Tianyu Du	2023-04-04 16:00:48	http://arxiv.org/abs/2304.01906v3	The $\texttt{torch-choice}$ is an open-source library for flexible, fast choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a $\texttt{ChoiceDataset}$ data structure to manage databases flexibly and memory-efficiently. The paper demonstrates constructing a $\texttt{ChoiceDataset}$ from databases of various formats and functionalities of $\texttt{ChoiceDataset}$. The package implements two widely used models, namely the multinomial logit and nested logit models, and supports regularization during model estimation. The package incorporates the option to take advantage of GPUs for estimation, allowing it to scale to massive datasets while being computationally efficient. Models can be initialized using either R-style formula strings or Python dictionaries. We conclude with a comparison of the computational efficiencies of $\texttt{torch-choice}$ and $\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the number of covariates increases, and (3) the expansion of item sets. Finally, we demonstrate the scalability of $\texttt{torch-choice}$ on large-scale datasets.			arxiv	['Ayush Kanodia', 'Susan Athey']	590.0
975	An Exploratory Study of Ad Hoc Parsers in Python	Michael Schr√∂der	2023-04-19 15:21:39	http://arxiv.org/abs/2304.09733v1	Background: Ad hoc parsers are pieces of code that use common string functions like split, trim, or slice to effectively perform parsing. Whether it is handling command-line arguments, reading configuration files, parsing custom file formats, or any number of other minor string processing tasks, ad hoc parsing is ubiquitous -- yet poorly understood. Objective: This study aims to reveal the common syntactic and semantic characteristics of ad hoc parsing code in real world Python projects. Our goal is to understand the nature of ad hoc parsers in order to inform future program analysis efforts in this area. Method: We plan to conduct an exploratory study based on large-scale mining of open-source Python repositories from GitHub. We will use program slicing to identify program fragments related to ad hoc parsing and analyze these parsers and their surrounding contexts across 9 research questions using 25 initial syntactic and semantic metrics. Beyond descriptive statistics, we will attempt to identify common parsing patterns by cluster analysis.			arxiv	['Marc Goritschnig', 'J√ºrgen Cito']	591.0
976	HiQ -- A Declarative, Non-intrusive, Dynamic and Transparent Observability and Optimization System	Fuheng Wu	2023-04-26 06:11:26	http://arxiv.org/abs/2304.13302v1	This paper proposes a non-intrusive, declarative, dynamic and transparent system called `HiQ` to track Python program runtime information without compromising on the run-time system performance and losing insight. HiQ can be used for monolithic and distributed systems, offline and online applications. HiQ is developed when we optimize our large deep neural network (DNN) models which are written in Python, but it can be generalized to any Python program or distributed system, or even other languages like Java. We have implemented the system and adopted it in our deep learning model life cycle management system to catch the bottleneck while keeping our production code clean and highly performant. The implementation is open-sourced at: [https://github.com/oracle/hiq](https://github.com/oracle/hiq).			arxiv	['Ivan Davchev', 'Jun Qian']	592.0
977	VoDEx: a Python library for time annotation and management of volumetric functional imaging data	Anna Nadtochiy	2023-05-11 05:00:16	http://arxiv.org/abs/2305.07438v1	"Summary: In functional imaging studies, accurately synchronizing the time course of experimental manipulations and stimulus presentations with resulting imaging data is crucial for analysis. Current software tools lack such functionality, requiring manual processing of the experimental and imaging data, which is error-prone and potentially non-reproducible. We present VoDEx, an open-source Python library that streamlines the data management and analysis of functional imaging data. VoDEx synchronizes the experimental timeline and events (eg. presented stimuli, recorded behavior) with imaging data. VoDEx provides tools for logging and storing the timeline annotation, and enables retrieval of imaging data based on specific time-based and manipulation-based experimental conditions. Availability and Implementation: VoDEx is an open-source Python library and can be installed via the ""pip install"" command. It is released under a BSD license, and its source code is publicly accessible on GitHub https://github.com/LemonJust/vodex. A graphical interface is available as a napari-vodex plugin, which can be installed through the napari plugins menu or using ""pip install."" The source code for the napari plugin is available on GitHub https://github.com/LemonJust/napari-vodex."			arxiv	['Peter Luu', 'Scott E. Fraser', 'Thai V. Truong']	593.0
978	UQpy v4.1: Uncertainty Quantification with Python	Dimitrios Tsapetis	2023-05-16 16:11:04	http://arxiv.org/abs/2305.09572v1	This paper presents the latest improvements introduced in Version 4 of the UQpy, Uncertainty Quantification with Python, library. In the latest version, the code was restructured to conform with the latest Python coding conventions, refactored to simplify previous tightly coupled features, and improve its extensibility and modularity. To improve the robustness of UQpy, software engineering best practices were adopted. A new software development workflow significantly improved collaboration between team members, and continous integration and automated testing ensured the robustness and reliability of software performance. Continuous deployment of UQpy allowed its automated packaging and distribution in system agnostic format via multiple channels, while a Docker image enables the use of the toolbox regardless of operating system limitations.			arxiv	['Michael D. Shields', 'Dimitris G. Giovanis', 'Audrey Olivier', 'Lukas Novak', 'Promit Chakroborty', 'Himanshu Sharma', 'Mohit Chauhan', 'Katiana Kontolati', 'Lohit Vandanapu', 'Dimitrios Loukrezis', 'Michael Gardner']	594.0
979	PyTorch Hyperparameter Tuning - A Tutorial for spotPython	Thomas Bartz-Beielstein	2023-05-19 17:47:50	http://arxiv.org/abs/2305.11930v2	The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (``Sequential Parameter Optimization Toolbox in Python'') is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow. As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPython achieves similar or even better results while being more flexible and transparent than Ray Tune.			arxiv	[]	595.0
980	PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series	Wenjie Du	2023-05-30 07:57:05	http://arxiv.org/abs/2305.18811v1	PyPOTS is an open-source Python library dedicated to data mining and analysis on multivariate partially-observed time series, i.e. incomplete time series with missing values, A.K.A. irregularlysampled time series. Particularly, it provides easy access to diverse algorithms categorized into four tasks: imputation, classification, clustering, and forecasting. The included models contain probabilistic approaches as well as neural-network methods, with a well-designed and fully-documented programming interface for both academic researchers and industrial professionals to use. With robustness and scalability in its design philosophy, best practices of software construction, for example, unit testing, continuous integration (CI) and continuous delivery (CD), code coverage, maintainability evaluation, interactive tutorials, and parallelization, are carried out as principles during the development of PyPOTS. The toolkit is available on both Python Package Index (PyPI) and Anaconda. PyPOTS is open-source and publicly available on GitHub https://github.com/WenjieDu/PyPOTS.			arxiv	[]	596.0
981	Attention, Compilation, and Solver-based Symbolic Analysis are All You Need	Prithwish Jana	2023-06-11 19:47:52	http://arxiv.org/abs/2306.06755v2	In this paper, we present a Java-to-Python (J2P) and Python-to-Java (P2J) back-to-back code translation method, and an associated tool called CoTran, based on large language models (LLMs). Our method leverages the attention mechanism of LLMs, compilation, and symbolic execution-based test generation for equivalence testing between the input and output programs. More precisely, we modify the typical LLM training loop to incorporate compiler and symbolic execution loss. Via extensive experiments comparing CoTran with 12 other transpilers and LLM-based translation tools over a benchmark of more than 57,000 Java-Python equivalent pairs, we show that CoTran outperforms them on relevant metrics such as compilation and runtime equivalence accuracy. For example, our tool gets 97.43% compilation accuracy and 49.66% runtime equivalence accuracy for J2P translation, whereas the nearest competing tool only gets 92.84% and 40.95% respectively.			arxiv	['Piyush Jha', 'Haoyang Ju', 'Gautham Kishore', 'Aryan Mahajan', 'Vijay Ganesh']	597.0
982	musif: a Python package for symbolic music feature extraction	Ana Llorens	2023-07-03 15:49:15	http://arxiv.org/abs/2307.01120v1	In this work, we introduce musif, a Python package that facilitates the automatic extraction of features from symbolic music scores. The package includes the implementation of a large number of features, which have been developed by a team of experts in musicology, music theory, statistics, and computer science. Additionally, the package allows for the easy creation of custom features using commonly available Python libraries. musif is primarily geared towards processing high-quality musicological data encoded in MusicXML format, but also supports other formats commonly used in music information retrieval tasks, including MIDI, MEI, Kern, and others. We provide comprehensive documentation and tutorials to aid in the extension of the framework and to facilitate the introduction of new and inexperienced users to its usage.			arxiv	['Federico Simonetta', 'Mart√≠n Serrano', '√Ålvaro Torrente']	598.0
983	Comparing with Python: Text Analysis in Stata	Xiangtai Zuo	2023-07-19 22:24:46	http://arxiv.org/abs/2307.10480v1	Text analysis is the process of constructing structured data from unstructured textual content, usually implemented in Python. In terms of the principles of text analysis, a computer program with the ability to read a file and match it with a regular expression is all that is needed for basic text analysis. However, few researchers have used Stata as their main text analysis tool. In this paper, I will take a step-by-step approach to the practical process, giving examples of how text analysis can be performed with Stata, and comparing the code and running time with Python.			arxiv	[]	599.0
984	RamanSPy: An open-source Python package for integrative Raman spectroscopy data analysis	Dimitar Georgiev	2023-07-05 08:56:45	http://arxiv.org/abs/2307.13650v1	Raman spectroscopy is a non-destructive and label-free chemical analysis technique, which plays a key role in the analysis and discovery cycle of various branches of science. Nonetheless, progress in Raman spectroscopic analysis is still impeded by the lack of software, methodological and data standardisation, and the ensuing fragmentation and lack of reproducibility of analysis workflows thereof. To address these issues, we introduce RamanSPy, an open-source Python package for Raman spectroscopic research and analysis. RamanSPy provides a comprehensive library of ready-to-use tools for spectroscopic analysis, which streamlines day-to-day tasks, integrative analyses, as well as novel research and algorithmic development. RamanSPy is modular and open source, not tied to a particular technology or data format, and can be readily interfaced with the burgeoning ecosystem for data science, statistical analysis and machine learning in Python.			arxiv	['Simon Vilms Pedersen', 'Ruoxiao Xie', '√Ålvaro Fern√°ndez-Galiana', 'Molly M. Stevens', 'Mauricio Barahona']	600.0
985	Causal-learn: Causal Discovery in Python	Yujia Zheng	2023-07-31 05:00:35	http://arxiv.org/abs/2307.16405v1	Causal discovery aims at revealing causal relations from observational data, which is a fundamental task in science and engineering. We describe $\textit{causal-learn}$, an open-source Python library for causal discovery. This library focuses on bringing a comprehensive collection of causal discovery methods to both practitioners and researchers. It provides easy-to-use APIs for non-specialists, modular building blocks for developers, detailed documentation for learners, and comprehensive methods for all. Different from previous packages in R or Java, $\textit{causal-learn}$ is fully developed in Python, which could be more in tune with the recent preference shift in programming languages within related communities. The library is available at https://github.com/py-why/causal-learn.			arxiv	['Biwei Huang', 'Wei Chen', 'Joseph Ramsey', 'Mingming Gong', 'Ruichu Cai', 'Shohei Shimizu', 'Peter Spirtes', 'Kun Zhang']	601.0
986	Simple Python tools for modelling few-level atom-light interactions	Lucy Downes	2023-07-31 17:02:05	http://arxiv.org/abs/2307.16839v1	Understanding the interactions between atoms and light is at the heart of atomic physics. Being able to `experiment' with various system parameters, produce plots of the results and interpret these is very useful, especially for those new to the field. This tutorial aims to provide an introduction to the equations governing near-resonant atom-light interactions and present examples of setting up and solving these equations in Python. Emphasis is placed on clarity and understanding by showing code snippets alongside relevant equations, and as such it is suitable for those without an excellent working knowledge of Python or the underlying physics. Hopefully the methods presented here can form the foundations on which more complex models and simulations can be built. All functions presented here and example codes can be found on GitHub.			arxiv	[]	602.0
987	Streamlined data analysis in Python	Luis Altenkort	2023-08-12 23:07:43	http://arxiv.org/abs/2308.06652v3	Python is a particularly appealing language to carry out data analysis, owing in part to its user-friendly character as well as its access to well maintained and powerful libraries like NumPy and SciPy. Still, for the purpose of analyzing data in a lattice QCD context, some desirable functionality is missing from these libraries. Moreover, scripting languages tend to be slower than compiled ones. To help address these points we present the AnalysisToolbox, a collection of Python modules to facilitate lattice QCD data analysis. Some highlighted features include general-purpose jackknife and bootstrap routines; modules for reading in and storing gauge configurations; a module to carry out hadron resonance gas model calculations; and convenience wrappers for SciPy integration, curve fitting, and splines. These features are sped up behind the scenes using parallelization and just-in-time compilation.			arxiv	['David Anthony Clarke', 'Jishnu Goswami', 'Hauke Sandmeyer']	603.0
988	Quantum MASALA: Quantum MAterialS Ab initio eLectronic-structure pAckage	Shri Hari Soundararaj	2023-08-14 17:06:33	http://arxiv.org/abs/2308.07277v1	We present QuantumMASALA, a compact package that implements different electronic structure methods in Python. Within just 8000 lines of pure Python code, we have implemented Density Functional Theory (DFT), Time dependent Density Functional Theory (TD-DFT) and the GW Method. The program can run across multiple process cores and in Graphical Processing Units (GPU) with the help of easily-accessible Python libraries. With QuantumESPRESSO and BerkeleyGW I/O interfaces implemented, it can also be used as a substitute for small scale calculations, making it a perfect learning tool for ab initio methods. The package is aimed to provide a framework with its modular and simple code design to rapidly build and test new methods for first-principles calculation.			arxiv	['Agrim Sharma', 'Manish Jain']	604.0
989	PycWB: A User-friendly, Modular, and Python-based Framework for Gravitational Wave Unmodelled Search	Yumeng Xu	2023-08-16 19:20:26	http://arxiv.org/abs/2308.08639v1	Unmodelled searches and reconstruction is a critical aspect of gravitational wave data analysis, requiring sophisticated software tools for robust data analysis. This paper introduces PycWB, a user-friendly and modular Python-based framework developed to enhance such analyses based on the widely used unmodelled search and reconstruction algorithm Coherent Wave Burst (cWB). The main features include a transition from C++ scripts to YAML format for user-defined parameters, improved modularity, and a shift from complex class-encapsulated algorithms to compartmentalized modules. The pycWB architecture facilitates efficient dependency management, better error-checking, and the use of parallel computation for performance enhancement. Moreover, the use of Python harnesses its rich library of packages, facilitating post-production analysis and visualization. The PycWB framework is designed to improve the user experience and accelerate the development of unmodelled gravitational wave analysis.			arxiv	['Shubhanshu Tiwari', 'Marco Drago']	605.0
990	Twice Upon a Time: Timelike-Separated Quantum Extremal Surfaces	Netta Engelhardt	2023-08-30 18:00:02	http://arxiv.org/abs/2308.16226v1	The Python's Lunch conjecture for the complexity of bulk reconstruction involves two types of nonminimal quantum extremal surfaces (QESs): bulges and throats, which differ by their local properties. The conjecture relies on the connection between bulk spatial geometry and quantum codes: a constricting geometry from bulge to throat encodes the bulk state nonisometrically, and so requires an exponentially complex Grover search to decode. However, thus far, the Python's Lunch conjecture is only defined for spacetimes where all QESs are spacelike-separated from one another. Here we explicitly construct (time-reflection symmetric) spacetimes featuring both timelike-separated bulges and timelike-separated throats. Interestingly, all our examples also feature a third type of QES, locally resembling a de Sitter bifurcation surface, which we name a bounce. By analyzing the Hessian of generalized entropy at a QES, we argue that this classification into throats, bulges and bounces is exhaustive. We then propose an updated Python's Lunch conjecture that can accommodate general timelike-separated QESs and bounces. Notably, our proposal suggests that the gravitational analogue of a tensor network is not necessarily the time-reflection symmetric slice, even when one exists.			arxiv	['Geoff Penington', 'Arvin Shahbazi-Moghaddam']	606.0
991	Can Programming Languages Boost Each Other via Instruction Tuning?	Daoguang Zan	2023-08-31 15:53:51	http://arxiv.org/abs/2308.16824v2	When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.			arxiv	['Ailun Yu', 'Bo Shen', 'Jiaxin Zhang', 'Taihong Chen', 'Bing Geng', 'Bei Chen', 'Jichuan Ji', 'Yafen Yao', 'Yongji Wang', 'Qianxiang Wang']	607.0
992	PyHGL: A Python-based Hardware Generation Language Framework	Jintao Sun	2023-09-09 18:28:41	http://arxiv.org/abs/2309.04859v1	Hardware generation languages (HGLs) increase hardware design productivity by creating parameterized modules and test benches. Unfortunately, existing tools are not widely adopted due to several demerits, including limited support for asynchronous circuits and unknown states, lack of concise and efficient language features, and low integration of simulation and verification functions. This paper introduces PyHGL, an open-source Python framework that aims to provide a simple and unified environment for hardware generation, simulation, and verification. PyHGL language is a syntactical superset of Python, which greatly reduces the lines of code (LOC) and improves productivity by providing unique features such as dynamic typing, vectorized operations, and automatic port deduction. In addition, PyHGL integrates an event-driven simulator that simulates the asynchronous behaviors of digital circuits using three-state logic. We also propose an algorithm that eliminates the calculation and transmission overhead of unknown state propagation for binary stimuli. The results suggest that PyHGL code is up to 6.1x denser than traditional RTL and generates high-quality synthesizable RTL code. Moreover, the optimized simulator achieves 2.9x speed up and matches the performance of a commonly used open-source logic simulator.			arxiv	['Zeke Wang', 'Tao Lu', 'Wenzhi Chen']	608.0
993	Integrating Python data analysis in an existing introductory laboratory course	Eugenio Tufino	2023-09-12 12:01:02	http://arxiv.org/abs/2309.06158v1	In this article we describe how we successfully incorporated data analysis in Python in a first-year laboratory course without significantly altering the course structure and without overburdening students. We show how we created and used carefully designed Jupyter Notebooks with exercises and physics application examples that allow students to master data analysis programming in the laboratory course. We use these Notebooks to guide students through the fundamentals of data handling and analysis in Python while performing simple experiments. We present our teaching approach and the developed materials. We discuss the effectiveness of our intervention based on the results from pre- and post- course questionnaires and students' group work. The results presented give insights about advantages and challenges of introducing computation at the early stage of the curriculum in a laboratory course setting and are informative for other instructors and the physics education research community.			arxiv	['Stefano Oss', 'Micol Alemani']	609.0
994	LPML: LLM-Prompting Markup Language for Mathematical Reasoning	Ryutaro Yamauchi	2023-09-21 02:46:20	http://arxiv.org/abs/2309.13078v2	In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.			arxiv	['Sho Sonoda', 'Akiyoshi Sannai', 'Wataru Kumagai']	610.0
995	PySimFrac: A Python Library for Synthetic Fracture Generation, Analysis, and Simulation	Eric Guiltinan	2023-09-25 03:24:30	http://arxiv.org/abs/2309.13849v1	In this paper, we introduce Pysimfrac, a open-source python library for generating 3-D synthetic fracture realizations, integrating with fluid simulators, and performing analysis. Pysimfrac allows the user to specify one of three fracture generation techniques (Box, Gaussian, or Spectral) and perform statistical analysis including the autocorrelation, moments, and probability density functions of the fracture surfaces and aperture. This analysis and accessibility of a python library allows the user to create realistic fracture realizations and vary properties of interest. In addition, Pysimfrac includes integration examples to two different pore-scale simulators and the discrete fracture network simulator, dfnWorks. The capabilities developed in this work provides opportunity for quick and smooth adoption and implementation by the wider scientific community for accurate characterization of fluid transport in geologic media. We present Pysimfrac along with integration examples and discuss the ability to extend Pysimfrac from a single complex fracture to complex fracture networks.			arxiv	['Javier E. Santos', 'Prakash Purswani', 'Jeffrey D. Hyman']	611.0
996	Bayesian Methodologies with pyhf	Matthew Feickert	2023-09-29 06:40:11	http://arxiv.org/abs/2309.17005v2	bayesian_pyhf is a Python package that allows for the parallel Bayesian and frequentist evaluation of multi-channel binned statistical models. The Python library pyhf is used to build such models according to the HistFactory framework and already includes many frequentist inference methodologies. The pyhf-built models are then used as data-generating model for Bayesian inference and evaluated with the Python library PyMC. Based on Monte Carlo Chain Methods, PyMC allows for Bayesian modelling and together with the arviz library offers a wide range of Bayesian analysis tools.			arxiv	['Lukas Heinrich', 'Malin Horstmann']	612.0
997	PyHexTop: a compact Python code for topology optimization using hexagonal elements	Aditi Agarwal	2023-10-03 11:21:34	http://arxiv.org/abs/2310.01968v1	"Python serves as an open-source and cost-effective alternative to the MATLAB programming language. This paper introduces a concise topology optimization Python code, named ``PyHexTop,"" primarily intended for educational purposes. Code employs hexagonal elements to parameterize design domains as such elements provide checkerboard-free optimized design naturally. PyHexTop is developed based on the ``HoneyTop90"" MATLAB code~\cite{kumar2023honeytop90} and uses the NumPy and SciPy libraries. Code is straightforward and easily comprehensible, proving a helpful tool that can help people new in the topology optimization field to learn and explore. PyHexTop is specifically tailored to address compliance minimization with specified volume constraints. The paper provides a detailed explanation of the code for solving the MBB design and extensions to solve problems with varying boundary and force conditions. The code is publicly shared at: \url{https://github.com/PrabhatIn/PyHexTop.}"			arxiv	['Anupam Saxena', 'Prabhat Kumar']	613.0
998	CAFA-evaluator: A Python Tool for Benchmarking Ontological Classification Methods	Damiano Piovesan	2023-10-10 10:51:47	http://arxiv.org/abs/2310.06881v1	We present CAFA-evaluator, a powerful Python program designed to evaluate the performance of prediction methods on targets with hierarchical concept dependencies. It generalizes multi-label evaluation to modern ontologies where the prediction targets are drawn from a directed acyclic graph and achieves high efficiency by leveraging matrix computation and topological sorting. The program requirements include a small number of standard Python libraries, making CAFA-evaluator easy to maintain. The code replicates the Critical Assessment of protein Function Annotation (CAFA) benchmarking, which evaluates predictions of the consistent subgraphs in Gene Ontology. Owing to its reliability and accuracy, the organizers have selected CAFA-evaluator as the official CAFA evaluation software.			arxiv	['Davide Zago', 'Parnal Joshi', 'M. Clara De Paolis Kaluza', 'Alexander Miguel Monzon', 'Walter Reade', 'Iddo Friedberg', 'Predrag Radivojac', 'Silvio C. E. Tosatto']	614.0
999	API-Assisted Code Generation for Question Answering on Varied Table Structures	Yihan Cao	2023-10-23 08:26:28	http://arxiv.org/abs/2310.14687v1	A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes -- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.			arxiv	['Shuyi Chen', 'Ryan Liu', 'Zhiruo Wang', 'Daniel Fried']	615.0
